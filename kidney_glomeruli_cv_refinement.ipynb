{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuBMAP - Hacking the Kidney - Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Kaggle competition page](https://www.kaggle.com/c/hubmap-kidney-segmentation)\n",
    "\n",
    "Helpful Notebooks:\n",
    "* [https://www.kaggle.com/markalavin/hubmap-tile-images-w-overlap-and-build-tfrecords](https://www.kaggle.com/markalavin/hubmap-tile-images-w-overlap-and-build-tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDO\n",
    "* Look at impact of different affine matrices\n",
    "* Look at impact of removing alpha channel on model size and performance\n",
    "* Add Deepmind's architecture optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Downloads for Offline use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update -n base conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda config --set always_yes True\n",
    "# ! conda install -c fastai -c pytorch fastai\n",
    "# #! conda install pytorch torchvision torchaudio fastai -c pytorch\n",
    "# #! conda update pytorch torchvision torchaudio cudatoolkit -c pytorch\n",
    "# ! conda install pandas\n",
    "# ! conda install -c conda-forge kaggle\n",
    "# ! conda install -c conda-forge tifffile\n",
    "# ! conda install -c conda-forge tqdm\n",
    "# ! conda install -c conda-forge matplotlib\n",
    "# ! conda install -c conda-forge pytorch-lightning\n",
    "# ! conda install -c conda-forge wandb\n",
    "# ! conda install -c conda-forge arrow\n",
    "# !conda install -c conda-forge pickle5\n",
    "# !conda install -n base -c conda-forge jupyterlab_widgets\n",
    "# !conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install arrow pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed if running in wsl2\n",
    "#! pip install pytorch-lightning wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea why the conda-forge version doesn't work\n",
    "\n",
    "#!python -m pip install opencv-python\n",
    "\n",
    "# If you are running this notebook on a server (like Linux on WSL2) you need the headless version of opencv\n",
    "# The regular opencv requires GUI packages that serves dont have, and will raise an error\n",
    "#!python -m pip install opencv-python-headless\n",
    "\n",
    "# temporary solution to use tab complete - something wrong with jupyter jedi - need to downgrade\n",
    "#!pip install jedi==0.17.2\n",
    "\n",
    "# !pip install torchio --upgrade\n",
    "\n",
    "#!pip install pytorch-lightning-bolts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the finicky local CUDA is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# prebuilt models\n",
    "# from pl_bolts.models import UNet\n",
    "\n",
    "import tensorboard as tb\n",
    "\n",
    "# Need to put kaggle.json in /%USERS%/.kaggle folder (C:/Users/Craig/.kaggle)\n",
    "try:\n",
    "    import kaggle\n",
    "except:\n",
    "    !echo '{\"username\":\"canadarmy\",\"key\": KAGGLEKEY}' > ~/.kaggle/kaggle.json\n",
    "    import kaggle\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from typing import Union\n",
    "\n",
    "# Read tiff images\n",
    "import tifffile\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torchio as tio\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import time\n",
    "import wandb\n",
    "import arrow\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Memory management tools\n",
    "import gc\n",
    "\n",
    "import timm\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.imports import *\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.metrics import Dice, Jaccard, JaccardCoeff\n",
    "\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./data\")\n",
    "# kaggle.api.dataset_download_files(\"iafoss/hubmap-1024x1024\", path=data_path)\n",
    "# kaggle.api.dataset_download_files(\"baesiann/glomeruli-hubmap-external-1024x1024\", path=data_path)\n",
    "# kaggle.api.dataset_download_files(\"iafoss/hubmap-256x256\", path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you are about to download the data in the cvorrect directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the data in the correct folder - commented out so as to not repeat the unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile(data_path/\"glomeruli-hubmap-external-1024x1024.zip\", 'r') as zipref:\n",
    "#     zipref.extractall(data_path)\n",
    "    \n",
    "# (data_path/\"masks_1024\").rename(data_path/\"masks_ext_1024\")\n",
    "# (data_path/\"images_1024\").rename(data_path/\"images_ext_1024\")\n",
    "    \n",
    "# with zipfile.ZipFile(data_path/\"hubmap-1024x1024.zip\", 'r') as zipref:\n",
    "#     zipref.extractall(data_path)\n",
    "    \n",
    "# (data_path/\"masks\").rename(data_path/\"masks_1024\")\n",
    "# (data_path/\"train\").rename(data_path/\"images_1024\")\n",
    "    \n",
    "# with zipfile.ZipFile(data_path/\"hubmap-256x256.zip\", 'r') as zipref:\n",
    "#     zipref.extractall(data_path)\n",
    "\n",
    "# (data_path/\"masks\").rename(data_path/\"masks_256\")\n",
    "# (data_path/\"train\").rename(data_path/\"images_256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((data_path/\"images_256\").ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((data_path/\"images_1024\").ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these two datasets are the same - 1 is just higher resolution than the other, we need to rename one set because most of the file names are the same\n",
    "\n",
    "I have also confirmed that in rach folder, the masks and the images are the same name. Se we will rename the 256 pixel images with a `-256` suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in (data_path/\"images_256\").ls():\n",
    "#     file.rename(data_path/\"images_256\"/(file.name.split(\".\")[0]+\"-256\"+\".png\"))\n",
    "    \n",
    "# for file in (data_path/\"masks_256\").ls():\n",
    "#     file.rename(data_path/\"masks_256\"/(file.name.split(\".\")[0]+\"-256\"+\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Main Functions\n",
    "################\n",
    "\n",
    "\n",
    "def rle2mask(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: encoding string value from csv\n",
    "    shape: (width,height) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    # return a list of starting pixels and a list of lengths\n",
    "    starts, lengths = [\n",
    "        np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])\n",
    "    ]\n",
    "    # subtract 1 from every starting pixel\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    # calculate a background of 0 (empty) with size defined by image\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    # replace every 0 within each range with 1\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = 1\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def mask2rle(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "# def get_id_by_index(index, df=train_df):\n",
    "#     return df.iloc[index]['img_id']\n",
    "\n",
    "def get_single_img(id, folder=\"train\"):\n",
    "    img = tifffile.imread(path/folder/(id+\".tiff\"))\n",
    "    if len(img.shape) == 5:\n",
    "        img = img.squeeze().transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "def show_single_img(id, **kwargs):\n",
    "    return plt.imshow(get_single_img(id), **kwargs)\n",
    "\n",
    "# def show_img_by_index(index, df=train_df):\n",
    "#     return plt.imshow(tifffile.imread(path/\"train\"/(train_df.iloc[TEST_IMAGE_INDEX]['id']+\".tiff\")))\n",
    "\n",
    "# def get_single_encs(id, df=train_df):\n",
    "#     return df[df['img_id'] == id]['encoding'].array[0]\n",
    "\n",
    "# def get_mask(id, df=train_df, folder=\"train\"):\n",
    "#     return rle2mask(\n",
    "#         get_single_encs(id, df=df),\n",
    "#         get_single_img(id, folder=folder).shape[::-1][1:]\n",
    "#     )\n",
    "\n",
    "def show_single_img_and_mask_by_id(id):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    mask = get_mask(id)\n",
    "    img = get_single_img(id)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image\", fontsize=18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, cmap=\"hot\", alpha=0.5)\n",
    "    plt.title(f\"Image + mask\", fontsize=18)    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, cmap=\"hot\")\n",
    "    plt.title(f\"Mask\", fontsize=18)    \n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def show_single_img_and_mask(subject: tio.data.subject.Subject, resize_to=50):\n",
    "    plt.figure(figsize=(120, 100))\n",
    "    \n",
    "    if not isinstance(subject, tio.data.subject.Subject):\n",
    "        raise TypeError(f\"The subject is required to be of type torchio.data.subject.Subject but you provided {type(subject)}\")\n",
    "    \n",
    "    img = subject[\"img\"][tio.DATA].squeeze().permute(1,2,0)\n",
    "    mask = subject[\"mask\"][tio.DATA].squeeze().unsqueeze(2)\n",
    "    \n",
    "    if resize_to:\n",
    "        img = resizer(img, scale=resize_to)\n",
    "        mask = resizer(mask, scale=resize_to)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image\", fontsize=18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, cmap=\"hot\", alpha=0.5)\n",
    "    plt.title(f\"Image + mask\", fontsize=18)    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, cmap=\"hot\")\n",
    "    plt.title(f\"Mask\", fontsize=18)    \n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def to_4d(img, input_chan_first=False, output_chan_first=True):\n",
    "    if not len(img.shape)==3:\n",
    "        raise ValueError(\"Function only converts 3D arrayto 4D array\")\n",
    "    return np.expand_dims(np.transpose(img, \n",
    "                   (0,1,2) if input_chan_first else (2,0,1)), \n",
    "                   3 if output_chan_first else 0)\n",
    "\n",
    "def to_3d(img, input_chan_first=True, output_chan_first=False):\n",
    "    if not len(img.shape)==4:\n",
    "        raise ValueError(\"Function only converts 4D arrayto 3D array\")\n",
    "    return np.transpose(img.squeeze(), (0,1,2) if output_chan_first else (1,2,0))\n",
    "\n",
    "def to_3chan(x, dim=1):\n",
    "    return torch.cat((x,x,x), dim=dim)\n",
    "\n",
    "def resizer(img, scale=5, show=False):\n",
    "    \"\"\"\n",
    "    Returns an smaller array of the same dimensions, but converts to 3D to allow for resizing\n",
    "    \"\"\"\n",
    "    scale_percent = scale # percent of original size\n",
    "    im_dims = (len(img.shape) == 4)\n",
    "    if im_dims:\n",
    "        img = to_3d(img)\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    img_reshaped = cv2.resize(img.numpy(), dim)\n",
    "    if show:\n",
    "        return plt.imshow(img_reshaped)\n",
    "    if im_dims:\n",
    "        return to_4d(img_reshaped)\n",
    "    return img_reshaped\n",
    "\n",
    "def squeeze_and_reshape(img_tensor, remove_alpha=False):\n",
    "    if not isinstance(img_tensor, torch.Tensor):\n",
    "        raise TypeError(\"Image needs to be a tensor\")\n",
    "    if len(img_tensor.shape) == 5:\n",
    "        img_tensor = img_tensor.squeeze().permute(2, 1, 0)\n",
    "    if img_tensor.shape[0] == 3:\n",
    "        img_tensor = img_tensor.permute(2, 1, 0)\n",
    "    img_tensor = img_tensor.unsqueeze(2).permute(3,1,0,2)\n",
    "    return img_tensor\n",
    "\n",
    "def to_pil(image):\n",
    "    # for \n",
    "    data = image.numpy().squeeze().T\n",
    "    data = data.astype(np.uint8)\n",
    "    image = Image.fromarray(data)\n",
    "    w, h = image.size\n",
    "    display(image)\n",
    "    print() \n",
    "    \n",
    "def normalize_array(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalized data\n",
    "    \"\"\"\n",
    "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remask(img, mask, tile, threshold=8, show=False):\n",
    "    \n",
    "    img_height = img.shape[1]\n",
    "    img_width = img.shape[0]\n",
    "    \n",
    "    number_of_vertical_tiles = (img_height // tile)+1\n",
    "    number_of_horizontal_tiles = (img_width // tile)+1\n",
    "    \n",
    "    #background = np.zeros((tile*number_of_horizontal_tiles, tile*number_of_vertical_tiles,3))[:img.shape[0],:img.shape[1],:img.shape[2]]\n",
    "    \n",
    "    tile_coords = []\n",
    "    for h_idx in range(number_of_horizontal_tiles):\n",
    "        for v_idx in range(number_of_vertical_tiles):\n",
    "            tile_coords.append((h_idx+1, v_idx+1)) # +1 to remove 0 indexing\n",
    "\n",
    "    cropped_images = []\n",
    "    for h,v in tile_coords:\n",
    "        cropped_images.append((h, v, img[tile*(h-1):tile*h, tile*(v-1):tile*v, :]))\n",
    "        \n",
    "    for horiz,vert,im in cropped_images:\n",
    "        if not 0 in im.shape:      #required in case tile is \n",
    "            \n",
    "            hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
    "            h, s, v = cv2.split(hsv)\n",
    "            if s.mean() < threshold:\n",
    "                all_black = np.full((im.shape[0], im.shape[1]),2)\n",
    "                mask_dims = mask[tile*(horiz-1):tile*horiz,tile*(vert-1):tile*vert]\n",
    "                all_black = np.full((mask_dims.shape[0], mask_dims.shape[1]),2)\n",
    "                mask[tile*(horiz-1):tile*horiz,tile*(vert-1):tile*vert] = all_black\n",
    "                #im = im*0.\n",
    "            #background[tile*(horiz-1):tile*horiz,tile*(vert-1):tile*vert,:] = im\n",
    "    \n",
    "    if show:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img.astype('uint8'))\n",
    "        plt.title(f\"Image\", fontsize=18)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(img.astype('uint8'))\n",
    "        plt.imshow(mask.astype('uint8'), cmap=\"hot\", alpha=0.5)\n",
    "        plt.title(f\"Image + mask\", fontsize=18)    \n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(mask.astype('uint8'), cmap=\"hot\")\n",
    "        plt.title(f\"Mask\", fontsize=18)    \n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "#img_id = get_id_by_index(7)\n",
    "#img_id = '095bf7a1f'\n",
    "#with tifffile.TiffFile(path/\"train\"/(img_id+\".tiff\")) as tif:\n",
    "#    imgg = tif.asarray()\n",
    "#print(imgg.shape)\n",
    "#mask = get_mask(img_id)\n",
    "#new_mask = remask(to_3d(squeeze_and_reshape(imgg)), mask, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_df(df, directory):\n",
    "    mask_list = []\n",
    "    for idx,_ in tqdm(enumerate(df.iterrows()), total=len(df)):\n",
    "        img_id = get_id_by_index(idx, df=df)\n",
    "        with tifffile.TiffFile(path/directory/(img_id+\".tiff\")) as tif:\n",
    "            base_im = tif.asarray()\n",
    "            print(base_im.shape)\n",
    "            im_tensor = squeeze_and_reshape(torch.from_numpy(base_im)).numpy()\n",
    "            print(im_tensor.shape)\n",
    "            mask = remask(to_3d(im_tensor), get_mask(img_id), 1000)\n",
    "            print(f\"Mask shape is {mask.shape}\")\n",
    "            \n",
    "            cut_image(im_tensor, img_id, mask, write_path/\"smaller\")\n",
    "            \n",
    "            del base_im, im_tensor, mask\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_image(img_id, source_path:Path, destination_path: Path, mask_df=None):\n",
    "    \"\"\"\n",
    "    Cut image (and corresponding mask - in Dataframe - if supplied) into QUARTERS and save them to a directory called smaller\n",
    "    \"\"\"\n",
    "    \n",
    "    img = tio.Image(source_path/f\"{img_id}.tiff\").data\n",
    "    if len(img.shape) != 4:\n",
    "        raise ValueError(\"Tensor shape needs to have 4 dimensions\")\n",
    "    if img.shape[0] != 4:\n",
    "        raise ValueError(\"First dimension must have 4 channels\")\n",
    "    vertical_tiles = img.shape[2] // 2\n",
    "    horizontal_tiles = img.shape[1] // 2\n",
    "    \n",
    "\n",
    "    \n",
    "    img1 = tio.Pad(((512,512,0)))(img[:,:horizontal_tiles,:vertical_tiles,:])\n",
    "    tio.Image(tensor=img1).save(destination_path/\"imgs\"/f\"{img_id}_1.tiff\")\n",
    "    del img1\n",
    "    gc.collect()\n",
    "\n",
    "    img2 = tio.Pad((512,512,0))(img[:,horizontal_tiles:,:vertical_tiles,:])\n",
    "    tio.Image(tensor=img2).save(destination_path/\"imgs\"/f\"{img_id}_2.tiff\")\n",
    "    del img2\n",
    "    gc.collect()\n",
    "    \n",
    "    img3 = tio.Pad((512,512,0))(img[:,:horizontal_tiles,vertical_tiles:,:])\n",
    "    tio.Image(tensor=img3).save(destination_path/\"imgs\"/f\"{img_id}_3.tiff\")\n",
    "    del img3\n",
    "    gc.collect()\n",
    "    \n",
    "    img4 = tio.Pad((512,512,0))(img[:,horizontal_tiles:,vertical_tiles:,:])\n",
    "    tio.Image(tensor=img4).save(destination_path/\"imgs\"/f\"{img_id}_4.tiff\")\n",
    "    del img4\n",
    "    gc.collect()\n",
    "    \n",
    "    del img\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if not isinstance(mask_df, NoneType):\n",
    "        mask = torch.from_numpy(mask_df[mask_df[\"img_id\"]==img_id][\"mask\"].values[0]).unsqueeze(0).unsqueeze(3)\n",
    "        # I have managed to flip the axes somewhere and am too lazy or stubborn to fix the root issue. So need to permute axes\n",
    "        mask = mask.permute(0,2,1,3)\n",
    "        \n",
    "        mask1 = tio.Pad((512,512,0))(mask[:,:horizontal_tiles,:vertical_tiles,:])\n",
    "        tio.Image(tensor=mask1).save(destination_path/\"masks\"/f\"{img_id}_1_mask.tiff\")\n",
    "        del mask1\n",
    "        gc.collect()\n",
    "        \n",
    "        mask2 = tio.Pad((512,512,0))(mask[:,horizontal_tiles:,:vertical_tiles,:])\n",
    "        tio.Image(tensor=mask2).save(destination_path/\"masks\"/f\"{img_id}_2_mask.tiff\")\n",
    "        del mask2\n",
    "        gc.collect()\n",
    "        \n",
    "        mask3 = tio.Pad((512,512,0))(mask[:,:horizontal_tiles,vertical_tiles:,:])\n",
    "        tio.Image(tensor=mask3).save(destination_path/\"masks\"/f\"{img_id}_3_mask.tiff\")\n",
    "        del mask3\n",
    "        gc.collect()\n",
    "        \n",
    "        mask4 = tio.Pad((512,512,0))(mask[:,horizontal_tiles:,vertical_tiles:,:])\n",
    "        tio.Image(tensor=mask4).save(destination_path/\"masks\"/f\"{img_id}_4_mask.tiff\")\n",
    "        \n",
    "        del mask4\n",
    "        gc.collect()\n",
    "\n",
    "        del mask\n",
    "        gc.collect()\n",
    "        \n",
    "# Uncomment if these files dont exist\n",
    "# [cut_image(item, path/\"train\", path/\"smaller\", mask_df=new_masks) for item in new_masks.img_id.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restitch_image(img_id, pred_mask=None):\n",
    "    for name in (path/\"smaller/imgs\").glob(f\"{img_id}_?.tiff\"):\n",
    "        img_quarter = name.name.split(\"_\")[1].split(\".\")[0]\n",
    "        if img_quarter == \"1\":\n",
    "            img1 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "        if img_quarter == \"2\":\n",
    "            img2 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "        if img_quarter == \"3\":\n",
    "            img3 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "        if img_quarter == \"4\":\n",
    "            img4 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "    \n",
    "    # make a 4D tensor with 4 channels and 1 depth channel\n",
    "    whole_image = torch.zeros(\n",
    "        img1.shape[0],\n",
    "        img1.shape[1] + img3.shape[1],\n",
    "        img1.shape[2] + img2.shape[2],\n",
    "    ).unsqueeze(3)\n",
    "    \n",
    "    whole_image[:,:whole_image.shape[1]//2, :whole_image.shape[2]//2, :] = img1\n",
    "    whole_image[:,whole_image.shape[1]//2-1:, :whole_image.shape[2]//2, :] = img2\n",
    "    whole_image[:,:whole_image.shape[1]//2,  whole_image.shape[2]//2:, :] = img3\n",
    "    whole_image[:,whole_image.shape[1]//2-1:,  whole_image.shape[2]//2:, :] = img4\n",
    "    \n",
    "    to_pil(whole_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restitch_image(get_id_by_index(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a baseline model that looked to have good accuracy but now we want to refine it. We will do the following:\n",
    "\n",
    "1. Update the colour normalisation\n",
    "2. Add blur and artifacts periodically - also update the frequency with which these transformations take place\n",
    "3. Update the loss function\n",
    "4. Patch overlap for TorchIO [ONLY DONE IN INFERENCE]\n",
    "5. Try something like TransUNET\n",
    "6. Try training on smaller patch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all file names in folders of interest\n",
    "# we know thaqt file names are the same regardless of wther the file is in masks or images directory\n",
    "\n",
    "img_dirs = [folder.name for folder in data_path.ls() if \"images\" in folder.name]\n",
    "\n",
    "img_names = []\n",
    "for folder in img_dirs:\n",
    "    for im in (data_path/folder).ls():\n",
    "        img_names.append((im.parent.name, im.name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label all masks with glom\n",
    "\n",
    "# holder = []\n",
    "# for folder in master_bar(img_dirs):\n",
    "#     for im in progress_bar((data_path/folder).ls()):\n",
    "#         mask = tio.LabelMap(data_path/(im.parent.name).replace(\"images\", \"masks\")/im.name)\n",
    "#         holder.append((im.parent.name, im.name, 1 in mask.data))\n",
    "        \n",
    "# has_glom = pd.DataFrame(holder, columns=[\"dir\", \"name\", \"has_glom\"])\n",
    "\n",
    "has_glom = pd.read_csv(\"./has_glom.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_glom[\"has_glom\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_glom[has_glom[\"name\"] == '0486052bb_0080-256.png'][\"has_glom\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_folder = random.choice(img_dirs)\n",
    "random_image = random.choice((data_path/random_folder).ls()).name\n",
    "rand_im = tio.ScalarImage(data_path/random_folder/random_image)\n",
    "rand_mask = tio.LabelMap(data_path/(random_folder).replace(\"images\", \"masks\")/random_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_im.data.permute(3,1,2,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_creator(affine = torch.tensor([[-1.,  0.,  0.,  0.], [ 0., -1.,  0.,  0.], [ 0.,  0.,  1.,  0.], [ 0.,  0.,  0.,  1.]])):\n",
    "    subjects_list = []\n",
    "    for img in img_names:\n",
    "        pic_name = img[1]\n",
    "        im = tio.ScalarImage(data_path/img[0]/img[1])\n",
    "        mask = tio.LabelMap(data_path/img[0].replace(\"images\", \"masks\")/img[1], affine=affine)\n",
    "\n",
    "        if has_glom[has_glom[\"name\"] == pic_name][\"has_glom\"].values[0] == False:\n",
    "            if random.choices([True, False], weights=(2,5))[0]:\n",
    "                subjects_list.append(tio.Subject(\n",
    "                    img = im,\n",
    "                    mask = mask,\n",
    "                    img_id = pic_name,\n",
    "                ))\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            subjects_list.append(tio.Subject(\n",
    "                    img = im,\n",
    "                    mask = mask,\n",
    "                    img_id = pic_name,\n",
    "                ))\n",
    "    return subjects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def subject_creator(df, affine = torch.tensor([[-1.,  0.,  0.,  0.], [ 0., -1.,  0.,  0.], [ 0.,  0.,  1.,  0.], [ 0.,  0.,  0.,  1.]])):\n",
    "#     subjects_list = []\n",
    "#     for idx,_ in tqdm(enumerate(df.iterrows()), total=len(df)):\n",
    "        \n",
    "#         img_id = get_id_by_index(idx, df=df)\n",
    "        \n",
    "#         pic_list = [item for item in (path/\"smaller/imgs\").rglob(\"*\") if not item.is_dir() and img_id in item.name]\n",
    "        \n",
    "#         for pic in pic_list:\n",
    "#             pic_name = pic.name.split(\".\")[0]\n",
    "#             im = tio.ScalarImage(path/\"smaller/imgs\"/(pic_name+\".tiff\"))\n",
    "#             mask = tio.LabelMap(path/\"smaller/masks\"/(pic_name+\"_mask.tiff\"), affine=affine)\n",
    "            \n",
    "#             ########CRITICAL TO NOTE###########\n",
    "#             # below is a check to see if any positives actually exist within the image quarters\n",
    "#             # if we do not run this step, we get an error when we train only on patches with positive values\n",
    "            \n",
    "#             if 1 in mask.data:\n",
    "#                 subjects_list.append(tio.Subject(\n",
    "#                     img = im,\n",
    "#                     mask = mask,\n",
    "#                     img_id = pic_name\n",
    "#                 ))\n",
    "#             else:\n",
    "#                 continue\n",
    "#     return subjects_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_items = subject_creator(new_masks)\n",
    "    transforms = tio.Compose([custom_reshape, custom_normalization])\n",
    "    test_dataset = tio.SubjectsDataset(test_items, transform=transforms)\n",
    "    \n",
    "    test_img = test_dataset[0]\n",
    "    \n",
    "    downsized_img = tio.Resample((4,4,1))(test_img[\"img\"][tio.DATA])\n",
    "    \n",
    "    downsized_img.shape\n",
    "    \n",
    "    plt.imshow(downsized_img.squeeze().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_single_img_and_mask(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_normalization = tio.Lambda(lambda x: (x/255).float(), types_to_apply=[tio.INTENSITY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshuffle(x):\n",
    "    return x.permute(3,1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_reshape = tio.Lambda(lambda x: x[:3,...], types_to_apply=[tio.INTENSITY])\n",
    "custom_reshape = tio.Lambda(lambda x: x.squeeze().unsqueeze(0), types_to_apply=[tio.INTENSITY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_to3d = tio.Lambda(lambda x: to_3chan(x, 0), types_to_apply=[tio.LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_squeeze = tio.Lambda(lambda x: x.squeeze().unsqueeze(0), types_to_apply=[tio.LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnecessary as I should find out why there are different shapes but I want to get to model building\n",
    "def shuffle_axes(img_tensor):\n",
    "    return img_tensor.permute(0,2,1,3)\n",
    "reshuffle = tio.Lambda(shuffle_axes, types_to_apply=[tio.LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_shrink = tio.Lambda(lambda x: torch.tensor(resizer(x, 15)))\n",
    "\n",
    "resample_2x = tio.Lambda(lambda x: tio.Resample((2,2,1))(x), types_to_apply=[tio.INTENSITY])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_size = (512, 512, 1)\n",
    "# patch_size = (224, 224, 1)\n",
    "patch_size = (256, 256, 1)\n",
    "sample_ratio = {0: 1, 1: 10, 2: 1}\n",
    "\n",
    "subjects_list = subject_creator()\n",
    "subjects_list_copy = subjects_list[:]     # needed because shuffle does in place\n",
    "\n",
    "random.seed(22222)\n",
    "#random.seed(57)\n",
    "random.shuffle(subjects_list_copy)\n",
    "\n",
    "train_subjects = subjects_list_copy[:round(len(subjects_list_copy)*0.8)]\n",
    "valid_subjects = subjects_list_copy[round(len(subjects_list_copy)*0.8):]\n",
    "#train_subjects = subjects_list_copy[:1]\n",
    "#valid_subjects = subjects_list_copy[1:2]\n",
    "\n",
    "#train_transforms = tio.Compose([tio.Resample((20,20,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "#valid_transforms = tio.Compose([tio.Resample((20,20,1)), custom_reshape, custom_normalization,])\n",
    "train_transforms = tio.Compose([\n",
    "#     custom_reshape, \n",
    "#     custom_to3d, \n",
    "#     tio.RescaleIntensity(percentiles=(0.5, 99.5)), \n",
    "    tio.OneOf([\n",
    "        tio.RandomFlip(),\n",
    "        tio.RandomAffine(),\n",
    "        tio.RandomNoise(\n",
    "            mean=(0.5,0.5),\n",
    "        ),\n",
    "        tio.RandomBlur(),\n",
    "        tio.RandomBiasField(),\n",
    "        tio.RandomSwap(\n",
    "            patch_size=(15,15,1)\n",
    "        )\n",
    "    ]),\n",
    "\n",
    "    custom_normalization, \n",
    "])\n",
    "valid_transforms = tio.Compose([\n",
    "#     tio.RescaleIntensity(percentiles=(0.5, 99.5)),\n",
    "#     custom_reshape, \n",
    "#     custom_to3d, \n",
    "    custom_normalization\n",
    "])\n",
    "\n",
    "train_dataset = tio.SubjectsDataset(train_subjects, transform=train_transforms)\n",
    "valid_dataset = tio.SubjectsDataset(valid_subjects, transform=valid_transforms)\n",
    "\n",
    "queue_length = 40\n",
    "samples_per_volume = 4\n",
    "\n",
    "sampler = tio.data.LabelSampler(patch_size, label_probabilities=sample_ratio)\n",
    "\n",
    "train_queue = tio.Queue(\n",
    "    train_dataset,\n",
    "    queue_length,\n",
    "    samples_per_volume,\n",
    "    sampler,\n",
    "    num_workers=0,\n",
    "    shuffle_subjects=True,\n",
    "    shuffle_patches=True,\n",
    ")\n",
    "\n",
    "valid_queue = tio.Queue(\n",
    "    valid_dataset,\n",
    "    queue_length,\n",
    "    samples_per_volume,\n",
    "    sampler,\n",
    "    num_workers=0,\n",
    "    shuffle_subjects=False,\n",
    "    shuffle_patches=False,\n",
    ")\n",
    "\n",
    "# train_loader = DataLoader(train_queue, batch_size=16)\n",
    "# valid_loader = DataLoader(valid_queue, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_img_ids = [i[\"img_id\"] for i in train_subjects]\n",
    "valid_img_ids = [i[\"img_id\"] for i in valid_subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_dataset[0][\"mask\"][tio.DATA].squeeze().unsqueeze(2), aspect='auto')\n",
    "# test_mask = tifffile.imread(path/\"smaller/masks/cb2d976f4_2_mask.tiff\")\n",
    "# plt.imshow(test_mask, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 in valid_dataset[-1][\"mask\"][tio.DATA].squeeze().unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_creator(subjects_list):\n",
    "    \"\"\"\n",
    "    Takes a list of objects and returns a tuple of same length\n",
    "    First value in tuple is a list of the x-values, second is a list of y-values\n",
    "    \"\"\"\n",
    "    x = torch.stack([img[\"img\"][tio.DATA].permute(3,0,1,2) for img in subjects_list], 0).squeeze()\n",
    "    y = torch.stack([mask[\"mask\"][tio.DATA].permute(3,0,1,2) for mask in subjects_list], 0).squeeze().unsqueeze(1)\n",
    "    return (x, y)\n",
    "\n",
    "dls = DataLoaders(\n",
    "    TfmdDL(\n",
    "        train_queue, \n",
    "        batch_size=8, \n",
    "        num_workers=12,\n",
    "        #chunkify=lambda x: print(str(x)),\n",
    "        # returns generator of indices (provided by sample attribute), length is provided by queue sample length\n",
    "        #create_batches=lambda x: print(x),\n",
    "        # passed a list of length batchsize and collates into a batch\n",
    "        #create_batch=lambda x: print(x[1][\"img\"][tio.DATA].shape),\n",
    "        create_batch=batch_creator,\n",
    "        after_batch=[\n",
    "            Normalize.from_stats(*imagenet_stats),\n",
    "            Hue(p=0.1),\n",
    "            Saturation(p=0.1),\n",
    "            Brightness(p=0.1)\n",
    "        ],\n",
    "    ),\n",
    "    TfmdDL(\n",
    "        valid_queue, \n",
    "        batch_size=8, \n",
    "        num_workers=12, \n",
    "        create_batch=batch_creator,\n",
    "        after_batch=[Normalize.from_stats(*imagenet_stats)],\n",
    "    ),\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(to_3d(dls.train_ds[0][\"img\"][tio.DATA]))\n",
    "dls.train_ds[50000][\"img\"][tio.DATA].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.one_batch()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prebatched=False\n",
    "\n",
    "#def create_batch(b): return (fa_collate,fa_convert)[prebatched](b)\n",
    "#create_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function to enable gradients on `torch.where`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroOrOneFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        res = torch.where(input > 0.5, torch.tensor(1.0, requires_grad=True).cuda(), torch.tensor(0.0, requires_grad=True).cuda())\n",
    "        ctx.save_for_backward(res)\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors\n",
    "        return input\n",
    "    \n",
    "zero_or_one = ZeroOrOneFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai callbacks\n",
    "\n",
    "class PrinterCallback(Callback):\n",
    "    \"\"\"\n",
    "    Snaps image of x, y, and preds every specified number of batches\n",
    "    Saves images to path specified\n",
    "    \"\"\"\n",
    "    def __init__(self, path, img_freq=105):\n",
    "        self.img_freq = img_freq\n",
    "        self.path = path\n",
    "    def after_batch(self):\n",
    "        if self.iter % self.img_freq == 0:\n",
    "            img_list = []\n",
    "            with torch.no_grad():\n",
    "                for i in range(self.pred.shape[0]):\n",
    "                    x = self.x[i,...]\n",
    "                    y = to_3chan(self.y[i,...], 0)\n",
    "                    pred = to_3chan(self.pred[i,...], 0)\n",
    "                    img_list.append(x)\n",
    "                    img_list.append(y)\n",
    "                    img_list.append(pred)\n",
    "\n",
    "                grid = torchvision.utils.make_grid(\n",
    "                    img_list,\n",
    "                    nrow=3,\n",
    "                )\n",
    "                self._save(self.path, grid, self.epoch, self.iter, round(self.loss.item(), 3))\n",
    "                \n",
    "        #print(f\"The learning rate is {self.opt.hypers[0]['lr']}\")\n",
    "        #print({self.dls.valid.subjects_dataset._transform})\n",
    "        \n",
    "    @staticmethod\n",
    "    def _save(img_path, img, epoch, batch, loss):\n",
    "        npimg = normalize_array(img.cpu().detach().float().numpy())\n",
    "        plt.imsave(img_path/f\"epoch{epoch}batch{batch}__{loss}.png\", np.transpose(npimg, (1,2,0)))\n",
    "        \n",
    "class ConvertY(Callback):\n",
    "    \"\"\"\n",
    "    Since we used TorchIO to sample the data, we first need to convert the y back to its normal values\n",
    "    \"\"\"\n",
    "    def before_batch(self):\n",
    "        \"\"\"\n",
    "        NOTE: as per the docs, you can only assign to `yb`, not `y`\n",
    "        `yb` is a tuple (which is immutable) therefore you must override the `self.learn.yb` - note we are assigning to to `learn.yb`\n",
    "        \"\"\"\n",
    "        #self.yb = tuple([torch.where(self.y != torch.tensor(1).cuda(), torch.tensor(0).cuda(), torch.tensor(1).cuda())])\n",
    "        self.learn.yb = tuple([torch.where(self.y != torch.tensor(1).cuda(), torch.tensor(0).cuda(), torch.tensor(1).cuda())])\n",
    "        \n",
    "        #print(self.yb[0].shape)\n",
    "        #print(len(self.yb))\n",
    "        \n",
    "    #def after_pred(self):\n",
    "        # To check to see that the overwritten values of y did change\n",
    "        #print(self.y)\n",
    "        #print(dir(self))\n",
    "        \n",
    "\n",
    "        \n",
    "## NOTE: this may not be needed anymore now that our loss function cobines this step        \n",
    "class AddSigmoidActivation(Callback):\n",
    "    \"\"\"\n",
    "    Change the output to add a Sigmoid function \n",
    "    Needed since:\n",
    "        a) Using a pretrained Resnet model that doesn't support adding a final activation layer\n",
    "        b) unlike `cnn_learner`, a `unet_learner` doesn't have the `custom_head` parameter (which the forums suggest is an option to effectively add a layer to a pretarined model)\n",
    "    Note: need to check if `learner.model[-1].add_module` would work if you subclassed `nn.Module` and created a `forward()` method that added this activation?\n",
    "    \"\"\"\n",
    "    def after_pred(self):\n",
    "        \"\"\"\n",
    "        As per the documentation, this callback hook is specifically designed for modifying the outputs BEFORE theyre sent to the loss function\n",
    "        Thus it is a perfect place to add our sigmoid function to the outputs\n",
    "        \"\"\"\n",
    "        self.learn.pred = nn.Sigmoid()(self.pred)\n",
    "#         self.learn.pred = zero_or_one(nn.Sigmoid()(self.pred))\n",
    "        \n",
    "class ProgressiveTransformsUpdateCallback(Callback):\n",
    "    def before_epoch(self):\n",
    "        if self.epoch < 4:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((4,4,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((4,4,1)), custom_reshape, custom_normalization,])\n",
    "            )\n",
    "            #for h in self.opt.hypers:\n",
    "            #    h[\"lr\"] = 0.00001\n",
    "        if 3 < self.epoch < 8:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((2,2,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((2,2,1)), custom_reshape, custom_normalization,])\n",
    "            )\n",
    "            for h in self.opt.hypers:\n",
    "                h[\"lr\"] = 0.00001\n",
    "        if self.epoch > 7:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([custom_reshape, custom_normalization,])\n",
    "            )\n",
    "        #print(self.data.dataset.subjects_dataset.dry_iter())\n",
    "        \n",
    "class ModifyTransformsCallback(Callback):\n",
    "    \"\"\"\n",
    "    Train and Valid transforms must each be a list of transforms wrapped in tio.Compose\n",
    "    \"\"\"\n",
    "    def __init__(self, train_callbacks, valid_callbacks):\n",
    "        self.train_callbacks = train_callbacks\n",
    "        self.valid_callbacks = valid_callbacks\n",
    "        \n",
    "    def before_epoch(self):\n",
    "        self.dls.train.subjects_dataset.set_transform(self.train_callbacks)\n",
    "        self.dls.valid.subjects_dataset.set_transform(self.valid_callbacks)\n",
    "        \n",
    "class UpsamplePredCallback(Callback):\n",
    "    \"\"\"\n",
    "    If we downsampled the x-values, the preds will be the same resolution. Therefore we need to upsample to the size the y-value (masks) expect\n",
    "    \"\"\"\n",
    "    def __init__(self, upscale):\n",
    "        self.upscale = upscale\n",
    "        \n",
    "    def after_pred(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class SwitchLossCallback(Callback):\n",
    "    def after_pred(self):\n",
    "        if self.iter > round(self.n_iter / 2):\n",
    "            self.learn.loss_func = hh_dtloss\n",
    "            \n",
    "class UpdateContourLoss(Callback):\n",
    "    def after_pred(self):\n",
    "        if self.iter % 1000 == 0 and self.iter != 0:\n",
    "            prior_lambda_coeff = self.learn.loss_func.lambda_coeff\n",
    "            self.learn.loss_func = AdaptiveLoss(prior_lambda_coeff+0.01)\n",
    "            print(f\"Lambda coefficient updated to {prior_lambda_coeff + 0.01}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner = unet_learner(dls, resnet34, n_out=1, loss_func=dice_loss, lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that none of our callbacks appear here as we have not set them at the `learner` level but rather at the `fit` level. This is because we (for the most part) want callbacks for training only. However it may be necessary to add callbacks here later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.show_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.dls.dataset.subjects_dataset.set_transform\n",
    "#new_train_transforms = tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "#new_valid_transforms = tio.Compose([custom_reshape, custom_normalization,])\n",
    "#learner.dls.train.subjects_dataset.set_transform(new_train_transforms)\n",
    "#learner.dls.valid.subjects_dataset.set_transform(new_valid_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    \"resnet\",\n",
    "    \"transforms\",\n",
    "    \"custom_loss\",\n",
    "    \"reduceLR\",\n",
    "    \"150_samples\"\n",
    "]\n",
    "\n",
    "group = \"resnet\"\n",
    "\n",
    "notes = \"Perimeter loss\"\n",
    "\n",
    "name = \"perimeter_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 20,\n",
    "    \"transforms\": \"all\",\n",
    "    \"loss\": \"perimeter_loss\",\n",
    "    \"lr\": 1e-5,\n",
    "    \"model\": \"resnet34\",\n",
    "    \"train_type\": \"fit\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project='HuBMAP_model_experiments', entity='stantonius', name=name, tags=tags, group=group, notes=notes, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks \n",
    "\n",
    "#os.environ['WANDB_MODE'] = 'dryrun'\n",
    "# wandb_callback = WandbCallback(log='all')\n",
    "path = Path()\n",
    "\n",
    "model_name = arrow.utcnow().format(\"DDMMMYY\") + \"_\" + name\n",
    "save_model_callback = SaveModelCallback(fname=model_name, every_epoch=True)\n",
    "save_image_path = path/\"training_image_logs\"\n",
    "\n",
    "\n",
    "\n",
    "updated_train_transforms = tio.Compose([tio.Resample((4,4,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "updated_valid_transforms = tio.Compose([tio.Resample((4,4,1)), custom_reshape, custom_normalization,])\n",
    "\n",
    "updated_transforms = ModifyTransformsCallback(updated_train_transforms, updated_valid_transforms)\n",
    "reduce_lr = ReduceLROnPlateau(min_delta=0.2, patience=5)\n",
    "\n",
    "cbs=[PrinterCallback(save_image_path), ConvertY(), AddSigmoidActivation(), save_model_callback, UpdateContourLoss(), reduce_lr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs) \n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        \n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "    \n",
    "dice_loss = DiceLoss()    \n",
    "    \n",
    "class DiceBCELoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs)  \n",
    "\n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        \n",
    "        # Note changed before from binary_cross_entropy to binary_cross_entropy_with_logits\n",
    "        # got an error\n",
    "        # However this step requires us to combine our sigmoid layer\n",
    "        BCE = F.binary_cross_entropy(inputs.float(), targets.float(), reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE\n",
    "    \n",
    "bce_dice_loss = DiceBCELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.ndimage.morphology import distance_transform_edt as edt\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "\"\"\"\n",
    "Hausdorff loss implementation based on paper:\n",
    "https://arxiv.org/pdf/1904.10030.pdf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class HausdorffDTLoss(nn.Module):\n",
    "    \"\"\"Binary Hausdorff loss based on distance transform\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=2.0, **kwargs):\n",
    "        super(HausdorffDTLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def distance_field(self, img: np.ndarray) -> np.ndarray:\n",
    "        field = np.zeros_like(img)\n",
    "\n",
    "        for batch in range(len(img)):\n",
    "            fg_mask = img[batch] > 0.5\n",
    "\n",
    "            if fg_mask.any():\n",
    "                bg_mask = ~fg_mask\n",
    "\n",
    "                fg_dist = edt(fg_mask)\n",
    "                bg_dist = edt(bg_mask)\n",
    "\n",
    "                field[batch] = fg_dist + bg_dist\n",
    "\n",
    "        return field\n",
    "\n",
    "    def forward(\n",
    "        self, pred: torch.Tensor, target: torch.Tensor, debug=False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Uses one binary channel: 1 - fg, 0 - bg\n",
    "        pred: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        target: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        \"\"\"\n",
    "        assert pred.dim() == 4 or pred.dim() == 5, \"Only 2D and 3D supported\"\n",
    "        assert (\n",
    "            pred.dim() == target.dim()\n",
    "        ), \"Prediction and target need to be of same dimension\"\n",
    "\n",
    "        # pred = torch.sigmoid(pred)\n",
    "\n",
    "        pred_dt = torch.from_numpy(self.distance_field(pred.detach().cpu().numpy())).float()\n",
    "        target_dt = torch.from_numpy(self.distance_field(target.detach().cpu().numpy())).float()\n",
    "\n",
    "        pred_error = (pred - target) ** 2\n",
    "        distance = pred_dt ** self.alpha + target_dt ** self.alpha\n",
    "\n",
    "        dt_field = pred_error.cpu() * distance\n",
    "        loss = dt_field.mean()\n",
    "\n",
    "        if debug:\n",
    "            return (\n",
    "                loss.cpu().numpy(),\n",
    "                (\n",
    "                    dt_field.cpu().numpy()[0, 0],\n",
    "                    pred_error.cpu().numpy()[0, 0],\n",
    "                    distance.cpu().numpy()[0, 0],\n",
    "                    pred_dt.cpu().numpy()[0, 0],\n",
    "                    target_dt.cpu().numpy()[0, 0],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "class HausdorffERLoss(nn.Module):\n",
    "    \"\"\"Binary Hausdorff loss based on morphological erosion\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=2.0, erosions=10, **kwargs):\n",
    "        super(HausdorffERLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.erosions = erosions\n",
    "        self.prepare_kernels()\n",
    "\n",
    "    def prepare_kernels(self):\n",
    "        cross = np.array([cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))])\n",
    "        bound = np.array([[[0, 0, 0], [0, 1, 0], [0, 0, 0]]])\n",
    "\n",
    "        self.kernel2D = cross * 0.2\n",
    "        self.kernel3D = np.array([bound, cross, bound]) * (1 / 7)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perform_erosion(\n",
    "        self, pred: np.ndarray, target: np.ndarray, debug\n",
    "    ) -> np.ndarray:\n",
    "        bound = (pred - target) ** 2\n",
    "\n",
    "        if bound.ndim == 5:\n",
    "            kernel = self.kernel3D\n",
    "        elif bound.ndim == 4:\n",
    "            kernel = self.kernel2D\n",
    "        else:\n",
    "            raise ValueError(f\"Dimension {bound.ndim} is nor supported.\")\n",
    "\n",
    "        eroted = np.zeros_like(bound)\n",
    "        erosions = []\n",
    "\n",
    "        for batch in range(len(bound)):\n",
    "\n",
    "            # debug\n",
    "            erosions.append(np.copy(bound[batch][0]))\n",
    "\n",
    "            for k in range(self.erosions):\n",
    "\n",
    "                # compute convolution with kernel\n",
    "                dilation = convolve(bound[batch], kernel, mode=\"constant\", cval=0.0)\n",
    "\n",
    "                # apply soft thresholding at 0.5 and normalize\n",
    "                erosion = dilation - 0.5\n",
    "                erosion[erosion < 0] = 0\n",
    "\n",
    "                if erosion.ptp() != 0:\n",
    "                    erosion = (erosion - erosion.min()) / erosion.ptp()\n",
    "\n",
    "                # save erosion and add to loss\n",
    "                bound[batch] = erosion\n",
    "                eroted[batch] += erosion * (k + 1) ** self.alpha\n",
    "\n",
    "                if debug:\n",
    "                    erosions.append(np.copy(erosion[0]))\n",
    "\n",
    "        # image visualization in debug mode\n",
    "        if debug:\n",
    "            return eroted, erosions\n",
    "        else:\n",
    "            return eroted\n",
    "\n",
    "    def forward(\n",
    "        self, pred: torch.Tensor, target: torch.Tensor, debug=False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Uses one binary channel: 1 - fg, 0 - bg\n",
    "        pred: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        target: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        \"\"\"\n",
    "        assert pred.dim() == 4 or pred.dim() == 5, \"Only 2D and 3D supported\"\n",
    "        assert (\n",
    "            pred.dim() == target.dim()\n",
    "        ), \"Prediction and target need to be of same dimension\"\n",
    "\n",
    "        # pred = torch.sigmoid(pred)\n",
    "\n",
    "        if debug:\n",
    "            eroted, erosions = self.perform_erosion(\n",
    "                pred.cpu().numpy(), target.cpu().numpy(), debug\n",
    "            )\n",
    "            return eroted.mean(), erosions\n",
    "\n",
    "        else:\n",
    "            eroted = torch.from_numpy(\n",
    "                self.perform_erosion(pred, target, debug)\n",
    "            ).float()\n",
    "\n",
    "            loss = eroted.mean()\n",
    "\n",
    "            return loss\n",
    "        \n",
    "hh_erloss = HausdorffERLoss()\n",
    "hh_dtloss = HausdorffDTLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import Variable\n",
    "# from torch.autograd.function import Function\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# def odd_flip(H):\n",
    "#     '''\n",
    "#     generate frequency map\n",
    "#     when height or width of image is odd number,\n",
    "#     creat a array concol [0,1,...,int(H/2)+1,int(H/2),...,0]\n",
    "#     len(concol) = H\n",
    "#     '''\n",
    "#     m = int(H/2)\n",
    "#     col = np.arange(0,m+1)\n",
    "#     flipcol = col[m-1::-1]\n",
    "#     concol = np.concatenate((col,flipcol),0)\n",
    "#     return concol\n",
    "\n",
    "# def even_flip(H):\n",
    "#     '''\n",
    "#     generate frequency map\n",
    "#     when height or width of image is even number,\n",
    "#     creat a array concol [0,1,...,int(H/2),int(H/2),...,0]\n",
    "#     len(concol) = H\n",
    "#     '''\n",
    "#     m = int(H/2)\n",
    "#     col = np.arange(0,m)\n",
    "#     flipcol = col[m::-1]\n",
    "#     concol = np.concatenate((col,flipcol),0)\n",
    "#     return concol\n",
    "\n",
    "# def dist(target):\n",
    "#     '''\n",
    "#     sqrt(m^2 + n^2) in eq(8)\n",
    "#     '''\n",
    "\n",
    "#     _,_,H,W = target.shape\n",
    "\n",
    "#     if H%2 ==1:\n",
    "#         concol = odd_flip(H)\n",
    "#     else:\n",
    "#         concol = even_flip(H)\n",
    "        \n",
    "#     if W%2 == 1:\n",
    "#         conrow = odd_flip(W)\n",
    "#     else:\n",
    "#         conrow = even_flip(W)\n",
    "        \n",
    "#     m_col = concol[:,np.newaxis] \n",
    "#     m_row = conrow[np.newaxis,:]\n",
    "#     dist = np.sqrt(m_col*m_col + m_row*m_row) # sqrt(m^2+n^2)\n",
    "  \n",
    "#     use_cuda = torch.cuda.is_available()\n",
    "#     if use_cuda:\n",
    "#         dist_ = torch.from_numpy(dist).float().cuda()\n",
    "#     else:\n",
    "#         dist_ = torch.from_numpy(dist).float()\n",
    "#     return dist_\n",
    "\n",
    "# class EnergyLoss(nn.Module):\n",
    "#     def __init__(self,cuda,alpha,sigma):\n",
    "#         super(EnergyLoss, self).__init__()\n",
    "#         self.energylossfunc = EnergylossFunc.apply\n",
    "#         self.alpha = alpha\n",
    "#         self.cuda = cuda\n",
    "#         self.sigma = sigma\n",
    "\n",
    "#     def forward(self,feat,label):\n",
    "#         return self.energylossfunc(self.cuda,feat, label,self.alpha,self.sigma)\n",
    "    \n",
    "# class EnergylossFunc(Function):\n",
    "#     '''\n",
    "#     target: ground truth \n",
    "#     feat: Z -0.5. Z：prob of your target class(here is vessel) with shape[B,H,W]. \n",
    "#     Z from softmax output of unet with shape [B,C,H,W]. C: number of classes\n",
    "#     alpha: default 0.35\n",
    "#     sigma: default 0.25\n",
    "#     '''\n",
    "#     @staticmethod\n",
    "#     def forward(ctx,cuda,feat_levelset,target,alpha,sigma,Gaussian = False):\n",
    "#         hardtanh = nn.Hardtanh(min_val=0, max_val=1, inplace=False)\n",
    "#         target = target.float()\n",
    "#         index_ = dist(target)\n",
    "#         dim_ = target.shape[1]\n",
    "#         target = torch.squeeze(target,1)\n",
    "#         I1 = target + alpha*hardtanh(feat_levelset/sigma) # G_t + alpha*H(phi) in eq(5)\n",
    "#         dmn = torch.rfft(I1,2,normalized = True, onesided = False)\n",
    "#         dmn_r = dmn[:,:,:,0] # dmn's real part\n",
    "#         dmn_i = dmn[:,:,:,1] # dmm's imagine part\n",
    "#         dmn2 = dmn_r * dmn_r + dmn_i * dmn_i # dmn^2\n",
    "\n",
    "#         ctx.save_for_backward(feat_levelset,target,dmn,index_)\n",
    "            \n",
    "#         F_energy = torch.sum(index_*dmn2)/feat_levelset.shape[0]/feat_levelset.shape[1]/feat_levelset.shape[2] # eq(8)\n",
    "        \n",
    "#         return F_energy\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx,grad_output):\n",
    "#         feature,label,dmn,index_ = ctx.saved_tensors\n",
    "#         index_ = torch.unsqueeze(index_,0)\n",
    "#         index_ = torch.unsqueeze(index_,3)\n",
    "#         F_diff = -0.5*index_*dmn # eq(9) \n",
    "#         diff = torch.irfft(F_diff,2,normalized = True, onesided = False)/feature.shape[0] # eq\n",
    "#         return None,Variable(-grad_output*diff),None,None,None\n",
    "    \n",
    "    \n",
    "# score1 = y_out[:,0,:,:] # prob for class target\n",
    "# score2 = (score1-0.5) # for energyloss\n",
    "\n",
    "# training_loss = self.loss(score2, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/rosanajurdi/Perimeter_loss/blob/master/losses.py\n",
    "# and https://github.com/jocpae/clDice/blob/master/cldice_loss/pytorch/soft_skeleton.py\n",
    "\n",
    "def contour(x):\n",
    "    minn1 = -F.max_pool2d(-x, (3,1), (1,1), (1,0) )\n",
    "    minn2 = -F.max_pool2d(-x, (1,3),(1,1), (0,1))\n",
    "    minn = torch.min(minn1, minn2)\n",
    "    maxx = F.max_pool2d(x, (3,3),(1,1), (1,1))\n",
    "    res = maxx-minn\n",
    "    return F.relu_(res)\n",
    "\n",
    "class ContourLoss(nn.Module):\n",
    "    def forward(self, pred, targ):\n",
    "        pred = zero_or_one(pred).float()\n",
    "        targ = targ.float()\n",
    "        \n",
    "#         b, _, w, h = pred.shape\n",
    "        b, w, h = pred.shape\n",
    "        \n",
    "#         cl_pred = contour(pred).sum(axis=(2,3))\n",
    "        cl_pred = contour(pred).sum(axis=(1,2))\n",
    "#         target_skeleton = contour(targ).sum(axis=(2,3))\n",
    "        target_skeleton = contour(targ).sum(axis=(1,2))\n",
    "        big_pen: Tensor = (cl_pred - target_skeleton) ** 2\n",
    "        contour_loss = big_pen / (w * h)\n",
    "    \n",
    "        return contour_loss.mean(axis=0)\n",
    "    \n",
    "contour_loss = ContourLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLoss(nn.Module):\n",
    "    def __init__(self, lambda_coeff=0.01):\n",
    "        super().__init__()\n",
    "        self.lambda_coeff = lambda_coeff\n",
    "    \n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        loss = torch.zeros(inputs.shape[0])\n",
    "        for i, targ in enumerate(targets):\n",
    "            if 1 in targ:\n",
    "                targ1 = targ.view(-1)\n",
    "                inp1 = inputs[i].view(-1)\n",
    "                loss1 = dice_loss(targ1, inp1)\n",
    "                loss2 = contour_loss(inputs[i], targ)\n",
    "                loss_tot = (1-self.lambda_coeff)*loss1 + self.lambda_coeff*loss2\n",
    "                loss[i] = loss_tot\n",
    "            else:\n",
    "                loss[i] = F.binary_cross_entropy(inputs[i].float(), targ.float(), reduction='mean')\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "adaptive_loss = AdaptiveLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def forward(self, pred, targ):\n",
    "        loss1 = hh_dtloss(pred, targ).cuda()\n",
    "        loss2 = adaptive_loss(pred, targ).cuda()\n",
    "        loss3 = perim_loss(pred, targ)\n",
    "#         return torch.mean(torch.stack((loss1*0.0001, loss2, loss3*1e-7)))\n",
    "        return loss1*0.0001 + loss2 + loss3*1e-7\n",
    "    \n",
    "combined_loss = CombinedLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annoyingly we cannot use mix precision fp16 with BCE loss. Otherwise we would slap .to_fp16() on the end of the learner\n",
    "\n",
    "learner = unet_learner(dls, resnet34, n_out=1, loss_func=adaptive_loss)\n",
    "# learner = unet_learner(dls, resnet34, n_out=1, loss_func=adaptive_loss)\n",
    "# learner = Learner(dls, test_model, loss_func=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.load(\"BASELINE_0.06-0.128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(config[\"epochs\"], lr=config[\"lr\"], cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learner.model, \"./models/test_export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = unet_learner(dls, resnet34, n_out=1, loss_func=combined_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load(\"./BASELINE_0.06-0.128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learner.model, \"./to_upload/baseline_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sett ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.cpu()\n",
    "test_data = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_data[0]\n",
    "test_y = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learner.model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = nn.Sigmoid()(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroOrOneFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        res = torch.where(input > 0.5, torch.tensor(1.0, requires_grad=True), torch.tensor(0.0, requires_grad=True))\n",
    "        ctx.save_for_backward(res)\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors\n",
    "        return input\n",
    "    \n",
    "zero_or_one = ZeroOrOneFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contour(x):\n",
    "#     '''\n",
    "#     Differenciable aproximation of contour extraction\n",
    "    \n",
    "#     '''   \n",
    "#     min_pool_x = torch.nn.functional.max_pool2d(x*-1, (3, 3), 1, 1)*-1\n",
    "#     max_min_pool_x = torch.nn.functional.max_pool2d(min_pool_x, (3, 3), 1, 1)\n",
    "#     contour = torch.nn.functional.relu(max_min_pool_x - min_pool_x)\n",
    "#     return contour\n",
    "\n",
    "\n",
    "# def soft_skeletonize(x, thresh_width=10):\n",
    "#     '''\n",
    "#     Differenciable aproximation of morphological skelitonization operaton\n",
    "#     thresh_width - maximal expected width of vessel\n",
    "#     '''\n",
    "#     for i in range(thresh_width):\n",
    "#         min_pool_x = torch.nn.functional.max_pool2d(x*-1, (3, 3), 1, 1)*-1\n",
    "#         max_min_pool_x = torch.nn.functional.max_pool2d(min_pool_x, (3, 3), 1, 1)\n",
    "#         contour = torch.nn.functional.relu(max_min_pool_x - min_pool_x)\n",
    "#         x = torch.nn.functional.relu(x - contour)\n",
    "#     return x\n",
    "\n",
    "def contour(x):\n",
    "    minn1 = -F.max_pool2d(-x, (3,1), (1,1), (1,0) )\n",
    "    minn2 = -F.max_pool2d(-x, (1,3),(1,1), (0,1))\n",
    "    minn = torch.min(minn1, minn2)\n",
    "    maxx = F.max_pool2d(x, (3,3),(1,1), (1,1))\n",
    "    res = maxx-minn\n",
    "    return F.relu_(res)\n",
    "\n",
    "\n",
    "class contour_loss():\n",
    "    '''\n",
    "    inputs shape  (batch, channel, height, width).\n",
    "    calculate clDice loss\n",
    "    Because pred and target at moment of loss calculation will be a torch tensors\n",
    "    it is preferable to calculate target_skeleton on the step of batch forming,\n",
    "    when it will be in numpy array format by means of opencv\n",
    "    '''\n",
    "        \n",
    "    def __call__(self, probs: Tensor, target: Tensor, ) -> Tensor:\n",
    "#         pc = probs[:, self.idc, ...].type(torch.float32)\n",
    "#         tc = target[:, self.idc, ...].type(torch.float32)\n",
    "\n",
    "        pc = zero_or_one(probs).float()\n",
    "        tc = target.float()\n",
    "\n",
    "        b, _, w, h = pc.shape\n",
    "        cl_pred = contour(pc).sum(axis=(2,3))\n",
    "        target_skeleton = contour(tc).sum(axis=(2,3))\n",
    "        big_pen: Tensor = (cl_pred - target_skeleton) ** 2\n",
    "        contour_loss = big_pen / (w * h)\n",
    "    \n",
    "        return contour_loss.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_formatted = zero_or_one(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_formatted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_loss()(test_preds_formatted, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a patch with a glom in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(test_preds_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 in test_y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(contour(test_preds_formatted[3]).detach().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(contour(test_y[3]).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.mse_loss(contour(test_preds[1]).sum(), contour(test_y[1]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(contour(test_preds[1]).sum() - contour(test_y[1]).sum())**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(test_y[1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
