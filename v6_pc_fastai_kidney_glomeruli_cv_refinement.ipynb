{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuBMAP - Hacking the Kidney - Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Kaggle competition page](https://www.kaggle.com/c/hubmap-kidney-segmentation)\n",
    "\n",
    "Helpful Notebooks:\n",
    "* [https://www.kaggle.com/markalavin/hubmap-tile-images-w-overlap-and-build-tfrecords](https://www.kaggle.com/markalavin/hubmap-tile-images-w-overlap-and-build-tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDO\n",
    "* Look at impact of different affine matrices\n",
    "* Look at impact of removing alpha channel on model size and performance\n",
    "* Add Deepmind's architecture optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Downloads for Offline use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda update -n base conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda config --set always_yes True\n",
    "# # ! conda install -c fastai -c pytorch fastai\n",
    "# # #! conda install pytorch torchvision torchaudio fastai -c pytorch\n",
    "# ! conda install pytorch torchvision torchaudio -c pytorch\n",
    "# # #! conda update pytorch torchvision torchaudio cudatoolkit -c pytorch\n",
    "# ! conda install pandas\n",
    "# ! conda install -c conda-forge kaggle\n",
    "# ! conda install -c conda-forge tifffile\n",
    "# ! conda install -c conda-forge tqdm\n",
    "# ! conda install -c conda-forge matplotlib\n",
    "# ! conda install -c conda-forge pytorch-lightning\n",
    "# ! conda install -c conda-forge wandb\n",
    "# ! conda install -c conda-forge arrow\n",
    "# !conda install -c conda-forge pickle5\n",
    "# !conda install -n base -c conda-forge jupyterlab_widgets\n",
    "# !conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -e git+https://github.com/fastai/fastai#egg=fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/fastai/fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Windows config only as per the documents https://pypi.org/project/pyvips/\n",
    "\n",
    "# !pip install --user pyvips\n",
    "\n",
    "# # this config enables vips to be used locally without adding to a busy list of system env variables\n",
    "import os\n",
    "vipshome = 'c:\\\\Users\\\\Craig\\\\Documents\\\\DevTools\\\\vips-dev-8.10\\\\bin'\n",
    "os.environ['PATH'] = vipshome + ';' + os.environ['PATH']\n",
    "\n",
    "# PROBABLY REQUIRES A RESTART AFTER pip install and then assigning env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install arrow pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed if running in wsl2\n",
    "#! pip install pytorch-lightning wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea why the conda-forge version doesn't work\n",
    "\n",
    "#!python -m pip install opencv-python\n",
    "\n",
    "# If you are running this notebook on a server (like Linux on WSL2) you need the headless version of opencv\n",
    "# The regular opencv requires GUI packages that serves dont have, and will raise an error\n",
    "#!python -m pip install opencv-python-headless\n",
    "\n",
    "# temporary solution to use tab complete - something wrong with jupyter jedi - need to downgrade\n",
    "#!pip install jedi==0.17.2\n",
    "\n",
    "# !pip install torchio --upgrade\n",
    "\n",
    "#!pip install pytorch-lightning-bolts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -e ./torchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the finicky local CUDA is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# prebuilt models\n",
    "# from pl_bolts.models import UNet\n",
    "\n",
    "import tensorboard as tb\n",
    "\n",
    "# Need to put kaggle.json in /%USERS%/.kaggle folder (C:/Users/Craig/.kaggle)\n",
    "try:\n",
    "    import kaggle\n",
    "except:\n",
    "    !echo '{\"username\":\"canadarmy\",\"key\": KAGGLEKEY}' > ~/.kaggle/kaggle.json\n",
    "    import kaggle\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from typing import Union\n",
    "\n",
    "# Read tiff images\n",
    "import tifffile\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torchio as tio\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import time\n",
    "import wandb\n",
    "import arrow\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Memory management tools\n",
    "import gc\n",
    "\n",
    "import timm\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "# have to add src.fastai in front if we installed locally\n",
    "# from src.fastai.fastai.vision.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastai.imports import *\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.metrics import Dice, Jaccard, JaccardCoeff\n",
    "\n",
    "import pickle5 as pickle\n",
    "# import pyvips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()\n",
    "data_path = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path/\"train.csv\").rename(columns={\"id\": \"img_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path/\"new_masks.pkl\", \"rb\") as fh:\n",
    "#     new_masks = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle.api.competition_download_files(\"hubmap-kidney-segmentation\", path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile(path/\"hubmap-kidney-segmentation.zip\", 'r') as zipref:\n",
    "#     zipref.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Main Functions\n",
    "################\n",
    "\n",
    "\n",
    "def rle2mask(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: encoding string value from csv\n",
    "    shape: (width,height) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    # return a list of starting pixels and a list of lengths\n",
    "    starts, lengths = [\n",
    "        np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])\n",
    "    ]\n",
    "    # subtract 1 from every starting pixel\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    # calculate a background of 0 (empty) with size defined by image\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    # replace every 0 within each range with 1\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = 1\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def mask2rle(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "# def get_id_by_index(index, df=train_df):\n",
    "#     return df.iloc[index]['img_id']\n",
    "\n",
    "def get_single_img(id, folder=\"train\"):\n",
    "    img = tifffile.imread(path/folder/(id+\".tiff\"))\n",
    "    if len(img.shape) == 5:\n",
    "        img = img.squeeze().transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "def show_single_img(id, **kwargs):\n",
    "    return plt.imshow(get_single_img(id), **kwargs)\n",
    "\n",
    "# def show_img_by_index(index, df=train_df):\n",
    "#     return plt.imshow(tifffile.imread(path/\"train\"/(train_df.iloc[TEST_IMAGE_INDEX]['id']+\".tiff\")))\n",
    "\n",
    "# def get_single_encs(id, df=train_df):\n",
    "#     return df[df['img_id'] == id]['encoding'].array[0]\n",
    "\n",
    "# def get_mask(id, df=train_df, folder=\"train\"):\n",
    "#     return rle2mask(\n",
    "#         get_single_encs(id, df=df),\n",
    "#         get_single_img(id, folder=folder).shape[::-1][1:]\n",
    "#     )\n",
    "\n",
    "def show_single_img_and_mask_by_id(id):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    mask = get_mask(id)\n",
    "    img = get_single_img(id)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image\", fontsize=18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, cmap=\"hot\", alpha=0.5)\n",
    "    plt.title(f\"Image + mask\", fontsize=18)    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, cmap=\"hot\")\n",
    "    plt.title(f\"Mask\", fontsize=18)    \n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def show_single_img_and_mask(subject: tio.data.subject.Subject, resize_to=50):\n",
    "    plt.figure(figsize=(120, 100))\n",
    "    \n",
    "    if not isinstance(subject, tio.data.subject.Subject):\n",
    "        raise TypeError(f\"The subject is required to be of type torchio.data.subject.Subject but you provided {type(subject)}\")\n",
    "    \n",
    "    img = subject[\"img\"][tio.DATA].squeeze().permute(1,2,0)\n",
    "    mask = subject[\"mask\"][tio.DATA].squeeze().unsqueeze(2)\n",
    "    \n",
    "    if resize_to:\n",
    "        img = resizer(img, scale=resize_to)\n",
    "        mask = resizer(mask, scale=resize_to)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image\", fontsize=18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, cmap=\"hot\", alpha=0.5)\n",
    "    plt.title(f\"Image + mask\", fontsize=18)    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, cmap=\"hot\")\n",
    "    plt.title(f\"Mask\", fontsize=18)    \n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def to_4d(img, input_chan_first=False, output_chan_first=True):\n",
    "    if not len(img.shape)==3:\n",
    "        raise ValueError(\"Function only converts 3D arrayto 4D array\")\n",
    "    return np.expand_dims(np.transpose(img, \n",
    "                   (0,1,2) if input_chan_first else (2,0,1)), \n",
    "                   3 if output_chan_first else 0)\n",
    "\n",
    "def to_3d(img, input_chan_first=True, output_chan_first=False):\n",
    "    if not len(img.shape)==4:\n",
    "        raise ValueError(\"Function only converts 4D arrayto 3D array\")\n",
    "    return np.transpose(img.squeeze(), (0,1,2) if output_chan_first else (1,2,0))\n",
    "\n",
    "def to_3chan(x, dim=1):\n",
    "    return torch.cat((x,x,x), dim=dim)\n",
    "\n",
    "def resizer(img, scale=5, show=False):\n",
    "    \"\"\"\n",
    "    Returns an smaller array of the same dimensions, but converts to 3D to allow for resizing\n",
    "    \"\"\"\n",
    "    scale_percent = scale # percent of original size\n",
    "    im_dims = (len(img.shape) == 4)\n",
    "    if im_dims:\n",
    "        img = to_3d(img)\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    img_reshaped = cv2.resize(img.numpy(), dim)\n",
    "    if show:\n",
    "        return plt.imshow(img_reshaped)\n",
    "    if im_dims:\n",
    "        return to_4d(img_reshaped)\n",
    "    return img_reshaped\n",
    "\n",
    "def squeeze_and_reshape(img_tensor, remove_alpha=False):\n",
    "    if not isinstance(img_tensor, torch.Tensor):\n",
    "        raise TypeError(\"Image needs to be a tensor\")\n",
    "    if len(img_tensor.shape) == 5:\n",
    "        img_tensor = img_tensor.squeeze().permute(2, 1, 0)\n",
    "    if img_tensor.shape[0] == 3:\n",
    "        img_tensor = img_tensor.permute(2, 1, 0)\n",
    "    img_tensor = img_tensor.unsqueeze(2).permute(3,1,0,2)\n",
    "    return img_tensor\n",
    "\n",
    "def to_pil(image):\n",
    "    # for \n",
    "    data = image.numpy().squeeze().T\n",
    "    data = data.astype(np.uint8)\n",
    "    image = Image.fromarray(data)\n",
    "    w, h = image.size\n",
    "    display(image)\n",
    "    print() \n",
    "    \n",
    "def normalize_array(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalized data\n",
    "    \"\"\"\n",
    "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/libvips/pyvips/blob/master/examples/pil-numpy-pyvips.py\n",
    "\n",
    "format_to_dtype = {\n",
    "    'uchar': np.uint8,\n",
    "    'char': np.int8,\n",
    "    'ushort': np.uint16,\n",
    "    'short': np.int16,\n",
    "    'uint': np.uint32,\n",
    "    'int': np.int32,\n",
    "    'float': np.float32,\n",
    "    'double': np.float64,\n",
    "    'complex': np.complex64,\n",
    "    'dpcomplex': np.complex128,\n",
    "}\n",
    "\n",
    "# map np dtypes to vips\n",
    "dtype_to_format = {\n",
    "    'uint8': 'uchar',\n",
    "    'int8': 'char',\n",
    "    'uint16': 'ushort',\n",
    "    'int16': 'short',\n",
    "    'uint32': 'uint',\n",
    "    'int32': 'int',\n",
    "    'float32': 'float',\n",
    "    'float64': 'double',\n",
    "    'complex64': 'complex',\n",
    "    'complex128': 'dpcomplex',\n",
    "}\n",
    "\n",
    "\n",
    "# numpy array to vips image\n",
    "def numpy2vips(a):\n",
    "    if len(a.shape) == 4:\n",
    "        _, height, width, bands = a.shape\n",
    "    elif len(a.shape) == 3:\n",
    "        height, width, bands = a.shape\n",
    "    elif len(a.shape) == 2:\n",
    "        a = np.expand_dims(a,axis=-1)\n",
    "        height, width, bands = a.shape\n",
    "    linear = a.reshape(width * height * bands)\n",
    "    vi = pyvips.Image.new_from_memory(linear.data, width, height, bands,\n",
    "                                      dtype_to_format[str(a.dtype)])\n",
    "    return vi\n",
    "\n",
    "\n",
    "# vips image to numpy array\n",
    "def vips2numpy(vi, dtype=None):\n",
    "    return np.ndarray(buffer=vi.write_to_memory(),\n",
    "                      dtype=dtype if dtype else format_to_dtype[vi.format],\n",
    "                      shape=[vi.bands, vi.height, vi.width]).astype(dtype=np.uint8)\n",
    "\n",
    "def vips2numpy_from_file(file_loc, dtype=None):\n",
    "    vips_im = pyvips.Image.vipsload(file_loc)\n",
    "    return vips2numpy(vips_im, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 0.25\n",
    "TILES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiff_thumbnail(img_path):\n",
    "    # This pc is so good we dont need to resize as it makes it slower\n",
    "    im = pyvips.Image.tiffload(img_path)\n",
    "    im_np = vips2numpy(im)\n",
    "    plt.imshow(im_np)\n",
    "    \n",
    "def vips_thumbnail_from_file(img_path):\n",
    "    im = pyvips.Image.vipsload(img_path).resize(0.1)\n",
    "    im_np = vips2numpy(im)\n",
    "    plt.imshow(im_np)\n",
    "    \n",
    "def vips_thumbnail_from_memory(vips_img):\n",
    "    im_np = vips2numpy(vips_img)\n",
    "    plt.imshow(im_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (path/\"train_formatted\").exists():\n",
    "    img_list = [(img, img.name) for img in (path/\"train_formatted\").glob(\"*.tiff\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all tiff images to 3 channels (bands)\n",
    "\n",
    "def vips_channel_sorter(img_list, save_dir=\"./train_formatted/\"):\n",
    "    \"\"\"\n",
    "    Many of the images need to have their dimensions reorganised to be consistent format\n",
    "    If the vips image only has 1 band (meaning channel), if extracts the channels from another dimension\n",
    "    and places it in the correct bands dimension\n",
    "    \"\"\"\n",
    "    for img, img_id in img_list:\n",
    "        im = pyvips.Image.tiffload(img.as_posix())\n",
    "\n",
    "        # could also use im.get(\"n-pages\") to distinguish which images needed formatting but this wasn't working for me on linux\n",
    "        # therefore we assume 3 pages instead of making this dynamic based on number of pages\n",
    "\n",
    "        if im.bands == 1:\n",
    "            # the first image to show is always the green one if image has separate pages. Therefore just stack the next 2 pages on it\n",
    "            # print(im.get(\"image-description\"), im.get(\"n-pages\"))\n",
    "            im = pyvips.Image.tiffload(img.as_posix(), page=0)\n",
    "            for i in range(1,3):\n",
    "                band = pyvips.Image.tiffload(img.as_posix(), page=i)\n",
    "                im = im.bandjoin(band)\n",
    "\n",
    "            # the next key is to change the colourspace - helped by this issue https://github.com/libvips/pyvips/issues/151\n",
    "            im_srgb = im.copy(interpretation=\"srgb\")\n",
    "            im_srgb.write_to_file(f\"{save_dir}{img_id.split('.')[0]+'.tiff'}\")\n",
    "        else:\n",
    "            im = pyvips.Image.tiffload(img.as_posix())\n",
    "            im.write_to_file(f\"{save_dir}{img_id.split('.')[0]+'.tiff'}\")\n",
    "            \n",
    "# vips_channel_sorter(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace background with another intergerto use the mask as both a mask and a sampling probibility map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle2vips(mask_rle, shape: tuple, dtype=None):\n",
    "    \"\"\"\n",
    "    Expands on the rle2mask\n",
    "    Shape is (width, height)\n",
    "    \"\"\"\n",
    "    return numpy2vips(rle2mask(mask_rle, shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sampling map using the mask data\n",
    "\n",
    "def sample_mask_creator(img_loc: Path, save_dir: Path=path/\"sample\", num_tiles=TILES):\n",
    "    \"\"\"\n",
    "    Function taks a vips image, its corresponding mask and generates a set of vips files - mask and sample\n",
    "    Both img and mask need to be in vips format\n",
    "    \"\"\"\n",
    "    \n",
    "    img_id = img_loc.name.split(\".\")[0]\n",
    "    \n",
    "    img = pyvips.Image.tiffload((path/\"train_formatted\"/img_loc.name).as_posix())\n",
    "    \n",
    "    mask = rle2vips(\n",
    "        train_df[train_df[\"img_id\"] == img_id][\"encoding\"].array[0],\n",
    "        (img.width, img.height),\n",
    "        np.uint8\n",
    "    )\n",
    "    \n",
    "    tile_width = img.width//num_tiles\n",
    "    tile_height = img.height//num_tiles\n",
    "    \n",
    "#     sample_vips_holder = []\n",
    "\n",
    "    for y in range(num_tiles):\n",
    "        for x in range(num_tiles):\n",
    "            img_cropped = img.crop(x*tile_width, y*tile_height, tile_width, tile_height)\n",
    "            mask_cropped = mask.crop(x*tile_width, y*tile_height, tile_width, tile_height)\n",
    "            mask_cropped_numpy = vips2numpy(mask_cropped)\n",
    "            img_h = img_cropped.sRGB2HSV().extract_band(1)\n",
    "            if vips2numpy(img_h).mean() < 8.0:\n",
    "                sample_cropped = numpy2vips(np.full((img_cropped.height, img_cropped.width), 1, dtype=np.uint8))\n",
    "                sample_cropped.write_to_file(f\"sample/{img_id}-y{y}x{x}_sample.tiff\")\n",
    "            elif 1 in mask_cropped_numpy:\n",
    "                sample_cropped = numpy2vips(np.full((img_cropped.height, img_cropped.width), 15, dtype=np.uint8))\n",
    "                sample_cropped.write_to_file(f\"sample/{img_id}-y{y}x{x}_sample.tiff\")\n",
    "            else:\n",
    "                sample_cropped = numpy2vips(np.full((img_cropped.height, img_cropped.width), 10, dtype=np.uint8))\n",
    "                sample_cropped.write_to_file(f\"sample/{img_id}-y{y}x{x}_sample.tiff\")\n",
    "            if vips2numpy(img_h).mean() > 8.0:\n",
    "                img_cropped.write_to_file(f\"train_sections/{img_id}-y{y}x{x}_sections.tiff\")\n",
    "            np.save(f\"mask/{img_id}-y{y}x{x}_masks.npy\", mask_cropped_numpy)\n",
    "            mask_cropped.write_to_file(f\"mask/{img_id}-y{y}x{x}_masks.tiff\")\n",
    "#     new_mask = pyvips.Image.arrayjoin(mask_vips_holder, across=num_tiles)\n",
    "#     new_mask.vipssave((save_dir/f\"{vips_img_id}_sample.vips\").as_posix())\n",
    "    \n",
    "    print(f\"Saved {img_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use function above to create new masks for all training images\n",
    "# can also use the function above to remove junk images with certainly no glomeruli\n",
    "\n",
    "# for img, img_name in img_list:\n",
    "#     sample_mask_creator(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a baseline model that looked to have good accuracy but now we want to refine it. We will do the following:\n",
    "\n",
    "1. Update the colour normalisation\n",
    "2. Add blur and artifacts periodically - also update the frequency with which these transformations take place\n",
    "3. Update the loss function\n",
    "4. Patch overlap for TorchIO [ONLY DONE IN INFERENCE]\n",
    "5. Try something like TransUNET\n",
    "6. Try training on smaller patch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of images with and without "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Image Stats (and remove useless images - mean<100 - already done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could have done the removal of images a lot easier with **weighted dataloaders** (see [this Kaggle notebook](https://www.kaggle.com/dienhoa/healthy-lung-classification-spectrogram-fast-ai) for an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = []\n",
    "mean = tensor(153.1368)\n",
    "# std = []\n",
    "std = tensor(37.1318)\n",
    "\n",
    "# for _, im in img_list:\n",
    "#     print(im)\n",
    "#     for y in range(20):\n",
    "#         for x in range(20):\n",
    "#             img = vips2numpy(pyvips.Image.tiffload(f\"train_sections/{im.split('.')[0]}-y{y}x{x}_sections.tiff\"))\n",
    "#             if img.mean() < 100:\n",
    "#                 (path/f\"train_sections/{im.split('.')[0]}-y{y}x{x}_sections.tiff\").unlink()\n",
    "#             mean.append(img.mean())\n",
    "#             std.append(img.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (path/\"has_glom.csv\").exists():\n",
    "    list_holder = []\n",
    "\n",
    "    for im in (path/\"train_sections\").glob(\"*.tiff\"):\n",
    "        mask = np.load(path/\"mask\"/f\"{im.name.split('_')[0]}_masks.npy\")\n",
    "        if 1 in mask:\n",
    "            list_holder.append((im.name, 1, 1.5))\n",
    "        else:\n",
    "            list_holder.append((im.name, 0))\n",
    "    has_glom = pd.DataFrame(list_holder, columns=[\"img\", \"has_glom\"])\n",
    "    has_glom.to_csv(path/\"has_glom.csv\")\n",
    "else:\n",
    "    has_glom = pd.read_csv(path/\"has_glom.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_glom[\"has_glom\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all file names in folders of interest\n",
    "# we know thaqt file names are the same regardless of wther the file is in masks or images directory\n",
    "\n",
    "# img_dirs = [folder.name for folder in data_path.ls() if \"images\" in folder.name]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale01(Transform):\n",
    "    def encodes(self, x):\n",
    "        return tio.RescaleIntensity(out_min_max=(0,1))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Printer(Transform):\n",
    "    def encodes(self, x):\n",
    "        print(\"Hello\")\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [\n",
    "#     Normalize.from_stats(mean, std),\n",
    "    Rotate(p=0.5, pad_mode=\"zeros\"),\n",
    "    Flip(p=0.25, pad_mode=\"zeros\"),\n",
    "#     Zoom(min_zoom=0.25, max_zoom=2, pad_mode=\"zeros\"),\n",
    "    Warp(pad_mode=\"zeros\"),\n",
    "    Brightness(max_lighting=0.4),\n",
    "#     DihedralItem(),\n",
    "    Contrast(),\n",
    "    Hue(max_hue=0.3),\n",
    "    Saturation(p=0.1),\n",
    "    RandomErasing(p=0.5, sl=0.1, sh=0.1, min_aspect=0.1, max_count=10),\n",
    "\n",
    "]\n",
    "\n",
    "splitter = RandomSplitter()\n",
    "\n",
    "gloms = DataBlock(blocks=(ImageBlock, MaskBlock),\n",
    "                  get_items=get_image_files,\n",
    "                  splitter=RandomSplitter(),\n",
    "                  get_y=lambda o: path/\"mask\"/f\"{o.name.split('_')[0]}_masks.tiff\",\n",
    "                  item_tfms=[RandomResizedCrop(256, min_scale=0.1, max_scale=2)],\n",
    "                  batch_tfms=setup_aug_tfms(tfms),\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = gloms.datasets(path/\"train_sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_prob = 1\n",
    "wgts_raw = [(has_glom[has_glom[\"img\"] == img.name][\"has_glom\"].values[0]*4)+1 for img in dsets.train.items]\n",
    "wgts_total_sum = np.array(wgts_raw).sum()\n",
    "wgts = [wgt/wgts_total_sum for wgt in wgts_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(wgts).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = gloms.dataloaders(path/\"train_sections\", bs=12, num_workers=0, dl_type=WeightedDL, wgts=wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of images with a positive example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "counter = 1765\n",
    "\n",
    "\n",
    "# for _, im in img_list:\n",
    "#     print(im)\n",
    "#     for y in range(20):\n",
    "#         for x in range(20):\n",
    "#             try:\n",
    "#                 mask = np.load(f\"mask/{im.split('.')[0]}-y{y}x{x}_masks.npy\")\n",
    "#                 if 1 in mask:\n",
    "#                     counter += 1\n",
    "#                     continue\n",
    "#             except:\n",
    "#                 continue   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter / len([img for img in (path/\"train_sections\").glob(\"*.tiff\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function to enable gradients on `torch.where`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroOrOneFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        res = torch.where(input > 0.5, torch.tensor(1.0, requires_grad=True).cuda(), torch.tensor(0.0, requires_grad=True).cuda())\n",
    "        ctx.save_for_backward(res)\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors\n",
    "        return input\n",
    "    \n",
    "zero_or_one = ZeroOrOneFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.one_batch()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai callbacks\n",
    "\n",
    "class PrinterCallback(Callback):\n",
    "    \"\"\"\n",
    "    Snaps image of x, y, and preds every specified number of batches\n",
    "    Saves images to path specified\n",
    "    \"\"\"\n",
    "    def __init__(self, path, img_freq=105):\n",
    "        self.img_freq = img_freq\n",
    "        self.path = path\n",
    "    def after_batch(self):\n",
    "        if self.iter % self.img_freq == 0:\n",
    "            img_list = []\n",
    "            with torch.no_grad():\n",
    "                for i in range(self.pred.shape[0]):\n",
    "                    # Note the sigmoid is like a \"renormalisation\" hack to put values between 0 and 1 to then compare with binary masks\n",
    "                    # otherwise the transforms distort the output (I think mean is around 0). I'm sure we could call 'decodes' on some of \n",
    "                    # these transforms but this is easier\n",
    "                    x = torch.sigmoid(tensor(self.x[i,...].cpu().numpy()))\n",
    "                    y = to_3chan(tensor(self.y[i,...].cpu().unsqueeze(0).numpy()), 0)\n",
    "                    pred = to_3chan(tensor(self.pred[i,...].cpu().numpy()), 0)\n",
    "                    img_list.append(x)\n",
    "                    img_list.append(y)\n",
    "                    img_list.append(pred)\n",
    "\n",
    "                grid = torchvision.utils.make_grid(\n",
    "                    img_list,\n",
    "                    nrow=3,\n",
    "#                     normalize=True,\n",
    "                )\n",
    "                self._save(self.path, grid, self.epoch, self.iter, round(self.loss.item(), 3))\n",
    "                \n",
    "        #print(f\"The learning rate is {self.opt.hypers[0]['lr']}\")\n",
    "        #print({self.dls.valid.subjects_dataset._transform})\n",
    "        \n",
    "    @staticmethod\n",
    "    def _save(img_path, img, epoch, batch, loss):\n",
    "        npimg = normalize_array(img.cpu().detach().float().numpy())\n",
    "        plt.imsave(img_path/f\"epoch{epoch}batch{batch}__{loss}.png\", np.transpose(npimg, (1,2,0)))\n",
    "        \n",
    "class ConvertY(Callback):\n",
    "    \"\"\"\n",
    "    Since we used TorchIO to sample the data, we first need to convert the y back to its normal values\n",
    "    \"\"\"\n",
    "    def before_batch(self):\n",
    "        \"\"\"\n",
    "        NOTE: as per the docs, you can only assign to `yb`, not `y`\n",
    "        `yb` is a tuple (which is immutable) therefore you must override the `self.learn.yb` - note we are assigning to to `learn.yb`\n",
    "        \"\"\"\n",
    "        #self.yb = tuple([torch.where(self.y != torch.tensor(1).cuda(), torch.tensor(0).cuda(), torch.tensor(1).cuda())])\n",
    "        self.learn.yb = tuple([torch.where(self.y != torch.tensor(1).cuda(), torch.tensor(0).cuda(), torch.tensor(1).cuda())])\n",
    "        #print(self.yb[0].shape)\n",
    "        #print(len(self.yb))\n",
    "        \n",
    "    #def after_pred(self):\n",
    "        # To check to see that the overwritten values of y did change\n",
    "        #print(self.y)\n",
    "        #print(dir(self))\n",
    "        \n",
    "\n",
    "        \n",
    "## NOTE: this may not be needed anymore now that our loss function cobines this step        \n",
    "class AddSigmoidActivation(Callback):\n",
    "    \"\"\"\n",
    "    Change the output to add a Sigmoid function \n",
    "    Needed since:\n",
    "        a) Using a pretrained Resnet model that doesn't support adding a final activation layer\n",
    "        b) unlike `cnn_learner`, a `unet_learner` doesn't have the `custom_head` parameter (which the forums suggest is an option to effectively add a layer to a pretarined model)\n",
    "    Note: need to check if `learner.model[-1].add_module` would work if you subclassed `nn.Module` and created a `forward()` method that added this activation?\n",
    "    \"\"\"\n",
    "    def after_pred(self):\n",
    "        \"\"\"\n",
    "        As per the documentation, this callback hook is specifically designed for modifying the outputs BEFORE theyre sent to the loss function\n",
    "        Thus it is a perfect place to add our sigmoid function to the outputs\n",
    "        \"\"\"\n",
    "        self.learn.pred = nn.Sigmoid()(self.pred)\n",
    "#         self.learn.pred = zero_or_one(nn.Sigmoid()(self.pred))\n",
    "        \n",
    "class ProgressiveTransformsUpdateCallback(Callback):\n",
    "    def before_epoch(self):\n",
    "        if self.epoch < 4:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((4,4,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((4,4,1)), custom_reshape, custom_normalization,])\n",
    "            )\n",
    "            #for h in self.opt.hypers:\n",
    "            #    h[\"lr\"] = 0.00001\n",
    "        if 3 < self.epoch < 8:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((2,2,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((2,2,1)), custom_reshape, custom_normalization,])\n",
    "            )\n",
    "            for h in self.opt.hypers:\n",
    "                h[\"lr\"] = 0.00001\n",
    "        if self.epoch > 7:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([custom_reshape, custom_normalization,])\n",
    "            )\n",
    "        #print(self.data.dataset.subjects_dataset.dry_iter())\n",
    "        \n",
    "class ModifyTransformsCallback(Callback):\n",
    "    \"\"\"\n",
    "    Train and Valid transforms must each be a list of transforms wrapped in tio.Compose\n",
    "    \"\"\"\n",
    "    def __init__(self, train_callbacks, valid_callbacks):\n",
    "        self.train_callbacks = train_callbacks\n",
    "        self.valid_callbacks = valid_callbacks\n",
    "        \n",
    "    def before_epoch(self):\n",
    "        self.dls.train.subjects_dataset.set_transform(self.train_callbacks)\n",
    "        self.dls.valid.subjects_dataset.set_transform(self.valid_callbacks)\n",
    "        \n",
    "class UpsamplePredCallback(Callback):\n",
    "    \"\"\"\n",
    "    If we downsampled the x-values, the preds will be the same resolution. Therefore we need to upsample to the size the y-value (masks) expect\n",
    "    \"\"\"\n",
    "    def __init__(self, upscale):\n",
    "        self.upscale = upscale\n",
    "        \n",
    "    def after_pred(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class SwitchLossCallback(Callback):\n",
    "    def after_pred(self):\n",
    "        if self.iter > round(self.n_iter / 2):\n",
    "            self.learn.loss_func = hh_dtloss\n",
    "            \n",
    "class UpdateContourLoss(Callback):\n",
    "    def before_epoch(self):\n",
    "        if self.epoch % 4 == 0 and self.epoch != 0:\n",
    "            prior_lambda_coeff = self.learn.loss_func.lambda_coeff\n",
    "            self.learn.loss_func = AdaptiveLoss(prior_lambda_coeff+0.01)\n",
    "            print(f\"Lambda coefficient updated to {prior_lambda_coeff + 0.01}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    \"resnet\",\n",
    "    \"transforms\",\n",
    "    \"custom_loss\",\n",
    "    \"reduceLR\",\n",
    "    \"150_samples\"\n",
    "]\n",
    "\n",
    "group = \"resnet\"\n",
    "\n",
    "notes = \"Perimeter loss\"\n",
    "\n",
    "name = \"perimeter_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 150,\n",
    "    \"transforms\": \"all\",\n",
    "    \"loss\": \"perimeter_loss\",\n",
    "    \"lr\": 1e-5,\n",
    "    \"model\": \"resnet34\",\n",
    "    \"train_type\": \"fit\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project='HuBMAP_model_experiments', entity='stantonius', name=name, tags=tags, group=group, notes=notes, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks \n",
    "\n",
    "#os.environ['WANDB_MODE'] = 'dryrun'\n",
    "# wandb_callback = WandbCallback(log='all')\n",
    "path = Path()\n",
    "\n",
    "model_name = arrow.utcnow().format(\"DDMMMYY\") + \"_\" + name\n",
    "save_model_callback = SaveModelCallback(fname=model_name, every_epoch=True)\n",
    "save_image_path = path/\"training_image_logs\"\n",
    "\n",
    "\n",
    "\n",
    "# updated_train_transforms = tio.Compose([tio.Resample((4,4,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "# updated_valid_transforms = tio.Compose([tio.Resample((4,4,1)), custom_reshape, custom_normalization,])\n",
    "\n",
    "# updated_transforms = ModifyTransformsCallback(updated_train_transforms, updated_valid_transforms)\n",
    "reduce_lr = ReduceLROnPlateau(min_delta=0.2, patience=75)\n",
    "\n",
    "cbs=[PrinterCallback(save_image_path, img_freq=100), ConvertY(), AddSigmoidActivation(), save_model_callback, UpdateContourLoss(), reduce_lr]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs) \n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        \n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "    \n",
    "dice_loss = DiceLoss()    \n",
    "    \n",
    "class DiceBCELoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs)  \n",
    "\n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        \n",
    "        # Note changed before from binary_cross_entropy to binary_cross_entropy_with_logits\n",
    "        # got an error\n",
    "        # However this step requires us to combine our sigmoid layer\n",
    "        BCE = F.binary_cross_entropy(inputs.float(), targets.float(), reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE\n",
    "    \n",
    "bce_dice_loss = DiceBCELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.ndimage.morphology import distance_transform_edt as edt\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "\"\"\"\n",
    "Hausdorff loss implementation based on paper:\n",
    "https://arxiv.org/pdf/1904.10030.pdf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class HausdorffDTLoss(nn.Module):\n",
    "    \"\"\"Binary Hausdorff loss based on distance transform\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=2.0, **kwargs):\n",
    "        super(HausdorffDTLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def distance_field(self, img: np.ndarray) -> np.ndarray:\n",
    "        field = np.zeros_like(img)\n",
    "\n",
    "        for batch in range(len(img)):\n",
    "            fg_mask = img[batch] > 0.5\n",
    "\n",
    "            if fg_mask.any():\n",
    "                bg_mask = ~fg_mask\n",
    "\n",
    "                fg_dist = edt(fg_mask)\n",
    "                bg_dist = edt(bg_mask)\n",
    "\n",
    "                field[batch] = fg_dist + bg_dist\n",
    "\n",
    "        return field\n",
    "\n",
    "    def forward(\n",
    "        self, pred: torch.Tensor, target: torch.Tensor, debug=False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Uses one binary channel: 1 - fg, 0 - bg\n",
    "        pred: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        target: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        \"\"\"\n",
    "        assert pred.dim() == 4 or pred.dim() == 5, \"Only 2D and 3D supported\"\n",
    "        assert (\n",
    "            pred.dim() == target.dim()\n",
    "        ), \"Prediction and target need to be of same dimension\"\n",
    "\n",
    "        # pred = torch.sigmoid(pred)\n",
    "\n",
    "        pred_dt = torch.from_numpy(self.distance_field(pred.detach().cpu().numpy())).float()\n",
    "        target_dt = torch.from_numpy(self.distance_field(target.detach().cpu().numpy())).float()\n",
    "\n",
    "        pred_error = (pred - target) ** 2\n",
    "        distance = pred_dt ** self.alpha + target_dt ** self.alpha\n",
    "\n",
    "        dt_field = pred_error.cpu() * distance\n",
    "        loss = dt_field.mean()\n",
    "\n",
    "        if debug:\n",
    "            return (\n",
    "                loss.cpu().numpy(),\n",
    "                (\n",
    "                    dt_field.cpu().numpy()[0, 0],\n",
    "                    pred_error.cpu().numpy()[0, 0],\n",
    "                    distance.cpu().numpy()[0, 0],\n",
    "                    pred_dt.cpu().numpy()[0, 0],\n",
    "                    target_dt.cpu().numpy()[0, 0],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "class HausdorffERLoss(nn.Module):\n",
    "    \"\"\"Binary Hausdorff loss based on morphological erosion\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=2.0, erosions=10, **kwargs):\n",
    "        super(HausdorffERLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.erosions = erosions\n",
    "        self.prepare_kernels()\n",
    "\n",
    "    def prepare_kernels(self):\n",
    "        cross = np.array([cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))])\n",
    "        bound = np.array([[[0, 0, 0], [0, 1, 0], [0, 0, 0]]])\n",
    "\n",
    "        self.kernel2D = cross * 0.2\n",
    "        self.kernel3D = np.array([bound, cross, bound]) * (1 / 7)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perform_erosion(\n",
    "        self, pred: np.ndarray, target: np.ndarray, debug\n",
    "    ) -> np.ndarray:\n",
    "        bound = (pred - target) ** 2\n",
    "\n",
    "        if bound.ndim == 5:\n",
    "            kernel = self.kernel3D\n",
    "        elif bound.ndim == 4:\n",
    "            kernel = self.kernel2D\n",
    "        else:\n",
    "            raise ValueError(f\"Dimension {bound.ndim} is nor supported.\")\n",
    "\n",
    "        eroted = np.zeros_like(bound)\n",
    "        erosions = []\n",
    "\n",
    "        for batch in range(len(bound)):\n",
    "\n",
    "            # debug\n",
    "            erosions.append(np.copy(bound[batch][0]))\n",
    "\n",
    "            for k in range(self.erosions):\n",
    "\n",
    "                # compute convolution with kernel\n",
    "                dilation = convolve(bound[batch], kernel, mode=\"constant\", cval=0.0)\n",
    "\n",
    "                # apply soft thresholding at 0.5 and normalize\n",
    "                erosion = dilation - 0.5\n",
    "                erosion[erosion < 0] = 0\n",
    "\n",
    "                if erosion.ptp() != 0:\n",
    "                    erosion = (erosion - erosion.min()) / erosion.ptp()\n",
    "\n",
    "                # save erosion and add to loss\n",
    "                bound[batch] = erosion\n",
    "                eroted[batch] += erosion * (k + 1) ** self.alpha\n",
    "\n",
    "                if debug:\n",
    "                    erosions.append(np.copy(erosion[0]))\n",
    "\n",
    "        # image visualization in debug mode\n",
    "        if debug:\n",
    "            return eroted, erosions\n",
    "        else:\n",
    "            return eroted\n",
    "\n",
    "    def forward(\n",
    "        self, pred: torch.Tensor, target: torch.Tensor, debug=False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Uses one binary channel: 1 - fg, 0 - bg\n",
    "        pred: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        target: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        \"\"\"\n",
    "        assert pred.dim() == 4 or pred.dim() == 5, \"Only 2D and 3D supported\"\n",
    "        assert (\n",
    "            pred.dim() == target.dim()\n",
    "        ), \"Prediction and target need to be of same dimension\"\n",
    "\n",
    "        # pred = torch.sigmoid(pred)\n",
    "\n",
    "        if debug:\n",
    "            eroted, erosions = self.perform_erosion(\n",
    "                pred.cpu().numpy(), target.cpu().numpy(), debug\n",
    "            )\n",
    "            return eroted.mean(), erosions\n",
    "\n",
    "        else:\n",
    "            eroted = torch.from_numpy(\n",
    "                self.perform_erosion(pred, target, debug)\n",
    "            ).float()\n",
    "\n",
    "            loss = eroted.mean()\n",
    "\n",
    "            return loss\n",
    "        \n",
    "hh_erloss = HausdorffERLoss()\n",
    "hh_dtloss = HausdorffDTLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import Variable\n",
    "# from torch.autograd.function import Function\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# def odd_flip(H):\n",
    "#     '''\n",
    "#     generate frequency map\n",
    "#     when height or width of image is odd number,\n",
    "#     creat a array concol [0,1,...,int(H/2)+1,int(H/2),...,0]\n",
    "#     len(concol) = H\n",
    "#     '''\n",
    "#     m = int(H/2)\n",
    "#     col = np.arange(0,m+1)\n",
    "#     flipcol = col[m-1::-1]\n",
    "#     concol = np.concatenate((col,flipcol),0)\n",
    "#     return concol\n",
    "\n",
    "# def even_flip(H):\n",
    "#     '''\n",
    "#     generate frequency map\n",
    "#     when height or width of image is even number,\n",
    "#     creat a array concol [0,1,...,int(H/2),int(H/2),...,0]\n",
    "#     len(concol) = H\n",
    "#     '''\n",
    "#     m = int(H/2)\n",
    "#     col = np.arange(0,m)\n",
    "#     flipcol = col[m::-1]\n",
    "#     concol = np.concatenate((col,flipcol),0)\n",
    "#     return concol\n",
    "\n",
    "# def dist(target):\n",
    "#     '''\n",
    "#     sqrt(m^2 + n^2) in eq(8)\n",
    "#     '''\n",
    "\n",
    "#     _,_,H,W = target.shape\n",
    "\n",
    "#     if H%2 ==1:\n",
    "#         concol = odd_flip(H)\n",
    "#     else:\n",
    "#         concol = even_flip(H)\n",
    "        \n",
    "#     if W%2 == 1:\n",
    "#         conrow = odd_flip(W)\n",
    "#     else:\n",
    "#         conrow = even_flip(W)\n",
    "        \n",
    "#     m_col = concol[:,np.newaxis] \n",
    "#     m_row = conrow[np.newaxis,:]\n",
    "#     dist = np.sqrt(m_col*m_col + m_row*m_row) # sqrt(m^2+n^2)\n",
    "  \n",
    "#     use_cuda = torch.cuda.is_available()\n",
    "#     if use_cuda:\n",
    "#         dist_ = torch.from_numpy(dist).float().cuda()\n",
    "#     else:\n",
    "#         dist_ = torch.from_numpy(dist).float()\n",
    "#     return dist_\n",
    "\n",
    "# class EnergyLoss(nn.Module):\n",
    "#     def __init__(self,cuda,alpha,sigma):\n",
    "#         super(EnergyLoss, self).__init__()\n",
    "#         self.energylossfunc = EnergylossFunc.apply\n",
    "#         self.alpha = alpha\n",
    "#         self.cuda = cuda\n",
    "#         self.sigma = sigma\n",
    "\n",
    "#     def forward(self,feat,label):\n",
    "#         return self.energylossfunc(self.cuda,feat, label,self.alpha,self.sigma)\n",
    "    \n",
    "# class EnergylossFunc(Function):\n",
    "#     '''\n",
    "#     target: ground truth \n",
    "#     feat: Z -0.5. Z：prob of your target class(here is vessel) with shape[B,H,W]. \n",
    "#     Z from softmax output of unet with shape [B,C,H,W]. C: number of classes\n",
    "#     alpha: default 0.35\n",
    "#     sigma: default 0.25\n",
    "#     '''\n",
    "#     @staticmethod\n",
    "#     def forward(ctx,cuda,feat_levelset,target,alpha,sigma,Gaussian = False):\n",
    "#         hardtanh = nn.Hardtanh(min_val=0, max_val=1, inplace=False)\n",
    "#         target = target.float()\n",
    "#         index_ = dist(target)\n",
    "#         dim_ = target.shape[1]\n",
    "#         target = torch.squeeze(target,1)\n",
    "#         I1 = target + alpha*hardtanh(feat_levelset/sigma) # G_t + alpha*H(phi) in eq(5)\n",
    "#         dmn = torch.rfft(I1,2,normalized = True, onesided = False)\n",
    "#         dmn_r = dmn[:,:,:,0] # dmn's real part\n",
    "#         dmn_i = dmn[:,:,:,1] # dmm's imagine part\n",
    "#         dmn2 = dmn_r * dmn_r + dmn_i * dmn_i # dmn^2\n",
    "\n",
    "#         ctx.save_for_backward(feat_levelset,target,dmn,index_)\n",
    "            \n",
    "#         F_energy = torch.sum(index_*dmn2)/feat_levelset.shape[0]/feat_levelset.shape[1]/feat_levelset.shape[2] # eq(8)\n",
    "        \n",
    "#         return F_energy\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx,grad_output):\n",
    "#         feature,label,dmn,index_ = ctx.saved_tensors\n",
    "#         index_ = torch.unsqueeze(index_,0)\n",
    "#         index_ = torch.unsqueeze(index_,3)\n",
    "#         F_diff = -0.5*index_*dmn # eq(9) \n",
    "#         diff = torch.irfft(F_diff,2,normalized = True, onesided = False)/feature.shape[0] # eq\n",
    "#         return None,Variable(-grad_output*diff),None,None,None\n",
    "    \n",
    "    \n",
    "# score1 = y_out[:,0,:,:] # prob for class target\n",
    "# score2 = (score1-0.5) # for energyloss\n",
    "\n",
    "# training_loss = self.loss(score2, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/rosanajurdi/Perimeter_loss/blob/master/losses.py\n",
    "# and https://github.com/jocpae/clDice/blob/master/cldice_loss/pytorch/soft_skeleton.py\n",
    "\n",
    "def contour(x):\n",
    "    minn1 = -F.max_pool2d(-x, (3,1), (1,1), (1,0) )\n",
    "    minn2 = -F.max_pool2d(-x, (1,3),(1,1), (0,1))\n",
    "    minn = torch.min(minn1, minn2)\n",
    "    maxx = F.max_pool2d(x, (3,3),(1,1), (1,1))\n",
    "    res = maxx-minn\n",
    "    return F.relu_(res)\n",
    "\n",
    "class ContourLoss(nn.Module):\n",
    "    def forward(self, pred, targ):\n",
    "        pred = zero_or_one(pred).float()\n",
    "        targ = targ.float()\n",
    "        \n",
    "#         b, _, w, h = pred.shape\n",
    "        b, w, h = pred.shape\n",
    "        \n",
    "#         cl_pred = contour(pred).sum(axis=(2,3))\n",
    "        cl_pred = contour(pred).sum(axis=(1,2))\n",
    "#         target_skeleton = contour(targ).sum(axis=(2,3))\n",
    "        target_skeleton = contour(targ).sum(axis=(1,2))\n",
    "        big_pen: Tensor = (cl_pred - target_skeleton) ** 2\n",
    "        contour_loss = big_pen / (w * h)\n",
    "    \n",
    "        return contour_loss.mean(axis=0)\n",
    "    \n",
    "contour_loss = ContourLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLoss(nn.Module):\n",
    "    def __init__(self, lambda_coeff=0.01):\n",
    "        super().__init__()\n",
    "        self.lambda_coeff = lambda_coeff\n",
    "    \n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        loss = torch.zeros(inputs.shape[0])\n",
    "        for i, targ in enumerate(targets):\n",
    "            if 1 in targ:\n",
    "                targ1 = targ.view(-1)\n",
    "                inp1 = inputs[i].view(-1)\n",
    "                loss1 = dice_loss(targ1, inp1)\n",
    "                loss2 = contour_loss(inputs[i], targ.unsqueeze(0))\n",
    "                loss_tot = (1-self.lambda_coeff)*loss1 + self.lambda_coeff*loss2\n",
    "                loss[i] = loss_tot\n",
    "            else:\n",
    "                loss[i] = F.binary_cross_entropy(inputs[i].float(), targ.unsqueeze(0).float(), reduction='mean')\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "adaptive_loss = AdaptiveLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def forward(self, pred, targ):\n",
    "        loss1 = hh_dtloss(pred, targ).cuda()\n",
    "        loss2 = adaptive_loss(pred, targ).cuda()\n",
    "        loss3 = perim_loss(pred, targ)\n",
    "#         return torch.mean(torch.stack((loss1*0.0001, loss2, loss3*1e-7)))\n",
    "        return loss1*0.0001 + loss2 + loss3*1e-7\n",
    "    \n",
    "combined_loss = CombinedLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annoyingly we cannot use mix precision fp16 with BCE loss. Otherwise we would slap .to_fp16() on the end of the learner\n",
    "\n",
    "learner = unet_learner(dls, resnet34, n_out=1, loss_func=adaptive_loss)\n",
    "# learner = unet_learner(dls, resnet34, n_out=1, loss_func=adaptive_loss)\n",
    "# learner = Learner(dls, test_model, loss_func=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.load(\"28Apr21_perimeter_loss_22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(config[\"epochs\"], lr=config[\"lr\"], cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learner.model, \"./models/test_export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner = unet_learner(dls, resnet34, n_out=1, loss_func=adaptive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.load(\"./BASELINE_0.06-0.128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(learner.model, \"./to_upload/baseline_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sett ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.cpu()\n",
    "test_data = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_data[0]\n",
    "test_y = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learner.model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = nn.Sigmoid()(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroOrOneFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        res = torch.where(input > 0.5, torch.tensor(1.0, requires_grad=True), torch.tensor(0.0, requires_grad=True))\n",
    "        ctx.save_for_backward(res)\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input = ctx.saved_tensors\n",
    "        return input\n",
    "    \n",
    "zero_or_one = ZeroOrOneFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contour(x):\n",
    "#     '''\n",
    "#     Differenciable aproximation of contour extraction\n",
    "    \n",
    "#     '''   \n",
    "#     min_pool_x = torch.nn.functional.max_pool2d(x*-1, (3, 3), 1, 1)*-1\n",
    "#     max_min_pool_x = torch.nn.functional.max_pool2d(min_pool_x, (3, 3), 1, 1)\n",
    "#     contour = torch.nn.functional.relu(max_min_pool_x - min_pool_x)\n",
    "#     return contour\n",
    "\n",
    "\n",
    "# def soft_skeletonize(x, thresh_width=10):\n",
    "#     '''\n",
    "#     Differenciable aproximation of morphological skelitonization operaton\n",
    "#     thresh_width - maximal expected width of vessel\n",
    "#     '''\n",
    "#     for i in range(thresh_width):\n",
    "#         min_pool_x = torch.nn.functional.max_pool2d(x*-1, (3, 3), 1, 1)*-1\n",
    "#         max_min_pool_x = torch.nn.functional.max_pool2d(min_pool_x, (3, 3), 1, 1)\n",
    "#         contour = torch.nn.functional.relu(max_min_pool_x - min_pool_x)\n",
    "#         x = torch.nn.functional.relu(x - contour)\n",
    "#     return x\n",
    "\n",
    "def contour(x):\n",
    "    minn1 = -F.max_pool2d(-x, (3,1), (1,1), (1,0) )\n",
    "    minn2 = -F.max_pool2d(-x, (1,3),(1,1), (0,1))\n",
    "    minn = torch.min(minn1, minn2)\n",
    "    maxx = F.max_pool2d(x, (3,3),(1,1), (1,1))\n",
    "    res = maxx-minn\n",
    "    return F.relu_(res)\n",
    "\n",
    "\n",
    "class contour_loss():\n",
    "    '''\n",
    "    inputs shape  (batch, channel, height, width).\n",
    "    calculate clDice loss\n",
    "    Because pred and target at moment of loss calculation will be a torch tensors\n",
    "    it is preferable to calculate target_skeleton on the step of batch forming,\n",
    "    when it will be in numpy array format by means of opencv\n",
    "    '''\n",
    "        \n",
    "    def __call__(self, probs: Tensor, target: Tensor, ) -> Tensor:\n",
    "#         pc = probs[:, self.idc, ...].type(torch.float32)\n",
    "#         tc = target[:, self.idc, ...].type(torch.float32)\n",
    "\n",
    "        pc = zero_or_one(probs).float()\n",
    "        tc = target.float()\n",
    "\n",
    "        b, _, w, h = pc.shape\n",
    "        cl_pred = contour(pc).sum(axis=(2,3))\n",
    "        target_skeleton = contour(tc).sum(axis=(2,3))\n",
    "        big_pen: Tensor = (cl_pred - target_skeleton) ** 2\n",
    "        contour_loss = big_pen / (w * h)\n",
    "    \n",
    "        return contour_loss.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_formatted = zero_or_one(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_formatted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_loss()(test_preds_formatted, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a patch with a glom in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(test_preds_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 in test_y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(contour(test_preds_formatted[3]).detach().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(contour(test_y[3]).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.mse_loss(contour(test_preds[1]).sum(), contour(test_y[1]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(contour(test_preds[1]).sum() - contour(test_y[1]).sum())**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour(test_y[1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:kidneys-cv] *",
   "language": "python",
   "name": "conda-env-kidneys-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
