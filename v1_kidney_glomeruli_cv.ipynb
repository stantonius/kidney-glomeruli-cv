{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuBMAP - Hacking the Kidney - Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Kaggle competition page](https://www.kaggle.com/c/hubmap-kidney-segmentation)\n",
    "\n",
    "Helpful Notebooks:\n",
    "* [https://www.kaggle.com/markalavin/hubmap-tile-images-w-overlap-and-build-tfrecords](https://www.kaggle.com/markalavin/hubmap-tile-images-w-overlap-and-build-tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "* Look at impact of different affine matrices\n",
    "* Look at impact of removing alpha channel on model size and performance\n",
    "* Add Deepmind's architecture optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! conda config --set always_yes True\n",
    "#! conda install -c fastai -c pytorch fastai\n",
    "#! conda update pytorch torchvision torchaudio fastai -c pytorch\n",
    "#! conda update pytorch torchvision torchaudio cudatoolkit -c pytorch\n",
    "#! conda install pandas\n",
    "#! conda install -c conda-forge kaggle\n",
    "#! conda install -c conda-forge tifffile\n",
    "#! conda install -c conda-forge tqdm\n",
    "# !conda install -c conda-forge matplotlib\n",
    "#! conda install -c conda-forge pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea why the conda-forge version doesn't work\n",
    "\n",
    "#!python -m pip install opencv-python\n",
    "\n",
    "# If you are running this notebook on a server (like Linux on WSL2) you need the headless version of opencv\n",
    "# The regular opencv requires GUI packages that serves dont have, and will raise an error\n",
    "#!python -m pip install opencv-python-headless\n",
    "\n",
    "# temporary solution to use tab complete - something wrong with jupyter jedi - need to downgrade\n",
    "#!pip install jedi==0.17.2\n",
    "\n",
    "# below 3 were needed for windows install on python3.9\n",
    "#!pip install --upgrade --pre SimpleITK --find-links https://github.com/SimpleITK/SimpleITK/releases/tag/latest\n",
    "#!python -m pip install scikit-build\n",
    "#!python -m pip install --upgrade pip\n",
    "#!pip install cmake\n",
    "#!pip install torchio --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the finicky local CUDA is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, import PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai.vision.all import *\n",
    "#from fastai.imports import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "# Need to put kaggle.json in /%USERS%/.kaggle folder (C:/Users/Craig/.kaggle)\n",
    "#import kaggle\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Read tiff images\n",
    "import tifffile\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torchio as tio\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Memory management tools\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()\n",
    "#kaggle.api.competition_download_files(\"hubmap-kidney-segmentation\", path=paLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you are about to download the data in the cvorrect directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the data in the correct folder - commented out so as to not repeat the unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "\n",
    "#with zipfile.ZipFile(path/\"hubmap-kidney-segmentation.zip\", 'r') as zipref:\n",
    "#    zipref.extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Convert the encodings to masks\n",
    "- [x] Show the kidney slices\n",
    "- [] Tile the images and masks (can do seperately or when superimposed)\n",
    "- [] Create an iterable of the tiled images\n",
    "- [] Do EDA on the meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGE_INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path/\"train.csv\").rename(columns={\"id\": \"img_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-193753be380d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()\n",
    "#path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert encodings to masks and show images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two options - to use the unencoded json data (in a separate file) or use the encoded data in the dataframe. We will use the latter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **encodings**: represent the pixel number followed by how many pixels *over* (called *lengths* in our functions below)...??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test TIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Main Functions\n",
    "################\n",
    "\n",
    "\n",
    "def rle2mask(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: encoding string value from csv\n",
    "    shape: (width,height) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    # return a list of starting pixels and a list of lengths\n",
    "    starts, lengths = [\n",
    "        np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])\n",
    "    ]\n",
    "    # subtract 1 from every starting pixel\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    # calculate a background of 0 (empty) with size defined by image\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    # replace every 0 within each range with 1\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = 1\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "\n",
    "##########\n",
    "# Helpers\n",
    "##########\n",
    "\n",
    "def get_id_by_index(index, df=train_df):\n",
    "    return df.iloc[index]['img_id']\n",
    "\n",
    "def get_single_img(id, folder=\"train\"):\n",
    "    img = tifffile.imread(path/folder/(id+\".tiff\"))\n",
    "    if len(img.shape) == 5:\n",
    "        img = img.squeeze().transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "def show_single_img(id, **kwargs):\n",
    "    return plt.imshow(get_single_img(id), **kwargs)\n",
    "\n",
    "def show_img_by_index(index, df=train_df):\n",
    "    return plt.imshow(tifffile.imread(path/\"train\"/(train_df.iloc[TEST_IMAGE_INDEX]['id']+\".tiff\")))\n",
    "\n",
    "def get_single_encs(id, df=train_df):\n",
    "    return df[df['img_id'] == id]['encoding'].array[0]\n",
    "\n",
    "def get_mask(id, df=train_df, folder=\"train\"):\n",
    "    return rle2mask(\n",
    "        get_single_encs(id, df=df),\n",
    "        get_single_img(id, folder=folder).shape[::-1][1:]\n",
    "    )\n",
    "\n",
    "def show_single_img_and_mask(id):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    mask = get_mask(id)\n",
    "    img = get_single_img(id)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image\", fontsize=18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, cmap=\"hot\", alpha=0.5)\n",
    "    plt.title(f\"Image + mask\", fontsize=18)    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, cmap=\"hot\")\n",
    "    plt.title(f\"Mask\", fontsize=18)    \n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def to_4d(img, input_chan_first=False, output_chan_first=True):\n",
    "    if not len(img.shape)==3:\n",
    "        raise ValueError(\"Function only converts 3D arrayto 4D array\")\n",
    "    return np.expand_dims(np.transpose(img, \n",
    "                   (0,1,2) if input_chan_first else (2,0,1)), \n",
    "                   3 if output_chan_first else 0)\n",
    "\n",
    "def to_3d(img, input_chan_first=True, output_chan_first=False):\n",
    "    if not len(img.shape)==4:\n",
    "        raise ValueError(\"Function only converts 4D arrayto 3D array\")\n",
    "    return np.transpose(img.squeeze(), (0,1,2) if output_chan_first else (1,2,0))\n",
    "\n",
    "def to_3chan(x, dim=1):\n",
    "    return torch.cat((x,x,x), dim=dim)\n",
    "\n",
    "def resizer(img, scale=5, show=False):\n",
    "    \"\"\"\n",
    "    Returns an smaller array of the same dimensions, but converts to 3D to allow for resizing\n",
    "    \"\"\"\n",
    "    scale_percent = scale # percent of original size\n",
    "    im_dims = (len(img.shape) == 4)\n",
    "    if im_dims:\n",
    "        img = to_3d(img)\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    img_reshaped = cv2.resize(img.numpy(), dim)\n",
    "    if show:\n",
    "        return plt.imshow(img_reshaped)\n",
    "    if im_dims:\n",
    "        return to_4d(img_reshaped)\n",
    "    return img_reshaped\n",
    "\n",
    "def squeeze_and_reshape(img_tensor, remove_alpha=False):\n",
    "    if not isinstance(img_tensor, torch.Tensor):\n",
    "        raise TypeError(\"Image needs to be a tensor\")\n",
    "    if len(img_tensor.shape) == 5:\n",
    "        img_tensor = img_tensor.squeeze().permute(2, 1, 0)\n",
    "    img_tensor = img_tensor.unsqueeze(2).permute(3,1,0,2)\n",
    "    return img_tensor\n",
    "\n",
    "def to_pil(image):\n",
    "    # for \n",
    "    data = image.numpy().squeeze().T\n",
    "    data = data.astype(np.uint8)\n",
    "    image = Image.fromarray(data)\n",
    "    w, h = image.size\n",
    "    display(image)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remask(img, mask, tile, threshold=8, show=False):\n",
    "    \n",
    "    img_height = img.shape[1]\n",
    "    img_width = img.shape[0]\n",
    "    \n",
    "    number_of_vertical_tiles = (img_height // tile)+1\n",
    "    number_of_horizontal_tiles = (img_width // tile)+1\n",
    "    \n",
    "    #background = np.zeros((tile*number_of_horizontal_tiles, tile*number_of_vertical_tiles,3))[:img.shape[0],:img.shape[1],:img.shape[2]]\n",
    "    \n",
    "    tile_coords = []\n",
    "    for h_idx in range(number_of_horizontal_tiles):\n",
    "        for v_idx in range(number_of_vertical_tiles):\n",
    "            tile_coords.append((h_idx+1, v_idx+1)) # +1 to remove 0 indexing\n",
    "\n",
    "    cropped_images = []\n",
    "    for h,v in tile_coords:\n",
    "        cropped_images.append((h, v, img[tile*(h-1):tile*h, tile*(v-1):tile*v, :]))\n",
    "        \n",
    "    for horiz,vert,im in cropped_images:\n",
    "        if not 0 in im.shape:      #required in case tile is \n",
    "            hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
    "            h, s, v = cv2.split(hsv)\n",
    "            if s.mean() < threshold:\n",
    "                all_black = np.full((im.shape[0], im.shape[1]),2)\n",
    "                mask[tile*(horiz-1):tile*horiz,tile*(vert-1):tile*vert] = all_black\n",
    "                #im = im*0.\n",
    "            #background[tile*(horiz-1):tile*horiz,tile*(vert-1):tile*vert,:] = im\n",
    "    \n",
    "    if show:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img.astype('uint8'))\n",
    "        plt.title(f\"Image\", fontsize=18)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(img.astype('uint8'))\n",
    "        plt.imshow(mask.astype('uint8'), cmap=\"hot\", alpha=0.5)\n",
    "        plt.title(f\"Image + mask\", fontsize=18)    \n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(mask.astype('uint8'), cmap=\"hot\")\n",
    "        plt.title(f\"Mask\", fontsize=18)    \n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "#img_id = get_id_by_index(7)\n",
    "#img_id = '095bf7a1f'\n",
    "#with tifffile.TiffFile(path/\"train\"/(img_id+\".tiff\")) as tif:\n",
    "#    imgg = tif.asarray()\n",
    "#print(imgg.shape)\n",
    "#mask = get_mask(img_id)\n",
    "#new_mask = remask(to_3d(squeeze_and_reshape(imgg)), mask, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_df(df, directory):\n",
    "    mask_list = []\n",
    "    for idx,_ in tqdm(enumerate(df.iterrows()), total=len(df)):\n",
    "        img_id = get_id_by_index(idx, df=df)\n",
    "        with tifffile.TiffFile(path/directory/(img_id+\".tiff\")) as tif:\n",
    "            base_im = tif.asarray()\n",
    "            im_tensor = squeeze_and_reshape(torch.from_numpy(base_im)).numpy()\n",
    "            mask = remask(to_3d(im_tensor), get_mask(img_id), 1000)\n",
    "            mask_list.append((img_id, mask))\n",
    "    return pd.DataFrame(mask_list, columns=[\"img_id\", \"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't recreate the dataset everytime - pull from local directory if available as pickle file\n",
    "# I understand pickles aren't safe - so only do this in your local env and never open an unfamiliar pickle file\n",
    "if not (path/\"new_masks.pkl\").exists():\n",
    "    new_masks = create_mask_df(train_df, \"train\")\n",
    "    new_masks.to_pickle(path/\"new_masks.pkl\")\n",
    "if (path/\"new_masks.pkl\").exists():\n",
    "    new_masks = pd.read_pickle(\"new_masks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_creator(df):\n",
    "    subjects_list = []\n",
    "    for idx,_ in tqdm(enumerate(df.iterrows()), total=len(df)):\n",
    "        \n",
    "        affine = torch.tensor([[-1.,  0.,  0.,  0.],\n",
    "       [ 0., -1.,  0.,  0.],\n",
    "       [ 0.,  0.,  1.,  0.],\n",
    "       [ 0.,  0.,  0.,  1.]])\n",
    "        \n",
    "        img_id = get_id_by_index(idx, df=df)\n",
    "        \n",
    "        pic_list = [item for item in (path/\"smaller/imgs\").rglob(\"*\") if not item.is_dir() and img_id in item.name]\n",
    "        \n",
    "        for pic in pic_list:\n",
    "            pic_name = pic.name.split(\".\")[0]\n",
    "            im = tio.ScalarImage(path/\"smaller/imgs\"/(pic_name+\".tiff\"))\n",
    "            mask = tio.LabelMap(path/\"smaller/masks\"/(pic_name+\"_mask.tiff\"))\n",
    "\n",
    "            subjects_list.append(tio.Subject(\n",
    "                img = im,\n",
    "                mask = mask,\n",
    "                img_id = pic_name\n",
    "            ))\n",
    "    return subjects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_image(img_id, mask_df):\n",
    "    img = tio.Image(path/\"train\"/f\"{img_id}.tiff\").data\n",
    "    if len(img.shape) != 4:\n",
    "        raise ValueError(\"Tensor shape needs to have 4 dimensions\")\n",
    "    if img.shape[0] != 4:\n",
    "        raise ValueError(\"First dimension must have 4 channels\")\n",
    "    vertical_tiles = img.shape[2] // 2\n",
    "    horizontal_tiles = img.shape[1] // 2\n",
    "    \n",
    "    mask = torch.from_numpy(mask_df[mask_df[\"img_id\"]==img_id][\"mask\"].values[0]).unsqueeze(0).unsqueeze(3)\n",
    "    # I have managed to flip the axes somewhere and am too lazy or stubborn to fix the root issue. So need to permute axes\n",
    "    mask = mask.permute(0,2,1,3)\n",
    "    \n",
    "    img1 = img[:,:horizontal_tiles,:vertical_tiles,:]\n",
    "    mask1 = mask[:,:horizontal_tiles,:vertical_tiles,:]\n",
    "    img2 = img[:,horizontal_tiles:,:vertical_tiles,:]\n",
    "    mask2 = mask[:,horizontal_tiles:,:vertical_tiles,:]\n",
    "    img3 = img[:,:horizontal_tiles,vertical_tiles:,:]\n",
    "    mask3 = mask[:,:horizontal_tiles,vertical_tiles:,:]\n",
    "    img4 = img[:,horizontal_tiles:,vertical_tiles:,:]\n",
    "    mask4 = mask[:,horizontal_tiles:,vertical_tiles:,:]\n",
    "    \n",
    "    tio.Image(tensor=img1).save(path/\"smaller/imgs\"/f\"{img_id}_1.tiff\")\n",
    "    tio.Image(tensor=mask1).save(path/\"smaller/masks\"/f\"{img_id}_1_mask.tiff\")\n",
    "    tio.Image(tensor=img2).save(path/\"smaller/imgs\"/f\"{img_id}_2.tiff\")\n",
    "    tio.Image(tensor=mask2).save(path/\"smaller/masks\"/f\"{img_id}_2_mask.tiff\")\n",
    "    tio.Image(tensor=img3).save(path/\"smaller/imgs\"/f\"{img_id}_3.tiff\")\n",
    "    tio.Image(tensor=mask3).save(path/\"smaller/masks\"/f\"{img_id}_3_mask.tiff\")\n",
    "    tio.Image(tensor=img4).save(path/\"smaller/imgs\"/f\"{img_id}_4.tiff\")\n",
    "    tio.Image(tensor=mask4).save(path/\"smaller/masks\"/f\"{img_id}_4_mask.tiff\")\n",
    "\n",
    "\n",
    "#[cut_image(item, new_masks) for item in new_masks.img_id.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_normalization = tio.Lambda(lambda x: torch.half(x/255), types_to_apply=[tio.IMAGE])\n",
    "custom_normalization = tio.Lambda(lambda x: (x/255).float(), types_to_apply=[tio.INTENSITY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_reshape = tio.Lambda(lambda x: x[:3,...], types_to_apply=[tio.INTENSITY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnecessary as I should find out why there are different shapes but I want to get to model building\n",
    "def shuffle_axes(img_tensor):\n",
    "    return img_tensor.permute(0,2,1,3)\n",
    "reshuffle = tio.Lambda(shuffle_axes, types_to_apply=[tio.LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_shrink = tio.Lambda(lambda x: torch.tensor(resizer(x, 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 571.46it/s]\n"
     ]
    }
   ],
   "source": [
    "subjects_list = subject_creator(new_masks)\n",
    "subjects_list_copy = subjects_list[:]     # needed because shuffle does in place\n",
    "random.seed(57)\n",
    "random.shuffle(subjects_list_copy)\n",
    "\n",
    "train_subjects = subjects_list_copy[:round(len(subjects_list_copy)*0.8)]\n",
    "valid_subjects = subjects_list_copy[round(len(subjects_list_copy)*0.8):]\n",
    "\n",
    "#train_subjects = subject_creator(new_masks.iloc[:1])\n",
    "#valid_subjects = subject_creator(new_masks.iloc[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_subject = train_subjects[0]\n",
    "    #to_pil(tio.RandomElasticDeformation()(test_subject[\"img\"][tio.DATA]))\n",
    "    \n",
    "    small_test_image = custom_shrink(test_subject[\"img\"][tio.DATA])\n",
    "    \n",
    "    max_displacement = 500, 400, 0\n",
    "    #to_pil(tio.RandomElasticDeformation(max_displacement=max_displacement, num_control_points=10)(small_test_image))\n",
    "    to_pil(tio.RandomElasticDeformation(max_displacement=max_displacement)(small_test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test what data types there are in the dataset - important when selectively trying to apply transforms\n",
    "# NOTE: i think the type should be IMAGE and not INTENSITY\n",
    "\n",
    "if TEST:\n",
    "    for images in test_subject.values():\n",
    "        print(images.type)\n",
    "    \n",
    "    test_subject.img.data[:3,...].shape\n",
    "    \n",
    "    flatten_image_no_transform = torch.flatten(xx.img.data).numpy()\n",
    "    #plt.hist(flatten_image_no_transform, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIO API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforms\n",
    "\n",
    "* I'm unsure about how useful affine is for kidney slices. On one hand you will never get a kidney slice which on its own is transformed this way, however this may help look at different angles of glomeruli (which themselves at random angles depending on how the kidney was sliced)  \n",
    "\n",
    "* There are a whole list of transforms that help with (artifacts like spikes, ghosting, motion blurring). However knowing how these samples are prepared and imaged, its unlikely that these effects will be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = tio.Compose([\n",
    "    custom_reshape,\n",
    "    #custom_shrink,\n",
    "    #reshuffle,\n",
    "    #tio.Resample(4),\n",
    "    #tio.ZNormalization(exclude=\"mask\"),\n",
    "    tio.RandomFlip(),\n",
    "    tio.RandomAffine(),\n",
    "    #tio.RescaleIntensity((0,1)),\n",
    "    custom_normalization,\n",
    "])\n",
    "\n",
    "valid_transforms = tio.Compose([\n",
    "    \n",
    "    custom_reshape,\n",
    "    #reshuffle,\n",
    "    #tio.Resample(4),\n",
    "    #tio.ZNormalization(exclude=\"mask\"),\n",
    "    #tio.RescaleIntensity((0,1)),\n",
    "    custom_normalization,\n",
    "])\n",
    "\n",
    "train_dataset = tio.SubjectsDataset(train_subjects, transform=train_transforms)\n",
    "valid_dataset = tio.SubjectsDataset(valid_subjects, transform=valid_transforms)\n",
    "\n",
    "label_samples = {\n",
    "    0: 8,\n",
    "    1: 3,\n",
    "    2: 2\n",
    "}\n",
    "\n",
    "# patch should be size that is continuously divisible by 2 \n",
    "# ie. not 100, as 100>50>25>12.5 \n",
    "\n",
    "patch_size = (256, 256, 1) \n",
    "\n",
    "sampler = tio.data.LabelSampler(patch_size, label_probabilities=label_samples)\n",
    "\n",
    "queue_length = 100\n",
    "samples_per_volume = 100\n",
    "\n",
    "train_queue = tio.Queue(\n",
    "    train_dataset,\n",
    "    queue_length,\n",
    "    samples_per_volume,\n",
    "    sampler,\n",
    "    num_workers=0,\n",
    "    #shuffle_subjects=False,\n",
    "    #shuffle_patches=False\n",
    ")\n",
    "\n",
    "valid_queue = tio.Queue(\n",
    "    valid_dataset,\n",
    "    queue_length,\n",
    "    samples_per_volume,\n",
    "    sampler,\n",
    "    num_workers=0,\n",
    "    #shuffle_subjects=False,\n",
    "    #shuffle_patches=False\n",
    ")\n",
    "\n",
    "#train_loader = DataLoader(train_queue, batch_size=16, pin_memory=True)\n",
    "train_loader = DataLoader(train_queue, batch_size=16)\n",
    "#valid_loader = DataLoader(valid_queue, batch_size=16, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_queue, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 8, 1: 3, 2: 2}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:    \n",
    "    one_batch = next(iter(train_loader))\n",
    "\n",
    "    batch_imgs = one_batch['img'][tio.DATA][:,:3,...].squeeze()\n",
    "    batch_label = to_3chan(one_batch['mask'][tio.DATA]).squeeze()\n",
    "\n",
    "    batch_imgs[0].shape\n",
    "\n",
    "    batch_imgs = one_batch['img'][tio.DATA][:,:3,...].squeeze()\n",
    "    batch_label = to_3chan(one_batch['mask'][tio.DATA]).squeeze()\n",
    "    slices = torch.cat((batch_imgs, batch_label))\n",
    "    image_path = 'batch_patches.png'\n",
    "    torchvision.utils.save_image(\n",
    "        slices,\n",
    "        image_path,\n",
    "        nrow=8//2,\n",
    "        normalize=True,\n",
    "        scale_each=True,\n",
    "    )\n",
    "\n",
    "    one_batch['img'][tio.DATA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNET Segmentation is the leading NN architecture for medical image analysis. \n",
    "\n",
    "The basic UNET architecure is below\n",
    "\n",
    "<img src = \"https://github.com/fastai/fastbook/raw/fb570779062177662fbfde0f5dbb1e9f08dabbee/images/att_00052.png\" width=700 />\n",
    "\n",
    "There are 3 phases:\n",
    "* **Down/contract**: image size is reduced (typically with a stride 2 configuration) with the focus of feature extraction, which comes from the number of filters. In the example above, a 3-channel input is passed through one convolution with 64 filters. The output of this convolution is then passed through batchnorm and finally a relu. \n",
    "    * Note the order of this is (used to be?) up for debate - do you batchnorm *then* activation function, or vice versa?\n",
    "    * After the second convolution, the model then performs a **maxpool** - you can tell it is the red arrow because the output is half the size of the input (standard maxpool behaviour)\n",
    "    \n",
    "Key term:\n",
    "* **Maxpooling**: feature map outputs of convolutions record the *precise position* of features. However small movements in the positon of the feature (ie. cropping, rotation, shifting) result in a different feature map (ie. feature maps are **sensitive to location**). Maxpooling is a **downsampling** technique (ie. will always make the image **smaller** - see down smapling definition below) that takes the max activation value within a maxpool grid (often 2x2 with stride of 2 - hence half the input size). This makes the model **more robust to changes in feature position** in the image.\n",
    "    * The pooling layer **operates upon each feature map separately** to create **a new set** of the same number of pooled feature maps.\n",
    "\n",
    "\n",
    "* **Down sampling**: where a lower resolution version of an input signal is created that still contains the large or important structural elements, without the fine detail that may not be as useful to the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Down block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    The user specifies what the first input channels size will be and the ultimate output size will be\n",
    "    The downblock and upblock functions also take input and out values - but these are PER CONVOLUTION\n",
    "    They do not necessarily inherit the values specified by the user\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channel_in, channel_out, stride=1, ks=3):\n",
    "        super(CustomUnet, self).__init__()\n",
    "        self.down_conv1 = self._downblock(channel_in, 64, stride=stride, ks=ks)\n",
    "    \n",
    "    # downward (contracting) block\n",
    "    def _downblock(self, n_in, n_out, stride, ks):\n",
    "        down_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_in, n_out, stride=stride, kernel_size=ks, padding=ks//2), # 256/2 = 128\n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_out, n_out, stride=stride, kernel_size=ks, padding=ks//2), # 128/2 = 64\n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=ks, stride=2, padding=ks//2) # 64/2 = 32\n",
    "        )\n",
    "        return down_conv\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.down_conv1(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified in the comments in the class, there are 3 down sampling steps that take place. Therefore if this were our only step, we would expect the same batch size, but 64 channels (because we specified this an the output), where each filter is 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_unet = CustomUnet(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dims = custom_unet(dls.one_batch()[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Up block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is similar to the down block with two notable additional steps:\n",
    "* Upsampling using `nn.ConvTranspose2d`\n",
    "* Consuming the output from the oppositse side of the \"U\"\n",
    "\n",
    "Also important to note: the inputs to the second and final upblocks **are doubled** since we are stacking two tensors (one from the opposite downblock, one from the previous block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    The user specifies what the first input channels size will be and the ultimate output size will be\n",
    "    The downblock and upblock functions also take input and out values - but these are PER CONVOLUTION\n",
    "    They do not necessarily inherit the values specified by the user\n",
    "    \n",
    "    The architecture is 3 down blocks, followed by 3 up blocks\n",
    "    Output is squeezed if the channel_out is 1 - masks are single channels so this matches the dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channel_in, channel_out, stride=1, ks=3):\n",
    "        super(CustomUnet, self).__init__()\n",
    "        self.down_conv1 = self._downblock(channel_in, 16, stride=stride, ks=ks)\n",
    "        self.down_conv2 = self._downblock(16, 32, stride=stride, ks=ks)\n",
    "        self.down_conv3 = self._downblock(32, 64, stride=stride, ks=ks)\n",
    "        self.up_conv3 = self._upblock(64, 32, stride=stride, ks=ks)\n",
    "        self.up_conv2 = self._upblock(32*2, 16, stride=stride, ks=ks) # key to notice the doubling of input size\n",
    "        self.up_conv1 = self._upblock(16*2, channel_out, stride=stride, ks=ks)\n",
    "    \n",
    "    # downward (contracting) block\n",
    "    def _downblock(self, n_in, n_out, stride, ks):\n",
    "        down_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_in, n_out, stride=stride, kernel_size=ks, padding=ks//2), \n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_out, n_out, stride=stride, kernel_size=ks, padding=ks//2), \n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=ks, stride=2, padding=ks//2) # 256/2 = 128\n",
    "        )\n",
    "        return down_conv\n",
    "    \n",
    "    def _upblock(self, n_in, n_out, stride, ks):\n",
    "        up_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_in, n_out, stride=stride, kernel_size=ks, padding=ks//2),\n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_out, n_out, stride=stride, kernel_size=ks, padding=ks//2), \n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_out, n_out, stride=2, kernel_size=ks, padding=ks//2, output_padding=ks//2),\n",
    "        )\n",
    "        return up_conv\n",
    "    \n",
    "    def forward(self, x):\n",
    "        down_conv1 = self.down_conv1(x)\n",
    "        down_conv2 = self.down_conv2(down_conv1)\n",
    "        down_conv3 = self.down_conv3(down_conv2)\n",
    "        \n",
    "        up_conv3 = self.up_conv3(down_conv3)\n",
    "        \n",
    "        up_conv2 = self.up_conv2(torch.cat([up_conv3, down_conv2], 1))\n",
    "        up_conv1 = self.up_conv1(torch.cat([up_conv2, down_conv1], 1))\n",
    "        \n",
    "        return nn.Sigmoid()(up_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_unet = CustomUnet(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function for this competition is **Dice Loss** - but we need to dive into the details of image segmentation first.\n",
    "\n",
    "A [blog post by Jeremy Jordan](https://www.jeremyjordan.me/semantic-segmentation/) has been extremely helpful in walking through the basics of image segmentation.\n",
    "\n",
    "The goal of image segmentation is to **label each pixel** with a **class** of what is being represented. This is often referred to as **dense prediction**\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/05/Screen-Shot-2018-05-17-at-9.02.15-PM.png\" width=500 />\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/05/Screen-Shot-2018-05-16-at-9.36.00-PM.png\" width=500 />\n",
    "\n",
    "##### Why not cross entropy?\n",
    "\n",
    "The statistical distributions of labels affect training accuracy. Although weighted cross entropy can partially solve this proble, another limitation arises: each pixel is evaluated discretely/independently whether it belongs to a class - it does not consider adjacent pixels. Therefore the outcome is per-pixel loss, as opposed to loss relative to the entire segmented object or boundary.\n",
    "\n",
    "##### Sigmoid - a refresher\n",
    "\n",
    "The purpose of Sigmoid is to squeeze values (predictions) to between 0 and 1. This is useful for **binary** classification only, as the model *learns to push one class towards 0 and the second class towards 1*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a single batch and run it through our model to see its predictions - the first value in the tuple (`y_preds`). Let's also get the second value in the tuple - the actuals (`y_true`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def dice_loss(y_true, y_pred):\n",
    "    numerator = 2. * torch.sum(y_pred * y_true)\n",
    "    denominator = torch.sum(y_pred + y_true)\n",
    "    return 1 - numerator/denominator\"\"\"\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "    \n",
    "dice_loss = DiceLoss()    \n",
    "    \n",
    "class DiceBCELoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE\n",
    "    \n",
    "bce_dice_loss = DiceBCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    custom_unet.to(\"cpu\")\n",
    "    \n",
    "    xx = test_subject['mask'][tio.DATA].squeeze()\n",
    "    yy = torch.where(xx != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "    \n",
    "    y_preds = custom_unet(one_batch['img'][tio.DATA].squeeze().cpu())\n",
    "    y_true = torch.where(one_batch['mask'][tio.DATA].squeeze() != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "    \n",
    "    y_true.to(\"cpu\")\n",
    "    \n",
    "    #assert y_true[0].shape == y_preds[0].shape\n",
    "    \n",
    "    print(dice_loss(y_preds, y_true.float()))\n",
    "    \n",
    "    print(acc_metric(y_preds, y_true))\n",
    "    #y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_metric(predb, yb):\n",
    "    return (torch.round(predb) == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot use fastai - need to figure out why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epochs = 1\n",
    "\n",
    "def trainer(dl_train, dl_valid, model, loss_fxn, optimizer, metric_fxn, epochs=1):\n",
    "    start = time.time()\n",
    "    \n",
    "    # set model to cuda - look into the .device() method\n",
    "    model.cuda()\n",
    "    \n",
    "    # collect training stats\n",
    "    train_loss, valid_loss = [], []\n",
    "    \n",
    "    # step for printing loss and metric\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for stage in [\"train\", \"valid\"]:\n",
    "            if stage == \"train\":\n",
    "                model.train(True)\n",
    "                dataloader = dl_train\n",
    "            else:\n",
    "                model.train(False)\n",
    "                dataloader = dl_valid\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_metric = 0.0\n",
    "        \n",
    "            for i, data in enumerate(dataloader):\n",
    "                # squeeze the depth dimension as our model doesn't account for this but TorchIO utilises this\n",
    "                inputs = data['img'][tio.DATA].squeeze()\n",
    "                labels = data['mask'][tio.DATA].squeeze()\n",
    "                # need to override the labels to ensure we are only guessing presence of glomeruli\n",
    "                # remember the additional category was introduced just to improve sampling selection area\n",
    "                labels = torch.where(labels != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "                \n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.float().cuda()\n",
    "                \n",
    "                if stage == \"train\":\n",
    "                    \n",
    "                    # set the grads to zero\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    # backward\n",
    "                    loss = loss_fxn(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "                        loss = loss_fxn(outputs, labels)\n",
    "                        \n",
    "                metric = metric_fxn(outputs, labels)\n",
    "                \n",
    "                del inputs, labels, outputs\n",
    "                gc.collect()\n",
    "                \n",
    "                running_loss += loss*dataloader.batch_size\n",
    "                running_metric += metric*dataloader.batch_size\n",
    "                \n",
    "                if step % 10 == 0:\n",
    "                    print('Current step: {}  Loss: {}  Metric: {}  AllocMem (Mb): {}'.format(step, loss, metric, torch.cuda.memory_allocated()/1024/1024))\n",
    "                \n",
    "            epoch_loss = running_loss/len(dataloader.dataset)\n",
    "            epoch_metric = running_metric/len(dataloader.dataset)\n",
    "            \n",
    "            train_loss.append(epoch_loss) if stage=='train' else valid_loss.append(epoch_loss)\n",
    "            \n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(custom_unet.parameters(), lr=0.001)\n",
    "#train_loss, valid_loss = trainer(train_loader, valid_loader, custom_unet, bce_dice_loss, opt, acc_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is **important to summarise** what is going on here. There are 5 keys steps:\n",
    "\n",
    "0. Extract `x` and `y` from batch (and perform any updates to them, ie. modify their shape)\n",
    "1. **forward**: calculate the **logits** by passing the input through the model\n",
    "2. Calculate the **loss function**\n",
    "3. **zero the gradients** - this is zeroing any grads from previous loops, as otherwise these would *aggregate*\n",
    "    * Note in the loop above, we are calling `opt.zero_grad()` - this is because the parameters also exist in the optimizer. However we could have also written `model.zero_grad()` or `model.parameters.grad.zero_()` (notice the trailing `_` which means update in place)\n",
    "4. **backward**: calculates the partial derivatives of the loss function with respect to the parameters\n",
    "    * More technically, the underlying function is `params.grad.add_(dloss/dparams)`\n",
    "5. **step**: update the parameters using the gradients\n",
    "    * Again this is done on the `optimizer.step()` because like before, the parameters exist in the optimizer and the Pytorch package knows how to update these\n",
    "    * Critical to remember this is equivalent to: **`with torch.no_grad(): params -= params.grad * lr`**, which you may also add batchnorm here as well\n",
    "\n",
    "One final summary: gradients are calculated using the loss (the partial derivative of loss with respect to params) by calling `loss.backward()` on the **loss function**, but those gradients are accessible on the **parameters** `params.grad`. These gradients are then multiplied by the elarning rate and subtracted from the previous params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the accuracy of guessing all 0's and the proportion of 1s to 0s in each image\n",
    "\n",
    "also try a new Unet, different tile size, different sampling proportions, different affine matrix, and more transforms, and the normal loss function\n",
    "\n",
    "What about Unet transfer learning or transformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ds = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ds_item_counts = torch.unique(test_ds.mask.data, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(test_ds_item_counts[1][1] / test_ds_item_counts[1][2])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_loader = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLDLitUNETDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, train_loader, valid_loader):\n",
    "        super().__init__()\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.valid_loader\n",
    "    \n",
    "    def transfer_batch_to_device(self, batch, device):\n",
    "        # squeeze the depth dimension as our model doesn't account for this but TorchIO utilises this\n",
    "        x = batch['img'][tio.DATA].squeeze()\n",
    "        y = batch['mask'][tio.DATA].squeeze()\n",
    "        # need to override the labels to ensure we are only guessing presence of glomeruli\n",
    "        # remember the additional category was introduced just to improve sampling selection area\n",
    "        y = torch.where(y != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "        return x.to(device), y.to(device)\n",
    "    \n",
    "    \n",
    "\n",
    "class LitUNET(pl.LightningModule):\n",
    "    def __init__(self, train_dl, valid_dl, model, loss_fxn, bs=32, learning_rate=3e-3, manual_optimization=False):\n",
    "        super().__init__()\n",
    "        #self.save_hyperparameters()\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.model = model\n",
    "        self.loss_fxn = loss_fxn\n",
    "        self.lr = learning_rate\n",
    "        self._manual_optimization = manual_optimization\n",
    "        if self._manual_optimization:\n",
    "            self.training_step = self.training_step_manual\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.valid_dl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # squeeze the depth dimension as our model doesn't account for this but TorchIO utilises this\n",
    "        x = batch['img'][tio.DATA].squeeze()\n",
    "        y = batch['mask'][tio.DATA].squeeze()\n",
    "        # need to override the labels to ensure we are only guessing presence of glomeruli\n",
    "        # remember the additional category was introduced just to improve sampling selection area\n",
    "        y = torch.where(y != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # squeeze the depth dimension as our model doesn't account for this but TorchIO utilises this\n",
    "        x = batch['img'][tio.DATA].squeeze()\n",
    "        y = batch['mask'][tio.DATA].squeeze()\n",
    "        # need to override the labels to ensure we are only guessing presence of glomeruli\n",
    "        # remember the additional category was introduced just to improve sampling selection area\n",
    "        y = torch.where(y != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('valid_loss', loss, on_step=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # squeeze the depth dimension as our model doesn't account for this but TorchIO utilises this\n",
    "        x = batch['img'][tio.DATA].squeeze()\n",
    "        y = batch['mask'][tio.DATA].squeeze()\n",
    "        # need to override the labels to ensure we are only guessing presence of glomeruli\n",
    "        # remember the additional category was introduced just to improve sampling selection area\n",
    "        y = torch.where(y != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # self.hparams available because we called self.save_hyperparameters()\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitUNETDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, train_loader, valid_loader):\n",
    "        super().__init__()\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.valid_loader\n",
    "    \n",
    "    def transfer_batch_to_device(self, batch, device):\n",
    "        # squeeze the depth dimension as our model doesn't account for this but TorchIO utilises this\n",
    "        x = batch['img'][tio.DATA].squeeze()\n",
    "        y = batch['mask'][tio.DATA].squeeze()\n",
    "        # need to override the labels to ensure we are only guessing presence of glomeruli\n",
    "        # remember the additional category was introduced just to improve sampling selection area\n",
    "        y = torch.where(y != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "        return x.to(device), y.to(device)\n",
    "    \n",
    "    \n",
    "\n",
    "class LitUNET(pl.LightningModule):\n",
    "    def __init__(self, model, loss_fxn, bs=32, learning_rate=3e-3, num_workers=0, manual_optimization=False):\n",
    "        super().__init__()\n",
    "        #self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.loss_fxn = loss_fxn\n",
    "        self.lr = learning_rate\n",
    "        self._manual_optimization = manual_optimization\n",
    "        if self._manual_optimization:\n",
    "            self.training_step = self.training_step_manual\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('valid_loss', loss, on_step=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # self.hparams available because we called self.save_hyperparameters()\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(range(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[round(len(test_list)*(1-0.5)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossSpikeAnalyzer(Callback):\n",
    "    def __init__(self):\n",
    "        self.train_batch_loss = []\n",
    "        self.valid_batch_loss = []\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        callback_stats = trainer.callback_metrics\n",
    "        if \"train_loss_step\" in callback_stats.keys():\n",
    "            self.train_batch_loss.append(callback_stats[\"train_loss_step\"].item())\n",
    "            #if len(self.train_batch_loss) > 10:\n",
    "                #print(f\"The rolling loss is {np.asarray(self.train_batch_loss).mean()}\")\n",
    "                #print(f\"The mean of the last 50% of batches is {np.asarray(self.train_batch_loss[round(len(self.train_batch_loss)*(1-0.5)):]).mean()}\")\n",
    "            #print(f\"The shape of the batch is {batch['img'][tio.DATA].shape} - the batch actually returns {batch.keys()} - and {outputs}, and the loss is {np.asarray(self.batch_loss).mean()}\")\n",
    "            if callback_stats[\"train_loss_step\"].item() > 0.9 or batch_idx % 60 == 0:\n",
    "                img_list = []\n",
    "                with torch.no_grad():\n",
    "                    pl_module.eval()\n",
    "                    preds = pl_module(batch['img'][tio.DATA].squeeze().cuda())\n",
    "                    pl_module.train()\n",
    "                for i, img in enumerate(batch['img'][tio.DATA].squeeze()):\n",
    "                    mask = torch.where(batch['mask'][tio.DATA].squeeze()[i,...].unsqueeze(0) != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "                    mask = to_3chan(mask, 0)\n",
    "                    pred = to_3chan(preds[i,...],0)\n",
    "                    img_list.append(img)\n",
    "                    img_list.append(mask)\n",
    "                    img_list.append(pred.cpu())\n",
    "                grid = torchvision.utils.make_grid(\n",
    "                    img_list,\n",
    "                    nrow=3,\n",
    "                )\n",
    "                self._save(grid, batch_idx, callback_stats[\"train_loss_step\"].item())\n",
    "    \n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        print(f\"The VALID BATCH END metrics are {trainer.callback_metrics}\")\n",
    "        \n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        print(f\"Note that the metrics when VALIDATION ENDS are {trainer.callback_metrics}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _show(img):\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _save(img, batch_idx, loss):\n",
    "        npimg = img.numpy()\n",
    "        loss = str(round(loss,2)).replace(\".\", \"_\")\n",
    "        plt.imsave(path/\"training_image_logs\"/f\"{batch_idx}__{loss}.png\", np.transpose(npimg, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | model    | CustomUnet | 123 K \n",
      "1 | loss_fxn | DiceLoss   | 0     \n",
      "----------------------------------------\n",
      "123 K     Trainable params\n",
      "0         Non-trainable params\n",
      "123 K     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|                                                                   | 0/2 [00:00<?, ?it/s]The VALID BATCH END metrics are {}\n",
      "Validation sanity check:  50%|█████████████████████████████▌                             | 1/2 [00:06<00:06,  6.40s/it]The VALID BATCH END metrics are {}\n",
      "Note that the metrics when VALIDATION ENDS are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0')}\n",
      "Epoch 0:  50%|█████████████████████████                         | 25/50 [01:22<01:22,  3.31s/it, loss=0.783, v_num=122]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                                                               | 0/25 [00:00<?, ?it/s]\u001b[AThe VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "Epoch 0:  56%|████████████████████████████                      | 28/50 [01:22<01:05,  2.96s/it, loss=0.783, v_num=122]The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "\n",
      "Validating:  16%|███████████▎                                                           | 4/25 [00:03<00:20,  1.01it/s]\u001b[AThe VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "Epoch 0:  64%|████████████████████████████████                  | 32/50 [01:26<00:48,  2.71s/it, loss=0.783, v_num=122]The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "\n",
      "Validating:  32%|██████████████████████▋                                                | 8/25 [00:04<00:07,  2.36it/s]\u001b[AThe VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "Epoch 0:  72%|████████████████████████████████████              | 36/50 [01:30<00:35,  2.52s/it, loss=0.783, v_num=122]The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "\n",
      "Validating:  44%|██████████████████████████████▊                                       | 11/25 [00:07<00:10,  1.31it/s]\u001b[AThe VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "Epoch 0:  80%|████████████████████████████████████████          | 40/50 [01:30<00:22,  2.27s/it, loss=0.783, v_num=122]The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "\n",
      "Validating:  60%|██████████████████████████████████████████                            | 15/25 [00:08<00:04,  2.21it/s]\u001b[AThe VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "\n",
      "Epoch 0:  88%|████████████████████████████████████████████      | 44/50 [01:34<00:12,  2.15s/it, loss=0.783, v_num=122]\u001b[AThe VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "\n",
      "Epoch 0:  96%|████████████████████████████████████████████████  | 48/50 [01:34<00:03,  1.98s/it, loss=0.783, v_num=122]\u001b[AThe VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "The VALID BATCH END metrics are {'valid_loss_epoch': tensor(0.7063, device='cuda:0'), 'valid_loss': tensor(0.7316, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 25/25 [00:16<00:00,  1.43it/s]\u001b[ANote that the metrics when VALIDATION ENDS are {'valid_loss_epoch': tensor(0.8317, device='cuda:0'), 'valid_loss': tensor(0.7927, device='cuda:0'), 'train_loss_step': tensor(0.8007, device='cuda:0')}\n",
      "Epoch 0: 100%|██████████████████████████████████████████████████| 50/50 [01:38<00:00,  1.98s/it, loss=0.783, v_num=122]\n",
      "Epoch 1:   0%|                                                           | 0/50 [00:17<?, ?it/s, loss=0.783, v_num=122]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Craig\\miniconda3\\envs\\kidneys-cv\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = LitUNETDataLoader(train_loader, valid_loader)\n",
    "model = LitUNET(custom_unet, dice_loss, num_workers=16)\n",
    "trainer = pl.Trainer(gpus=1, callbacks=[LossSpikeAnalyzer()])\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[img.unlink() for img in (path/\"training_image_logs\").glob(\"*\") if img.name != \".ipynb_checkpoints\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-13e1e44aa950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#dls = DataLoaders.from_dsets(train_queue, valid_queue)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataLoaders' is not defined"
     ]
    }
   ],
   "source": [
    "dls = DataLoaders(train_loader, valid_loader)\n",
    "#dls = DataLoaders.from_dsets(train_queue, valid_queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know that we have predictions and targets with the same shape, and ultimately we want the predictions to be as close to 0 or 1 for each pixel.\n",
    "\n",
    "The **dice loss** function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = custom_unet\n",
    "model.cuda()\n",
    "learn = Learner(train_dl, model, loss_func=bce_dice_loss, metrics=acc_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create background segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "- [] incorporate scale into below functions\n",
    "- [] convert to a class as a list of functions is confusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encs = get_single_encs(get_id_by_index(0))\n",
    "#baseimage = get_single_img(get_id_by_index(0))\n",
    "\n",
    "#show_single_img_and_mask(get_id_by_index(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was concerned about memory management - note on this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TILE_SIZE = 256\n",
    "s_th = 40  #saturation blancking threshold\n",
    "p_th = 200*TILE_SIZE//TILE_SIZE #threshold for the minimum number of pixels\n",
    "reduce = 1\n",
    "x_tot,x2_tot = [],[]\n",
    "\n",
    "def image_tiles(df: pd.DataFrame = train_df):\n",
    "    \n",
    "    (path/\"formatted\"/str(TILE_SIZE)/\"imgs\").mkdir(parents=True, exist_ok=True)\n",
    "    impath = path/\"formatted\"/str(TILE_SIZE)/\"imgs\"\n",
    "    (path/\"formatted\"/str(TILE_SIZE)/\"masks\").mkdir(parents=True, exist_ok=True)\n",
    "    maskpath = path/\"formatted\"/str(TILE_SIZE)/\"masks\"\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for index, item in tqdm(enumerate(df.id), total=len(df)):\n",
    "\n",
    "        # read images and generate masks\n",
    "        img_id = get_id_by_index(index)\n",
    "        img = get_single_img(img_id)\n",
    "        mask = get_mask(img_id)\n",
    "\n",
    "        # add padding to make the image dividable into tiles\n",
    "        shape = img.shape\n",
    "        pad0 = (reduce*TILE_SIZE - shape[0]%(reduce*TILE_SIZE))%(reduce*TILE_SIZE)\n",
    "        pad1 = (reduce*TILE_SIZE - shape[1]%(reduce*TILE_SIZE))%(reduce*TILE_SIZE)\n",
    "        img = np.pad(img,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2],[0,0]],\n",
    "                    constant_values=0)\n",
    "        mask = np.pad(mask,[[pad0//2,pad0-pad0//2],[pad1//2,pad1-pad1//2]],\n",
    "                    constant_values=0)\n",
    "\n",
    "        #split image and mask into tiles using the reshape+transpose trick\n",
    "        img = cv2.resize(img,(img.shape[1]//reduce,img.shape[0]//reduce),\n",
    "                         interpolation = cv2.INTER_AREA)\n",
    "        img = img.reshape(img.shape[0]//TILE_SIZE,TILE_SIZE,img.shape[1]//TILE_SIZE,TILE_SIZE,3)\n",
    "        img = img.transpose(0,2,1,3,4).reshape(-1,TILE_SIZE,TILE_SIZE,3)\n",
    "\n",
    "        mask = cv2.resize(mask,(mask.shape[1]//reduce,mask.shape[0]//reduce),\n",
    "                          interpolation = cv2.INTER_NEAREST)\n",
    "        mask = mask.reshape(mask.shape[0]//TILE_SIZE,TILE_SIZE,mask.shape[1]//TILE_SIZE,TILE_SIZE)\n",
    "        mask = mask.transpose(0,2,1,3).reshape(-1,TILE_SIZE,TILE_SIZE)\n",
    "\n",
    "        #write data\n",
    "        for i,(im,m) in enumerate(zip(img,mask)):\n",
    "            #remove black or gray images based on saturation check\n",
    "            hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
    "            h, s, v = cv2.split(hsv)\n",
    "            if (s>s_th).sum() <= p_th or im.sum() <= p_th: continue\n",
    "\n",
    "            x_tot.append((im/255.0).reshape(-1,3).mean(0))\n",
    "            x2_tot.append(((im/255.0)**2).reshape(-1,3).mean(0))\n",
    "\n",
    "            im = cv2.imencode('.png',cv2.cvtColor(im, cv2.COLOR_RGB2BGR))[1]\n",
    "            cv2.imwrite((impath/f'{img_id}_{i}.png').as_posix(), im)\n",
    "            #m = cv2.imencode('.png',m)[1]\n",
    "            #cv2.imwrite(str(path.absolute()/\"train\"/\"formatted\"/\"masks\"/f'{index}_{i}.png'), m)\n",
    "            output.append((img_id, i, im, m))\n",
    "            #if i % 1000 == 0:\n",
    "                #print(type(im))\n",
    "                #return plt.imshow(im)\n",
    "            del i, im, m\n",
    "        del img, mask, pad0, pad1, item, index\n",
    "    output_data = pd.DataFrame(output, columns=[\"parent_img_id\", \"tile_index\", \"img\", \"mask\"])\n",
    "    del output\n",
    "    gc.collect()\n",
    "    return output_data\n",
    "    #print(type(output_data['img'][0]))\n",
    "    #output_data.to_csv(path/\"train\"/\"train_tiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = image_tiles(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't recreate the dataset everytime - pull from local directory if available as pickle file\n",
    "# I understand pickles aren't safe - so only do this in your local env and never open an unfamiliar pickle file\n",
    "if not (path/\"datastore.pkl\").exists():\n",
    "    train_data = image_tiles(train_df)\n",
    "    train_data.to_pickle(path/\"datastore.pkl\")\n",
    "if (path/\"datastore.pkl\").exists():\n",
    "    train_data = pd.read_pickle(\"datastore.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on Python memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to take a look at how much RAM this programme has taken thus far - when I opened up Windows Task Manager I noticed that after the above loop ran, my memory usage was at 64% (20.3GB out of 32GB). However when I look at the size of the output DataFrame, it is only 16.8MB: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having spent several hours researching this, I have come to conclude the following:\n",
    "\n",
    "* Python memory manages on its own - this is one of its advantages to lower-level languages like C.\n",
    "    * Any attempt to help with this doesn't really work. I tried using `del` and then `gc.collect()` after several points in the loop above and not only did they not reduce the memory usage at the end of the run, but adding `gc.collect()` everywhere slowed down the loop significantly (like 4x).\n",
    "    * I think this approach may be useful if you were running out of memory mid-run (ie. getting an `out of memory` error). However this wasn't the case for me\n",
    "    * I tried memory management libraries like `objgraph` but they didn't really help shed any more light on this\n",
    "* From some of the reading I've done, it looks like Windows may not \"recall\" the unused memory from Python after it has been used (it assumes the programme may need similar amounts in the future). Therefore despite my Task Manager Memory still sitting at 64% used, I can only conclude that this is misleading - and Windows would recover this if I ran another programme\n",
    "    * Again it seems that objects in Python are assigned for deletion (either automatically or when `del` is used) - however all this means is that it *can* be overwritten (ie. when some other programme or Python variable needs it). An object isn't just overwritten and assigned with zeroes or null - it will replace these tagged objects only when required\n",
    "    * *Update*: a simple test has confirmed this. I opened Microsoft Flight Simulator 2020 (knowing it was memory intensive) and Python's memory usage dropped by 30% instantaneously. Closing MFS (ie. removing off memory) has reduced the current baseline memory usage now to 16GB (50% max capacity). This confirms that Windows 10 looks like Python is storing onto objects in memory, when in fact that memory is available.\n",
    "    * Finally, while this experiment has ensured my approach is still OK (ie. using DataFrames instead of downloading images locally into files), memory management is still something to be aware of - ensuring objects can be overwritten by other variables/programmes (by not unnecessarily storing large objects in your programme) helps ensure the system stays functional\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image stats\n",
    "img_avr =  np.array(x_tot).mean(0)\n",
    "img_std =  np.sqrt(np.array(x2_tot).mean(0) - img_avr**2)\n",
    "print('mean:',img_avr, ', std:', img_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the key here is converting the mask numpy array to HSV values through **changing colorspaces**. The function `cv2.cvtColor` takes an image and a colorspace to change to. In our case, we are converting the numpy arrays (which only contain 0s and 1s - we replaced 0s with 1s in the `rle2mask` function) to HSV values, and summing the number of 1s in each image.\n",
    "\n",
    "We then show a random tile set depending on if we want the presence of glomeruli (generally yes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3_channels(x):\n",
    "    return np.stack((x, x, x), 2)\n",
    "\n",
    "# Function to return images with or without glom\n",
    "def get_glom_idxs(glom: bool = True):\n",
    "    idxs=[]\n",
    "    for i, item in enumerate(train_data[\"mask\"]):\n",
    "        item = to_3_channels(item)\n",
    "        h,s,v = cv2.split(cv2.cvtColor(item, cv2.COLOR_BGR2HSV))\n",
    "        if glom:\n",
    "            if v.sum() > 0:\n",
    "                idxs.append(i)\n",
    "        else:\n",
    "            if v.sum() < 0:\n",
    "                idxs.append(i)\n",
    "    return idxs\n",
    "\n",
    "def show_tile(index: int=None, glom: bool = True):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    if not index:\n",
    "        index = random.choice(get_glom_idxs(glom=glom))\n",
    "    mask = train_data[\"mask\"][index]\n",
    "    img = train_data[\"img\"][index]\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image\", fontsize=18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, cmap=\"hot\", alpha=0.5)\n",
    "    plt.title(f\"Image + mask\", fontsize=18)    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, cmap=\"hot\")\n",
    "    plt.title(f\"Mask\", fontsize=18)    \n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "show_tile(241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the Digit Recognizer notebook, the data here is stored in the DataFrame as pixel values instead of as larger images in local directories. Therefore we need to create a custom `Transform` to process this data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed something while inspecting the data: not only is the dataset imbalanced, but even if there is a presence of a glomeruli, it could be just a few pixels. So the question is - do we include these or not?\n",
    "\n",
    "Below we are adding to the dataframe a column that evaluates the relative presence of glomeruli in an image. It evaluates how many pixels a glomeruli take up in an image - and if its below a certain threshold (default 5%) then we tag it as such and may remove for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glom_present(x, thresh=0.05):\n",
    "    mask = to_3_channels(x)\n",
    "    h,s,v = cv2.split(cv2.cvtColor(mask, cv2.COLOR_BGR2HSV))\n",
    "    if v.sum()>0:\n",
    "        if v.sum()/(TILE_SIZE**2) < thresh:\n",
    "            return \"partial\"\n",
    "        return \"yes\"\n",
    "    return \"no\"\n",
    "\n",
    "# apply() over dataframe has been very fast for me in the past \n",
    "train_data[\"glom_present\"] = train_data[\"mask\"].apply(glom_present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of image type categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"glom_present\"].value_counts().plot(kind=\"bar\", title=\"Presence of Glomeruli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_partial = train_data[train_data[\"glom_present\"] != \"partial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KidneyTransform(Transform):\n",
    "    def encodes(self, df: pd.core.series.Series):\n",
    "        #return PILImage.create(tensor(df[\"img\"]))\n",
    "        return tensor(df[\"img\"]).transpose(2,0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms = TfmdLists(train_data, KidneyTransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlomMaskImg(Transform):\n",
    "    def encodes(self, df: pd.core.series.Series):\n",
    "        #return TensorCategory(df[''])\n",
    "        mask = to_3_channels(df[\"mask\"])\n",
    "        h,s,v = cv2.split(cv2.cvtColor(mask, cv2.COLOR_BGR2HSV))\n",
    "        #return v.sum()\n",
    "        return PILMask.create(df[\"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HasGlom(Transform):\n",
    "    def encodes(self, df: pd.core.series.Series):\n",
    "        if df[\"glom_present\"] == \"yes\":\n",
    "            return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlomMask(Transform):\n",
    "    def encodes(self, df: pd.core.series.Series):\n",
    "        return tensor(df[\"mask\"]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tfms = TfmdLists(train_data_no_partial, GlomMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(y_tfms[241], cmap=\"hot\")\n",
    "y_tfms[241].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a place for this\n",
    "\n",
    "* **Input normalisation**: important because some images or features may have *more variability* than others - if you do not control for this, large values will dominate and cascade through the network resulting in the *exploding gradient* problem. I also think the model will not generalise as well and will learn to identify only features with low variance. However by **subtracting the mean and dividing by stdev** you ensure the variability is consistent across all features.\n",
    "    * This may not matter as much for images (although it is still important), imagine a tabular dataset with values on completely different scales (cost in dollars vs satisfaction 1-5) - by normalising the data you will have better luck finding the optimal parameters for the lowest loss function\n",
    "    * Actually this does matter for images - doing this trains the model faster and improves the loss function (avoids local iminima)\n",
    "    * **NOTE**: you should normalise the validation and test sets using the **same mean and stdev** as the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KidneyTransform(Transform):\n",
    "    def encodes(self, df: pd.core.series.Series):\n",
    "        #return PILImage.create(tensor(df[\"img\"]))\n",
    "        return tensor(df[\"img\"]).transpose(2,0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms = TfmdLists(train_data, KidneyTransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfms[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlomMask(Transform):\n",
    "    \"\"\"\n",
    "    Unlike the x values, this should return a scalar not a tensor - therefore no float()\n",
    "    \"\"\"\n",
    "    def encodes(self, df: pd.core.series.Series):\n",
    "        return tensor(df[\"mask\"]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tfms = TfmdLists(train_data, GlomMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(y_tfms[241], cmap=\"hot\")\n",
    "y_tfms[241].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_imgs = train_data.parent_img_id.unique().tolist()\n",
    "assert len(parent_imgs[:-2]) + len(parent_imgs[-2:]) == len(parent_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = len([im for im in train_data[\"parent_img_id\"].tolist() if im in parent_imgs[:-2]])\n",
    "splits = [list(range(split_idx)), list(range(split_idx,len(train_data)))]\n",
    "\n",
    "# need to be careful here - may not want all of these transforms applied to validation set in the future\n",
    "tfms = [[KidneyTransform], [GlomMask]]\n",
    "ds = Datasets(train_data, tfms, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having trouble with cuda so creating a new dataloaders on cpu\n",
    "\n",
    "#dls = ds.dataloaders(bs=64, num_workers=0).cuda()\n",
    "dls = ds.dataloaders(bs=8, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dls, model, loss_fn, optimizer, acc_fn, epochs):\n",
    "    model.cuda()\n",
    "    \n",
    "    train_loss, valid_loss = [], []\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "    \n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = dls.train\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = dls.valid\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "            step = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for x, y in dataloader:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                if phase == 'train':\n",
    "                    # zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "\n",
    "                    # the backward pass frees the graph memory, so there is no \n",
    "                    # need for torch.no_grad in this training pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # scheduler.step()\n",
    "\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y.long())\n",
    "\n",
    "                # stats - whatever is the phase\n",
    "                acc = acc_fn(outputs, y)\n",
    "\n",
    "                running_acc  += acc*dataloader.bs\n",
    "                running_loss += loss*dataloader.bs \n",
    "\n",
    "                if step % 10 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))\n",
    "                    # print(torch.cuda.memory_summary())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_acc / len(dataloader.dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return train_loss, valid_loss    \n",
    "\n",
    "opt = torch.optim.Adam(custom_unet.parameters(), lr=0.01)\n",
    "#train_loss, valid_loss = train(dls, custom_unet, dice_loss, opt, acc_metric, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"del custom_unet, dls, learn\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_unet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, custom_unet, loss_func=dice_loss, cbs=ActivationStats(with_hist=True), metrics=acc_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.activation_stats.plot_layer_stats(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional notes\n",
    "\n",
    "* The number of blocks and the input/output numbers is arbitrary - you need to test different combinations (always considering the input size of your image - no sense windling it down to a vector of 1 pixel)\n",
    "* The only thing you need to create in a model architecture to then be used in fastai is CHECK THIS the `forward` method\n",
    "    * Is this true? See the function we created above and ran through the first training loop - all it was was a sequential class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this is entirely based on the fantastic [fastbook Chapter 13](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is so popular for analysing images because it is great at finding edges and shapes, which help to differentiate a 3 for a 7, or a car vs a human. More specifically to quote [machinelearningmastery.com](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/#:~:text=Pooling%20layers%20provide%20an%20approach,presence%20of%20a%20feature%20respectively.):\n",
    "\n",
    "> stacking convolutional layers in deep models allows layers **close to the input to learn low-level features (e.g. lines)** and **layers deeper in the model to learn high-order or more abstract features, like shapes or specific objects**\n",
    "\n",
    "Some key terms:\n",
    "* **kernel**: a little matrix (such as 3x3) that looks for specific patterns in an image - the presence of the pattern returns a number greater than 0; the absence, close to 0.\n",
    "* **convolution**: applies the kernel across the entire image - each 3x3 section of the image is multiplied elementwise by the 3x3 kernel and the values are **summed together**.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/fastai/fastbook/3916b71bdf2f9e587ac82f3c2ef4aabd05b8f51c/images/chapter9_conv_basic.png\" width=750/>\n",
    "\n",
    "Note that kernels can take all kinds of \"patterns\" - and its the different patterns that can distinguish different shapes within an image. \n",
    "\n",
    "Therefore in the image above, if we assume that kernel was set to look for rounded lines, we could conclude that there is a rounded line in the image at that location because **the *sum* of the kernel x image is 94.2** which is much greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each of the examples below, the \n",
    "\n",
    "top_edge = tensor([[-1,-1,-1],\n",
    "                   [ 0, 0, 0],\n",
    "                   [ 1, 1, 1]]).float()\n",
    "\n",
    "left_edge = tensor([[-1,1,0],\n",
    "                    [-1,1,0],\n",
    "                    [-1,1,0]]).float()\n",
    "\n",
    "diag1_edge = tensor([[ 0,-1, 1],\n",
    "                     [-1, 1, 0],\n",
    "                     [ 1, 0, 0]]).float()\n",
    "\n",
    "diag2_edge = tensor([[ 1,-1, 0],\n",
    "                     [ 0, 1,-1],\n",
    "                     [ 0, 0, 1]]).float()\n",
    "\n",
    "edge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge]).unsqueeze(1) # required to be a rank-4 tensor\n",
    "edge_kernels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to preform this calculation with the Pytorch tools, we need to undertsand the basic conv function `F.conv2d`\n",
    "\n",
    "The first two parameters:\n",
    "* **input**:: input tensor of shape (minibatch, in_channels, iH, iW)\n",
    "* **weight**:: filters of shape (out_channels, in_channels, kH, kW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In place of a mini-batch, we will just use a single image for now representing the mini-batch, with a single channel. However it is important to note that Pytorch is able to perform the convolutions on the entire batch **simultaneously**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a rank-4 tensor analogous to a minibatch\n",
    "def get_single_channel_img(idx):\n",
    "    im = tensor(train_data[\"img\"][idx]) # returns rank 3 tensor with channel in rightmost position\n",
    "    im = im.transpose(2,0) # move channel to leftmost - cannot use view or reshape\n",
    "    im = im[0] # take only one of the 3 channels (we are removing data here so this is only for example)\n",
    "    im = im.unsqueeze(0).unsqueeze(0) # add back the channel we just stripped away, plus add another\n",
    "    return im.float()\n",
    "\n",
    "test_image = get_single_channel_img(257)\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(test_image[0]) # take the first item of the minibatch, even though there is only 1 image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noiw that we have our image (representing a minibatch) and tensor of kernels, we can run a convolution over the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_img = F.conv2d(test_image, edge_kernels)\n",
    "conv_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `F.conv2d` is a **channel for every kernel**. Because the kernels look for different things (the patterns of the kernels in our `edge_kernels` object each look for specific shapes), each of the channels output something different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(conv_img[0][0]), show_image(conv_img[0][1]), show_image(conv_img[0][2]), show_image(conv_img[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding** is used to ensure that the output of the convolution produces an image of the same size (and ensure the edge of the image can be processed).\n",
    "\n",
    "<img src=\"https://github.com/fastai/fastbook/raw/3916b71bdf2f9e587ac82f3c2ef4aabd05b8f51c/images/att_00030.png\" width=750/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a rule of thumb that kernels only have **odd dimensions** (presumably because a kernel with even dimensions would have to have different padding on different axes). In order to calculate the padding for a kernel of size `k` you simply get the **floor division** aka **interger division**.\n",
    "\n",
    "Therefore in the image above, with a kernel of size 3x3, the padding is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3//2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stride** is the amount of pixels you more the kernel over. By increasing the **stride from 1 to 2**, the output of the convnet is **twice as small**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple refactored convnet step that uses `nn.Conv2d` instead of `F.conv2d` because the former creates the **weight matrix** (ie. the **values in the kernels**) when we instantiate it.\n",
    "\n",
    "Remember that with CNNs, we use backpropogation to improve the kernel values of the filters - aka the weight matrices - against which gradients are calculated to improve the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(ni, nf, ks=3, act=True):\n",
    "    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n",
    "    if act: res = nn.Sequential(res, nn.ReLU())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, the output is half the size of the input (due to the stride of 2). Again the kernel size is an odd number, and the interger division of this odd number is the padding. Equally important is specifying the number of filters (ie. the number of small matrices that collect an output for a convolution).\n",
    "\n",
    "In the simple CNN below, we can see that an image input of 28\\*28 pixels is **reduced in size from each convolution** because of the stride. However to combat the possible loss in capacity of each layer, we **increase twofold the number of filters in each convolution** - these are the values suppiled below. Like hidden layers in a perceptron, *we can decide how many filters we want*, and each of them will learn to specialise in something.\n",
    "\n",
    "Also note that the size of the input isn't needed - the filters in each convolution are **applied over every pixel that is supplied** to it. This is why there is a 1 as input in the first conv.\n",
    "\n",
    "Finally to get the output of a single value, we **repeat the number of convs until the output is reduced** to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = sequential(\n",
    "    conv(1 ,4),            #14x14\n",
    "    conv(4 ,8),            #7x7\n",
    "    conv(8 ,16),           #4x4\n",
    "    conv(16,32),           #2x2\n",
    "    conv(32,2, act=False), #1x1\n",
    "    Flatten(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For colour images, our kernels grow in the third dimension with the number of channels supplied - ie. the same kernel isn't used for each of the channels. \n",
    "\n",
    "**Bias** is present for every kernel - each kernel has a correspondinding bias, again much like the perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the *number of output features in the first layer and how it relates to the kernel size*. If we were to increase that number from 4 to 8, we would have a kernel size of 3x3=9 trying to differentiate on 8 features - this is hard for them to learn much of anything. But if we increase the kernel size, it is *more likely that 25 pixels can distinguish 8 unique and useful patterns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn():\n",
    "    return sequential(\n",
    "        conv(1 ,8, ks=5),        #14x14\n",
    "        conv(8 ,16),             #7x7\n",
    "        conv(16,32),             #4x4\n",
    "        conv(32,64),             #2x2\n",
    "        conv(64,10, act=False),  #1x1\n",
    "        Flatten(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we evaluate model effectiveness? Through **callbacks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `ActivationStats` callback to evaluate the following scenarios:\n",
    "\n",
    "* **Mean** of activations - ideally should be normally distributed and its change should be smooth (consistent) during training\n",
    "* **Standard deviation** of activations - also its change should be smooth (consistent) during training\n",
    "* **% of activations near 0** - we want to **reduce this as much as possible** - activations of/at zero don't provide any benefit (0x0=0 and cause the subsequent layers to also have 0s) and **take up valuable space**\n",
    "    * *Poor initialisation* **compounds the frequency of 0 activations** over the layers and results in most of the final layers being 0:\n",
    "    ![](https://cdn.craigstanton.com/images/ai/fastai/bad_training_stats.png)\n",
    "\n",
    "By watching the `ActivationStats` during training by plotting `learn.activation_stats.plot_layer_stats(0)` you can see how these change and make "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the solutions to the issues above could be:\n",
    "* increasing batch size\n",
    "* variable learning rate\n",
    "* batch normalisation\n",
    "\n",
    "#### 1cycle Training\n",
    "\n",
    "If you have randomly initialised weights, they clearly are not good. Therefore having a high learning rate at the start of training risks missing the loss minimum and thus wastes time (or completely disguises - causing divergence) finding optimal weights.\n",
    "\n",
    "> Remember, `weights.data -= weights.grad * learning_rate`. \n",
    "\n",
    "Again if the learning rate is high when we are just starting out while using random values, you are moving the weights massively - how can they consistently move in the right direction? Equally we don't want a high learning rate near the end of training. Therefore, we should change the learning rate during training, from low, to high, and then back to low again\n",
    "\n",
    "fastai scales the learning rate up and down - but you need to specify the max lr to be used. Use the `fit_one_cycle` method you can specify the following parameters:\n",
    "* *lr_max*: The highest learning rate that will be used (this can also be a list of learning rates for each layer group, or a Python slice object containing the first and last layer group learning rates)\n",
    "* *div*: How much to divide lr_max by to get the starting learning rate\n",
    "* *div_final*: How much to divide lr_max by to get the ending learning rate\n",
    "* *pct_start*: What percentage of the batches to use for the warmup\n",
    "* *moms*: A tuple (mom1,mom2,mom3) where mom1 is the initial momentum, mom2 is the minimum momentum, and mom3 is the final momentum\n",
    "\n",
    "Note: **high learning rates aren't all bad**. Higher learning rates in the middle of training allow for **faster training** and also help **prevent overfitting** (by *passing over sharp local minima* - and thus making the model more **generalisable**). By preventing overfitting (passing the sharp local minima), we satisfy the rule that a model that generalizes well is one whose loss would not change very much if you changed the input by a small amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch normalisation\n",
    "\n",
    "The 1cycle training doesn't completely solve our training inefficiency - there is still a propensity to return the parameters to zero. Using fastai's *colorful dimensions* we can visualise the distribution of parameter values (through colorful histograms) with the goal of having normally distributed activations.\n",
    "\n",
    "The problem arises that the distibution of each layer *changes during training* as the previous layers impact the next layers - this requires lower learning rates (and thus slower training) and careful parameter initialisation. This phenomenon is called **internal covariate shift**.\n",
    "\n",
    "**Batch normalisation** (aka **batchnorm**) tries to counter this by using the *mean and standard deviation of the **prior layer** to **normalise the next layer***. This technique allows us to use much higher learning rates and be less careful about initialization.\n",
    "\n",
    "However there is a catch (of course!) - with normalisation alone, we \"dampen\" activations that may require a high value (ie. a very specific attribute of the image is a huge indicator of a class - therefore specific activations are required to be really high in order to make accurate predictions). To counter this, we add **two learnable parameters `gamma` and `beta`**. The output of each layer is therefore `gamma*y + beta` where `y` has been normalised with the previous layer's parameter mean and standard deviation.\n",
    "\n",
    "As stated, models with batchnorm tend to generalise well. To quote the fastbook directly:\n",
    "> most researchers believe that the reason for this is that batch normalization adds some extra randomness to the training process. Each mini-batch will have a somewhat different mean and standard deviation than other mini-batches. Therefore, the activations will be normalized by different values each time. In order for the model to make accurate predictions, it will have to learn to become robust to these variations. In general, adding additional randomization to the training process often helps.\n",
    "\n",
    "This is a helpful reminder - training happens over a batch, where all images are processed simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??nn.BatchNorm2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical concepts\n",
    "\n",
    "* The size of the input image isnt specified as the conv2d is applied over every pixel that is supplied to it\n",
    "* In order to end up with an output of a single value, we often repeat the number of convs until this happens (ie. number of convs needed is 28 / 2...)\n",
    "* The first input of `1` looks like a placeholder. However each of the next conv layers takes the output of the previous one\n",
    "* Because we are using a stride of 2, the output of each layer is a vector that is half the size of the input. To prevent the lack of feature recognition as this happens, we doube the number of filters in each conv\n",
    "* Jargon: filters vs channels vs kernels vs weight matrix\n",
    "    * **kernel**: a symmetrical matrix of weights applied to each pixel in an image\n",
    "    * **filter**: for a 2D convolution, a filter is the same as a kernel. For a 3D convolution, the filter refers to the collection of kernels as a unit\n",
    "        * Also important that filters are often discussed in terms of number of filters (ie. the second layer has 30 filters)\n",
    "    * **channels**: also mean filters, but can also refer to the input image - channels in this context are different\n",
    "    * **weight matrix**: the matrix of values (ie. the kernel values) that are updated on the backwards pass\n",
    "    * **activations**: the actual values of the weight matrix/kernels\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final note on CNNs: \n",
    "> Although the universal approximation theorem shows that it should be possible to represent anything in a fully connected network in one hidden layer, we've seen now that in practice we can train much better models by being thoughtful about network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kidneys-cv] *",
   "language": "python",
   "name": "conda-env-kidneys-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
