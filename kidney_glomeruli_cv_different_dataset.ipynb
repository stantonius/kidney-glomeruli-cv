{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuBMAP - Hacking the Kidney - Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Kaggle competition page](https://www.kaggle.com/c/hubmap-kidney-segmentation)\n",
    "\n",
    "Helpful Notebooks:\n",
    "* [https://www.kaggle.com/markalavin/hubmap-tile-images-w-overlap-and-build-tfrecords](https://www.kaggle.com/markalavin/hubmap-tile-images-w-overlap-and-build-tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDO\n",
    "* Look at impact of different affine matrices\n",
    "* Look at impact of removing alpha channel on model size and performance\n",
    "* Add Deepmind's architecture optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Downloads for Offline use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update -n base conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda config --set always_yes True\n",
    "# ! conda install -c fastai -c pytorch fastai\n",
    "# #! conda install pytorch torchvision torchaudio fastai -c pytorch\n",
    "# #! conda update pytorch torchvision torchaudio cudatoolkit -c pytorch\n",
    "# ! conda install pandas\n",
    "# ! conda install -c conda-forge kaggle\n",
    "# ! conda install -c conda-forge tifffile\n",
    "# ! conda install -c conda-forge tqdm\n",
    "# ! conda install -c conda-forge matplotlib\n",
    "# ! conda install -c conda-forge pytorch-lightning\n",
    "# ! conda install -c conda-forge wandb\n",
    "# ! conda install -c conda-forge arrow\n",
    "# !conda install -c conda-forge pickle5\n",
    "# !conda install -n base -c conda-forge jupyterlab_widgets\n",
    "# !conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install arrow pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed if running in wsl2\n",
    "#! pip install pytorch-lightning wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea why the conda-forge version doesn't work\n",
    "\n",
    "#!python -m pip install opencv-python\n",
    "\n",
    "# If you are running this notebook on a server (like Linux on WSL2) you need the headless version of opencv\n",
    "# The regular opencv requires GUI packages that serves dont have, and will raise an error\n",
    "#!python -m pip install opencv-python-headless\n",
    "\n",
    "# temporary solution to use tab complete - something wrong with jupyter jedi - need to downgrade\n",
    "#!pip install jedi==0.17.2\n",
    "\n",
    "# !pip install torchio --upgrade\n",
    "\n",
    "#!pip install pytorch-lightning-bolts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the finicky local CUDA is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# prebuilt models\n",
    "# from pl_bolts.models import UNet\n",
    "\n",
    "import tensorboard as tb\n",
    "\n",
    "# Need to put kaggle.json in /%USERS%/.kaggle folder (C:/Users/Craig/.kaggle)\n",
    "try:\n",
    "    import kaggle\n",
    "except:\n",
    "    !echo '{\"username\":\"canadarmy\",\"key\": KAGGLEKEY}' > ~/.kaggle/kaggle.json\n",
    "    import kaggle\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from typing import Union\n",
    "\n",
    "# Read tiff images\n",
    "import tifffile\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torchio as tio\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import time\n",
    "import wandb\n",
    "import arrow\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Memory management tools\n",
    "import gc\n",
    "\n",
    "import timm\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.imports import *\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.metrics import Dice, Jaccard, JaccardCoeff\n",
    "\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./data\")\n",
    "# kaggle.api.dataset_download_files(\"iafoss/hubmap-1024x1024\", path=data_path)\n",
    "# kaggle.api.dataset_download_files(\"baesiann/glomeruli-hubmap-external-1024x1024\", path=data_path)\n",
    "# kaggle.api.dataset_download_files(\"iafoss/hubmap-256x256\", path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you are about to download the data in the cvorrect directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the data in the correct folder - commented out so as to not repeat the unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "\n",
    "# with zipfile.ZipFile(data_path/\"glomeruli-hubmap-external-1024x1024.zip\", 'r') as zipref:\n",
    "#     zipref.extractall(data_path)\n",
    "    \n",
    "# (data_path/\"masks_1024\").rename(data_path/\"masks_ext_1024\")\n",
    "# (data_path/\"images_1024\").rename(data_path/\"images_ext_1024\")\n",
    "    \n",
    "# with zipfile.ZipFile(data_path/\"hubmap-1024x1024.zip\", 'r') as zipref:\n",
    "#     zipref.extractall(data_path)\n",
    "    \n",
    "# (data_path/\"masks\").rename(data_path/\"masks_1024\")\n",
    "# (data_path/\"train\").rename(data_path/\"images_1024\")\n",
    "    \n",
    "# with zipfile.ZipFile(data_path/\"hubmap-256x256.zip\", 'r') as zipref:\n",
    "#     zipref.extractall(data_path)\n",
    "\n",
    "# (data_path/\"masks\").rename(data_path/\"masks_256\")\n",
    "# (data_path/\"train\").rename(data_path/\"images_256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((data_path/\"images_256\").ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len((data_path/\"images_1024\").ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these two datasets are the same - 1 is just higher resolution than the other, we need to rename one set because most of the file names are the same\n",
    "\n",
    "I have also confirmed that in rach folder, the masks and the images are the same name. Se we will rename the 256 pixel images with a `-256` suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in (data_path/\"images_256\").ls():\n",
    "#     file.rename(data_path/\"images_256\"/(file.name.split(\".\")[0]+\"-256\"+\".png\"))\n",
    "    \n",
    "# for file in (data_path/\"masks_256\").ls():\n",
    "#     file.rename(data_path/\"masks_256\"/(file.name.split(\".\")[0]+\"-256\"+\".png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Main Functions\n",
    "################\n",
    "\n",
    "\n",
    "def rle2mask(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: encoding string value from csv\n",
    "    shape: (width,height) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    # return a list of starting pixels and a list of lengths\n",
    "    starts, lengths = [\n",
    "        np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])\n",
    "    ]\n",
    "    # subtract 1 from every starting pixel\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    # calculate a background of 0 (empty) with size defined by image\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    # replace every 0 within each range with 1\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = 1\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def mask2rle(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "# def get_id_by_index(index, df=train_df):\n",
    "#     return df.iloc[index]['img_id']\n",
    "\n",
    "def get_single_img(id, folder=\"train\"):\n",
    "    img = tifffile.imread(path/folder/(id+\".tiff\"))\n",
    "    if len(img.shape) == 5:\n",
    "        img = img.squeeze().transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "def show_single_img(id, **kwargs):\n",
    "    return plt.imshow(get_single_img(id), **kwargs)\n",
    "\n",
    "# def show_img_by_index(index, df=train_df):\n",
    "#     return plt.imshow(tifffile.imread(path/\"train\"/(train_df.iloc[TEST_IMAGE_INDEX]['id']+\".tiff\")))\n",
    "\n",
    "# def get_single_encs(id, df=train_df):\n",
    "#     return df[df['img_id'] == id]['encoding'].array[0]\n",
    "\n",
    "# def get_mask(id, df=train_df, folder=\"train\"):\n",
    "#     return rle2mask(\n",
    "#         get_single_encs(id, df=df),\n",
    "#         get_single_img(id, folder=folder).shape[::-1][1:]\n",
    "#     )\n",
    "\n",
    "def show_single_img_and_mask_by_id(id):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    mask = get_mask(id)\n",
    "    img = get_single_img(id)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image\", fontsize=18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, cmap=\"hot\", alpha=0.5)\n",
    "    plt.title(f\"Image + mask\", fontsize=18)    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, cmap=\"hot\")\n",
    "    plt.title(f\"Mask\", fontsize=18)    \n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def show_single_img_and_mask(subject: tio.data.subject.Subject, resize_to=50):\n",
    "    plt.figure(figsize=(120, 100))\n",
    "    \n",
    "    if not isinstance(subject, tio.data.subject.Subject):\n",
    "        raise TypeError(f\"The subject is required to be of type torchio.data.subject.Subject but you provided {type(subject)}\")\n",
    "    \n",
    "    img = subject[\"img\"][tio.DATA].squeeze().permute(1,2,0)\n",
    "    mask = subject[\"mask\"][tio.DATA].squeeze().unsqueeze(2)\n",
    "    \n",
    "    if resize_to:\n",
    "        img = resizer(img, scale=resize_to)\n",
    "        mask = resizer(mask, scale=resize_to)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image\", fontsize=18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, cmap=\"hot\", alpha=0.5)\n",
    "    plt.title(f\"Image + mask\", fontsize=18)    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, cmap=\"hot\")\n",
    "    plt.title(f\"Mask\", fontsize=18)    \n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def to_4d(img, input_chan_first=False, output_chan_first=True):\n",
    "    if not len(img.shape)==3:\n",
    "        raise ValueError(\"Function only converts 3D arrayto 4D array\")\n",
    "    return np.expand_dims(np.transpose(img, \n",
    "                   (0,1,2) if input_chan_first else (2,0,1)), \n",
    "                   3 if output_chan_first else 0)\n",
    "\n",
    "def to_3d(img, input_chan_first=True, output_chan_first=False):\n",
    "    if not len(img.shape)==4:\n",
    "        raise ValueError(\"Function only converts 4D arrayto 3D array\")\n",
    "    return np.transpose(img.squeeze(), (0,1,2) if output_chan_first else (1,2,0))\n",
    "\n",
    "def to_3chan(x, dim=1):\n",
    "    return torch.cat((x,x,x), dim=dim)\n",
    "\n",
    "def resizer(img, scale=5, show=False):\n",
    "    \"\"\"\n",
    "    Returns an smaller array of the same dimensions, but converts to 3D to allow for resizing\n",
    "    \"\"\"\n",
    "    scale_percent = scale # percent of original size\n",
    "    im_dims = (len(img.shape) == 4)\n",
    "    if im_dims:\n",
    "        img = to_3d(img)\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    img_reshaped = cv2.resize(img.numpy(), dim)\n",
    "    if show:\n",
    "        return plt.imshow(img_reshaped)\n",
    "    if im_dims:\n",
    "        return to_4d(img_reshaped)\n",
    "    return img_reshaped\n",
    "\n",
    "def squeeze_and_reshape(img_tensor, remove_alpha=False):\n",
    "    if not isinstance(img_tensor, torch.Tensor):\n",
    "        raise TypeError(\"Image needs to be a tensor\")\n",
    "    if len(img_tensor.shape) == 5:\n",
    "        img_tensor = img_tensor.squeeze().permute(2, 1, 0)\n",
    "    if img_tensor.shape[0] == 3:\n",
    "        img_tensor = img_tensor.permute(2, 1, 0)\n",
    "    img_tensor = img_tensor.unsqueeze(2).permute(3,1,0,2)\n",
    "    return img_tensor\n",
    "\n",
    "def to_pil(image):\n",
    "    # for \n",
    "    data = image.numpy().squeeze().T\n",
    "    data = data.astype(np.uint8)\n",
    "    image = Image.fromarray(data)\n",
    "    w, h = image.size\n",
    "    display(image)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remask(img, mask, tile, threshold=8, show=False):\n",
    "    \n",
    "    img_height = img.shape[1]\n",
    "    img_width = img.shape[0]\n",
    "    \n",
    "    number_of_vertical_tiles = (img_height // tile)+1\n",
    "    number_of_horizontal_tiles = (img_width // tile)+1\n",
    "    \n",
    "    #background = np.zeros((tile*number_of_horizontal_tiles, tile*number_of_vertical_tiles,3))[:img.shape[0],:img.shape[1],:img.shape[2]]\n",
    "    \n",
    "    tile_coords = []\n",
    "    for h_idx in range(number_of_horizontal_tiles):\n",
    "        for v_idx in range(number_of_vertical_tiles):\n",
    "            tile_coords.append((h_idx+1, v_idx+1)) # +1 to remove 0 indexing\n",
    "\n",
    "    cropped_images = []\n",
    "    for h,v in tile_coords:\n",
    "        cropped_images.append((h, v, img[tile*(h-1):tile*h, tile*(v-1):tile*v, :]))\n",
    "        \n",
    "    for horiz,vert,im in cropped_images:\n",
    "        if not 0 in im.shape:      #required in case tile is \n",
    "            \n",
    "            hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
    "            h, s, v = cv2.split(hsv)\n",
    "            if s.mean() < threshold:\n",
    "                all_black = np.full((im.shape[0], im.shape[1]),2)\n",
    "                mask_dims = mask[tile*(horiz-1):tile*horiz,tile*(vert-1):tile*vert]\n",
    "                all_black = np.full((mask_dims.shape[0], mask_dims.shape[1]),2)\n",
    "                mask[tile*(horiz-1):tile*horiz,tile*(vert-1):tile*vert] = all_black\n",
    "                #im = im*0.\n",
    "            #background[tile*(horiz-1):tile*horiz,tile*(vert-1):tile*vert,:] = im\n",
    "    \n",
    "    if show:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img.astype('uint8'))\n",
    "        plt.title(f\"Image\", fontsize=18)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(img.astype('uint8'))\n",
    "        plt.imshow(mask.astype('uint8'), cmap=\"hot\", alpha=0.5)\n",
    "        plt.title(f\"Image + mask\", fontsize=18)    \n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(mask.astype('uint8'), cmap=\"hot\")\n",
    "        plt.title(f\"Mask\", fontsize=18)    \n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "#img_id = get_id_by_index(7)\n",
    "#img_id = '095bf7a1f'\n",
    "#with tifffile.TiffFile(path/\"train\"/(img_id+\".tiff\")) as tif:\n",
    "#    imgg = tif.asarray()\n",
    "#print(imgg.shape)\n",
    "#mask = get_mask(img_id)\n",
    "#new_mask = remask(to_3d(squeeze_and_reshape(imgg)), mask, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_df(df, directory):\n",
    "    mask_list = []\n",
    "    for idx,_ in tqdm(enumerate(df.iterrows()), total=len(df)):\n",
    "        img_id = get_id_by_index(idx, df=df)\n",
    "        with tifffile.TiffFile(path/directory/(img_id+\".tiff\")) as tif:\n",
    "            base_im = tif.asarray()\n",
    "            print(base_im.shape)\n",
    "            im_tensor = squeeze_and_reshape(torch.from_numpy(base_im)).numpy()\n",
    "            print(im_tensor.shape)\n",
    "            mask = remask(to_3d(im_tensor), get_mask(img_id), 1000)\n",
    "            print(f\"Mask shape is {mask.shape}\")\n",
    "            \n",
    "            cut_image(im_tensor, img_id, mask, write_path/\"smaller\")\n",
    "            \n",
    "            del base_im, im_tensor, mask\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_image(img_id, source_path:Path, destination_path: Path, mask_df=None):\n",
    "    \"\"\"\n",
    "    Cut image (and corresponding mask - in Dataframe - if supplied) into QUARTERS and save them to a directory called smaller\n",
    "    \"\"\"\n",
    "    \n",
    "    img = tio.Image(source_path/f\"{img_id}.tiff\").data\n",
    "    if len(img.shape) != 4:\n",
    "        raise ValueError(\"Tensor shape needs to have 4 dimensions\")\n",
    "    if img.shape[0] != 4:\n",
    "        raise ValueError(\"First dimension must have 4 channels\")\n",
    "    vertical_tiles = img.shape[2] // 2\n",
    "    horizontal_tiles = img.shape[1] // 2\n",
    "    \n",
    "\n",
    "    \n",
    "    img1 = tio.Pad(((512,512,0)))(img[:,:horizontal_tiles,:vertical_tiles,:])\n",
    "    tio.Image(tensor=img1).save(destination_path/\"imgs\"/f\"{img_id}_1.tiff\")\n",
    "    del img1\n",
    "    gc.collect()\n",
    "\n",
    "    img2 = tio.Pad((512,512,0))(img[:,horizontal_tiles:,:vertical_tiles,:])\n",
    "    tio.Image(tensor=img2).save(destination_path/\"imgs\"/f\"{img_id}_2.tiff\")\n",
    "    del img2\n",
    "    gc.collect()\n",
    "    \n",
    "    img3 = tio.Pad((512,512,0))(img[:,:horizontal_tiles,vertical_tiles:,:])\n",
    "    tio.Image(tensor=img3).save(destination_path/\"imgs\"/f\"{img_id}_3.tiff\")\n",
    "    del img3\n",
    "    gc.collect()\n",
    "    \n",
    "    img4 = tio.Pad((512,512,0))(img[:,horizontal_tiles:,vertical_tiles:,:])\n",
    "    tio.Image(tensor=img4).save(destination_path/\"imgs\"/f\"{img_id}_4.tiff\")\n",
    "    del img4\n",
    "    gc.collect()\n",
    "    \n",
    "    del img\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if not isinstance(mask_df, NoneType):\n",
    "        mask = torch.from_numpy(mask_df[mask_df[\"img_id\"]==img_id][\"mask\"].values[0]).unsqueeze(0).unsqueeze(3)\n",
    "        # I have managed to flip the axes somewhere and am too lazy or stubborn to fix the root issue. So need to permute axes\n",
    "        mask = mask.permute(0,2,1,3)\n",
    "        \n",
    "        mask1 = tio.Pad((512,512,0))(mask[:,:horizontal_tiles,:vertical_tiles,:])\n",
    "        tio.Image(tensor=mask1).save(destination_path/\"masks\"/f\"{img_id}_1_mask.tiff\")\n",
    "        del mask1\n",
    "        gc.collect()\n",
    "        \n",
    "        mask2 = tio.Pad((512,512,0))(mask[:,horizontal_tiles:,:vertical_tiles,:])\n",
    "        tio.Image(tensor=mask2).save(destination_path/\"masks\"/f\"{img_id}_2_mask.tiff\")\n",
    "        del mask2\n",
    "        gc.collect()\n",
    "        \n",
    "        mask3 = tio.Pad((512,512,0))(mask[:,:horizontal_tiles,vertical_tiles:,:])\n",
    "        tio.Image(tensor=mask3).save(destination_path/\"masks\"/f\"{img_id}_3_mask.tiff\")\n",
    "        del mask3\n",
    "        gc.collect()\n",
    "        \n",
    "        mask4 = tio.Pad((512,512,0))(mask[:,horizontal_tiles:,vertical_tiles:,:])\n",
    "        tio.Image(tensor=mask4).save(destination_path/\"masks\"/f\"{img_id}_4_mask.tiff\")\n",
    "        \n",
    "        del mask4\n",
    "        gc.collect()\n",
    "\n",
    "        del mask\n",
    "        gc.collect()\n",
    "        \n",
    "# Uncomment if these files dont exist\n",
    "# [cut_image(item, path/\"train\", path/\"smaller\", mask_df=new_masks) for item in new_masks.img_id.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restitch_image(img_id, pred_mask=None):\n",
    "    for name in (path/\"smaller/imgs\").glob(f\"{img_id}_?.tiff\"):\n",
    "        img_quarter = name.name.split(\"_\")[1].split(\".\")[0]\n",
    "        if img_quarter == \"1\":\n",
    "            img1 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "        if img_quarter == \"2\":\n",
    "            img2 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "        if img_quarter == \"3\":\n",
    "            img3 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "        if img_quarter == \"4\":\n",
    "            img4 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "    \n",
    "    # make a 4D tensor with 4 channels and 1 depth channel\n",
    "    whole_image = torch.zeros(\n",
    "        img1.shape[0],\n",
    "        img1.shape[1] + img3.shape[1],\n",
    "        img1.shape[2] + img2.shape[2],\n",
    "    ).unsqueeze(3)\n",
    "    \n",
    "    whole_image[:,:whole_image.shape[1]//2, :whole_image.shape[2]//2, :] = img1\n",
    "    whole_image[:,whole_image.shape[1]//2-1:, :whole_image.shape[2]//2, :] = img2\n",
    "    whole_image[:,:whole_image.shape[1]//2,  whole_image.shape[2]//2:, :] = img3\n",
    "    whole_image[:,whole_image.shape[1]//2-1:,  whole_image.shape[2]//2:, :] = img4\n",
    "    \n",
    "    to_pil(whole_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restitch_image(get_id_by_index(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    The user specifies what the first input channels size will be and the ultimate output size will be\n",
    "    The downblock and upblock functions also take input and out values - but these are PER CONVOLUTION\n",
    "    They do not necessarily inherit the values specified by the user\n",
    "    \n",
    "    The architecture is 3 down blocks, followed by 3 up blocks\n",
    "    Output is squeezed if the channel_out is 1 - masks are single channels so this matches the dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channel_in, channel_out, stride=1, ks=3):\n",
    "        super(CustomUnet, self).__init__()\n",
    "        self.down_conv1 = self._downblock(channel_in, 16, stride=stride, ks=ks)\n",
    "        self.down_conv2 = self._downblock(16, 32, stride=stride, ks=ks)\n",
    "        self.down_conv3 = self._downblock(32, 64, stride=stride, ks=ks)\n",
    "        self.up_conv3 = self._upblock(64, 32, stride=stride, ks=ks)\n",
    "        self.up_conv2 = self._upblock(32*2, 16, stride=stride, ks=ks) # key to notice the doubling of input size\n",
    "        self.up_conv1 = self._upblock(16*2, channel_out, stride=stride, ks=ks)\n",
    "    \n",
    "    # downward (contracting) block\n",
    "    def _downblock(self, n_in, n_out, stride, ks):\n",
    "        down_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_in, n_out, stride=stride, kernel_size=ks, padding=ks//2), \n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_out, n_out, stride=stride, kernel_size=ks, padding=ks//2), \n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=ks, stride=2, padding=ks//2) # 256/2 = 128\n",
    "        )\n",
    "        return down_conv\n",
    "    \n",
    "    def _upblock(self, n_in, n_out, stride, ks):\n",
    "        up_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_in, n_out, stride=stride, kernel_size=ks, padding=ks//2),\n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_out, n_out, stride=stride, kernel_size=ks, padding=ks//2), \n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_out, n_out, stride=2, kernel_size=ks, padding=ks//2, output_padding=ks//2),\n",
    "        )\n",
    "        return up_conv\n",
    "    \n",
    "    def forward(self, x):\n",
    "        down_conv1 = self.down_conv1(x)\n",
    "        down_conv2 = self.down_conv2(down_conv1)\n",
    "        down_conv3 = self.down_conv3(down_conv2)\n",
    "        \n",
    "        up_conv3 = self.up_conv3(down_conv3)\n",
    "        \n",
    "        up_conv2 = self.up_conv2(torch.cat([up_conv3, down_conv2], 1))\n",
    "        up_conv1 = self.up_conv1(torch.cat([up_conv2, down_conv1], 1))\n",
    "        \n",
    "        return nn.Sigmoid()(up_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timm Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reference for the Timm model setup can be found [here](https://github.com/fastai/timmdocs)\n",
    "\n",
    "A tutorial by Zach Mueller on how to integrate Timm models can be found [here](https://walkwithfastai.com/vision.external.timm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avail_pretrained_models = timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avail_pretrained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = timm.create_model(\"vit_base_resnet50_224_in21k\", pretrained=True, num_classes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite creating a model that outputs 1 class, we need to chop off the head of this model and place out own on it. Zach's blog outlines how to do this well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_body(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_metric(predb, yb):\n",
    "    return (torch.round(predb) == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all file names in folders of interest\n",
    "# we know thaqt file names are the same regardless of wther the file is in masks or images directory\n",
    "\n",
    "img_dirs = [folder.name for folder in data_path.ls() if \"images\" in folder.name]\n",
    "\n",
    "img_names = []\n",
    "for folder in img_dirs:\n",
    "    for im in (data_path/folder).ls():\n",
    "        img_names.append((im.parent.name, im.name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label all masks with glom\n",
    "\n",
    "# holder = []\n",
    "# for folder in master_bar(img_dirs):\n",
    "#     for im in progress_bar((data_path/folder).ls()):\n",
    "#         mask = tio.LabelMap(data_path/(im.parent.name).replace(\"images\", \"masks\")/im.name)\n",
    "#         holder.append((im.parent.name, im.name, 1 in mask.data))\n",
    "        \n",
    "# has_glom = pd.DataFrame(holder, columns=[\"dir\", \"name\", \"has_glom\"])\n",
    "\n",
    "has_glom = pd.read_csv(\"./has_glom.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_glom[\"has_glom\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_glom[has_glom[\"name\"] == '0486052bb_0080-256.png'][\"has_glom\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_folder = random.choice(img_dirs)\n",
    "random_image = random.choice((data_path/random_folder).ls()).name\n",
    "rand_im = tio.ScalarImage(data_path/random_folder/random_image)\n",
    "rand_mask = tio.LabelMap(data_path/(random_folder).replace(\"images\", \"masks\")/random_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_im.data.permute(3,1,2,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_creator(affine = torch.tensor([[-1.,  0.,  0.,  0.], [ 0., -1.,  0.,  0.], [ 0.,  0.,  1.,  0.], [ 0.,  0.,  0.,  1.]])):\n",
    "    subjects_list = []\n",
    "    for img in img_names:\n",
    "        pic_name = img[1]\n",
    "        im = tio.ScalarImage(data_path/img[0]/img[1])\n",
    "        mask = tio.LabelMap(data_path/img[0].replace(\"images\", \"masks\")/img[1], affine=affine)\n",
    "\n",
    "        if has_glom[has_glom[\"name\"] == pic_name][\"has_glom\"].values[0] == False:\n",
    "            if random.choices([True, False], weights=(2,5))[0]:\n",
    "                subjects_list.append(tio.Subject(\n",
    "                    img = im,\n",
    "                    mask = mask,\n",
    "                    img_id = pic_name,\n",
    "                ))\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            subjects_list.append(tio.Subject(\n",
    "                    img = im,\n",
    "                    mask = mask,\n",
    "                    img_id = pic_name,\n",
    "                ))\n",
    "    return subjects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def subject_creator(df, affine = torch.tensor([[-1.,  0.,  0.,  0.], [ 0., -1.,  0.,  0.], [ 0.,  0.,  1.,  0.], [ 0.,  0.,  0.,  1.]])):\n",
    "#     subjects_list = []\n",
    "#     for idx,_ in tqdm(enumerate(df.iterrows()), total=len(df)):\n",
    "        \n",
    "#         img_id = get_id_by_index(idx, df=df)\n",
    "        \n",
    "#         pic_list = [item for item in (path/\"smaller/imgs\").rglob(\"*\") if not item.is_dir() and img_id in item.name]\n",
    "        \n",
    "#         for pic in pic_list:\n",
    "#             pic_name = pic.name.split(\".\")[0]\n",
    "#             im = tio.ScalarImage(path/\"smaller/imgs\"/(pic_name+\".tiff\"))\n",
    "#             mask = tio.LabelMap(path/\"smaller/masks\"/(pic_name+\"_mask.tiff\"), affine=affine)\n",
    "            \n",
    "#             ########CRITICAL TO NOTE###########\n",
    "#             # below is a check to see if any positives actually exist within the image quarters\n",
    "#             # if we do not run this step, we get an error when we train only on patches with positive values\n",
    "            \n",
    "#             if 1 in mask.data:\n",
    "#                 subjects_list.append(tio.Subject(\n",
    "#                     img = im,\n",
    "#                     mask = mask,\n",
    "#                     img_id = pic_name\n",
    "#                 ))\n",
    "#             else:\n",
    "#                 continue\n",
    "#     return subjects_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_items = subject_creator(new_masks)\n",
    "    transforms = tio.Compose([custom_reshape, custom_normalization])\n",
    "    test_dataset = tio.SubjectsDataset(test_items, transform=transforms)\n",
    "    \n",
    "    test_img = test_dataset[0]\n",
    "    \n",
    "    downsized_img = tio.Resample((4,4,1))(test_img[\"img\"][tio.DATA])\n",
    "    \n",
    "    downsized_img.shape\n",
    "    \n",
    "    plt.imshow(downsized_img.squeeze().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_single_img_and_mask(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_normalization = tio.Lambda(lambda x: (x/255).float(), types_to_apply=[tio.INTENSITY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshuffle(x):\n",
    "    return x.permute(3,1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_reshape = tio.Lambda(lambda x: x[:3,...], types_to_apply=[tio.INTENSITY])\n",
    "custom_reshape = tio.Lambda(lambda x: x.squeeze().unsqueeze(0), types_to_apply=[tio.INTENSITY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_to3d = tio.Lambda(lambda x: to_3chan(x, 0), types_to_apply=[tio.LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_squeeze = tio.Lambda(lambda x: x.squeeze().unsqueeze(0), types_to_apply=[tio.LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnecessary as I should find out why there are different shapes but I want to get to model building\n",
    "def shuffle_axes(img_tensor):\n",
    "    return img_tensor.permute(0,2,1,3)\n",
    "reshuffle = tio.Lambda(shuffle_axes, types_to_apply=[tio.LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_shrink = tio.Lambda(lambda x: torch.tensor(resizer(x, 15)))\n",
    "\n",
    "resample_2x = tio.Lambda(lambda x: tio.Resample((2,2,1))(x), types_to_apply=[tio.INTENSITY])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_size = (512, 512, 1)\n",
    "# patch_size = (224, 224, 1)\n",
    "patch_size = (256, 256, 1)\n",
    "sample_ratio = {0: 1, 1: 10, 2: 1}\n",
    "\n",
    "subjects_list = subject_creator()\n",
    "subjects_list_copy = subjects_list[:]     # needed because shuffle does in place\n",
    "\n",
    "random.seed(22222)\n",
    "#random.seed(57)\n",
    "random.shuffle(subjects_list_copy)\n",
    "\n",
    "train_subjects = subjects_list_copy[:round(len(subjects_list_copy)*0.8)]\n",
    "valid_subjects = subjects_list_copy[round(len(subjects_list_copy)*0.8):]\n",
    "#train_subjects = subjects_list_copy[:1]\n",
    "#valid_subjects = subjects_list_copy[1:2]\n",
    "\n",
    "#train_transforms = tio.Compose([tio.Resample((20,20,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "#valid_transforms = tio.Compose([tio.Resample((20,20,1)), custom_reshape, custom_normalization,])\n",
    "train_transforms = tio.Compose([\n",
    "#     custom_reshape, \n",
    "#     custom_to3d, \n",
    "#     tio.RescaleIntensity(percentiles=(0.5, 99.5)), \n",
    "    tio.RandomFlip(), \n",
    "    tio.RandomAffine(),\n",
    "#     tio.RandomNoise(),\n",
    "#     tio.RandomBlur(),\n",
    "    custom_normalization, \n",
    "])\n",
    "valid_transforms = tio.Compose([\n",
    "#     tio.RescaleIntensity(percentiles=(0.5, 99.5)),\n",
    "#     custom_reshape, \n",
    "#     custom_to3d, \n",
    "    custom_normalization\n",
    "])\n",
    "\n",
    "train_dataset = tio.SubjectsDataset(train_subjects, transform=train_transforms)\n",
    "valid_dataset = tio.SubjectsDataset(valid_subjects, transform=valid_transforms)\n",
    "\n",
    "queue_length = 40\n",
    "samples_per_volume = 4\n",
    "\n",
    "sampler = tio.data.LabelSampler(patch_size, label_probabilities=sample_ratio)\n",
    "\n",
    "train_queue = tio.Queue(\n",
    "    train_dataset,\n",
    "    queue_length,\n",
    "    samples_per_volume,\n",
    "    sampler,\n",
    "    num_workers=0,\n",
    "    shuffle_subjects=True,\n",
    "    shuffle_patches=True,\n",
    ")\n",
    "\n",
    "valid_queue = tio.Queue(\n",
    "    valid_dataset,\n",
    "    queue_length,\n",
    "    samples_per_volume,\n",
    "    sampler,\n",
    "    num_workers=0,\n",
    "    shuffle_subjects=False,\n",
    "    shuffle_patches=False,\n",
    ")\n",
    "\n",
    "# train_loader = DataLoader(train_queue, batch_size=16)\n",
    "# valid_loader = DataLoader(valid_queue, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_img_ids = [i[\"img_id\"] for i in train_subjects]\n",
    "valid_img_ids = [i[\"img_id\"] for i in valid_subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_dataset[0][\"mask\"][tio.DATA].squeeze().unsqueeze(2), aspect='auto')\n",
    "# test_mask = tifffile.imread(path/\"smaller/masks/cb2d976f4_2_mask.tiff\")\n",
    "# plt.imshow(test_mask, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 in valid_dataset[-1][\"mask\"][tio.DATA].squeeze().unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_creator(subjects_list):\n",
    "    \"\"\"\n",
    "    Takes a list of objects and returns a tuple of same length\n",
    "    First value in tuple is a list of the x-values, second is a list of y-values\n",
    "    \"\"\"\n",
    "    x = torch.stack([img[\"img\"][tio.DATA].permute(3,0,1,2) for img in subjects_list], 0).squeeze()\n",
    "    y = torch.stack([mask[\"mask\"][tio.DATA].permute(3,0,1,2) for mask in subjects_list], 0).squeeze().unsqueeze(1)\n",
    "    return (x, y)\n",
    "\n",
    "dls = DataLoaders(\n",
    "    TfmdDL(\n",
    "        train_queue, \n",
    "        batch_size=8, \n",
    "        num_workers=12,\n",
    "        #chunkify=lambda x: print(str(x)),\n",
    "        # returns generator of indices (provided by sample attribute), length is provided by queue sample length\n",
    "        #create_batches=lambda x: print(x),\n",
    "        # passed a list of length batchsize and collates into a batch\n",
    "        #create_batch=lambda x: print(x[1][\"img\"][tio.DATA].shape),\n",
    "        create_batch=batch_creator,\n",
    "        after_batch=[Normalize.from_stats(*imagenet_stats)],\n",
    "    ),\n",
    "    TfmdDL(\n",
    "        valid_queue, \n",
    "        batch_size=8, \n",
    "        num_workers=12, \n",
    "        create_batch=batch_creator,\n",
    "        after_batch=[Normalize.from_stats(*imagenet_stats)],\n",
    "    ),\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(to_3d(dls.train_ds[0][\"img\"][tio.DATA]))\n",
    "dls.train_ds[50000][\"img\"][tio.DATA].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.one_batch()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prebatched=False\n",
    "\n",
    "#def create_batch(b): return (fa_collate,fa_convert)[prebatched](b)\n",
    "#create_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai callbacks\n",
    "\n",
    "class PrinterCallback(Callback):\n",
    "    \"\"\"\n",
    "    Snaps image of x, y, and preds every specified number of batches\n",
    "    Saves images to path specified\n",
    "    \"\"\"\n",
    "    def __init__(self, path, img_freq=105):\n",
    "        self.img_freq = img_freq\n",
    "        self.path = path\n",
    "    def after_batch(self):\n",
    "        if self.iter % self.img_freq == 0:\n",
    "            img_list = []\n",
    "            with torch.no_grad():\n",
    "                for i in range(self.pred.shape[0]):\n",
    "                    x = self.x[i,...]\n",
    "                    y = to_3chan(self.y[i,...], 0)\n",
    "                    pred = to_3chan(self.pred[i,...], 0)\n",
    "                    img_list.append(x)\n",
    "                    img_list.append(y)\n",
    "                    img_list.append(pred)\n",
    "\n",
    "                grid = torchvision.utils.make_grid(\n",
    "                    img_list,\n",
    "                    nrow=3,\n",
    "                )\n",
    "                self._save(self.path, grid, self.epoch, self.iter, round(self.loss.item(), 3))\n",
    "                \n",
    "        #print(f\"The learning rate is {self.opt.hypers[0]['lr']}\")\n",
    "        #print({self.dls.valid.subjects_dataset._transform})\n",
    "        \n",
    "    @staticmethod\n",
    "    def _save(img_path, img, epoch, batch, loss):\n",
    "        npimg = img.cpu().detach().float().numpy()\n",
    "        plt.imsave(img_path/f\"epoch{epoch}batch{batch}__{loss}.png\", np.transpose(npimg, (1,2,0)))\n",
    "        \n",
    "class ConvertY(Callback):\n",
    "    \"\"\"\n",
    "    Since we used TorchIO to sample the data, we first need to convert the y back to its normal values\n",
    "    \"\"\"\n",
    "    def before_batch(self):\n",
    "        \"\"\"\n",
    "        NOTE: as per the docs, you can only assign to `yb`, not `y`\n",
    "        `yb` is a tuple (which is immutable) therefore you must override the `self.learn.yb` - note we are assigning to to `learn.yb`\n",
    "        \"\"\"\n",
    "        #self.yb = tuple([torch.where(self.y != torch.tensor(1).cuda(), torch.tensor(0).cuda(), torch.tensor(1).cuda())])\n",
    "        self.learn.yb = tuple([torch.where(self.y != torch.tensor(1).cuda(), torch.tensor(0).cuda(), torch.tensor(1).cuda())])\n",
    "        \n",
    "        #print(self.yb[0].shape)\n",
    "        #print(len(self.yb))\n",
    "        \n",
    "    #def after_pred(self):\n",
    "        # To check to see that the overwritten values of y did change\n",
    "        #print(self.y)\n",
    "        #print(dir(self))\n",
    "        \n",
    "\n",
    "        \n",
    "## NOTE: this may not be needed anymore now that our loss function cobines this step        \n",
    "class AddSigmoidActivation(Callback):\n",
    "    \"\"\"\n",
    "    Change the output to add a Sigmoid function \n",
    "    Needed since:\n",
    "        a) Using a pretrained Resnet model that doesn't support adding a final activation layer\n",
    "        b) unlike `cnn_learner`, a `unet_learner` doesn't have the `custom_head` parameter (which the forums suggest is an option to effectively add a layer to a pretarined model)\n",
    "    Note: need to check if `learner.model[-1].add_module` would work if you subclassed `nn.Module` and created a `forward()` method that added this activation?\n",
    "    \"\"\"\n",
    "    def after_pred(self):\n",
    "        \"\"\"\n",
    "        As per the documentation, this callback hook is specifically designed for modifying the outputs BEFORE theyre sent to the loss function\n",
    "        Thus it is a perfect place to add our sigmoid function to the outputs\n",
    "        \"\"\"\n",
    "        self.learn.pred = nn.Sigmoid()(self.pred)\n",
    "        \n",
    "class ProgressiveTransformsUpdateCallback(Callback):\n",
    "    def before_epoch(self):\n",
    "        if self.epoch < 4:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((4,4,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((4,4,1)), custom_reshape, custom_normalization,])\n",
    "            )\n",
    "            #for h in self.opt.hypers:\n",
    "            #    h[\"lr\"] = 0.00001\n",
    "        if 3 < self.epoch < 8:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((2,2,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((2,2,1)), custom_reshape, custom_normalization,])\n",
    "            )\n",
    "            for h in self.opt.hypers:\n",
    "                h[\"lr\"] = 0.00001\n",
    "        if self.epoch > 7:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([custom_reshape, custom_normalization,])\n",
    "            )\n",
    "        #print(self.data.dataset.subjects_dataset.dry_iter())\n",
    "        \n",
    "class ModifyTransformsCallback(Callback):\n",
    "    \"\"\"\n",
    "    Train and Valid transforms must each be a list of transforms wrapped in tio.Compose\n",
    "    \"\"\"\n",
    "    def __init__(self, train_callbacks, valid_callbacks):\n",
    "        self.train_callbacks = train_callbacks\n",
    "        self.valid_callbacks = valid_callbacks\n",
    "        \n",
    "    def before_epoch(self):\n",
    "        self.dls.train.subjects_dataset.set_transform(self.train_callbacks)\n",
    "        self.dls.valid.subjects_dataset.set_transform(self.valid_callbacks)\n",
    "        \n",
    "class UpsamplePredCallback(Callback):\n",
    "    \"\"\"\n",
    "    If we downsampled the x-values, the preds will be the same resolution. Therefore we need to upsample to the size the y-value (masks) expect\n",
    "    \"\"\"\n",
    "    def __init__(self, upscale):\n",
    "        self.upscale = upscale\n",
    "        \n",
    "    def after_pred(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class SwitchLossCallback(Callback):\n",
    "    def after_pred(self):\n",
    "        if self.iter > round(self.n_iter / 2):\n",
    "            self.learn.loss_func = hh_dtloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner = unet_learner(dls, resnet34, n_out=1, loss_func=dice_loss, lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that none of our callbacks appear here as we have not set them at the `learner` level but rather at the `fit` level. This is because we (for the most part) want callbacks for training only. However it may be necessary to add callbacks here later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.show_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.dls.dataset.subjects_dataset.set_transform\n",
    "#new_train_transforms = tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "#new_valid_transforms = tio.Compose([custom_reshape, custom_normalization,])\n",
    "#learner.dls.train.subjects_dataset.set_transform(new_train_transforms)\n",
    "#learner.dls.valid.subjects_dataset.set_transform(new_valid_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    \"resnet\",\n",
    "    \"transforms\",\n",
    "    \"dice_loss\",\n",
    "    \"reduceLR\",\n",
    "    \"150_samples\"\n",
    "]\n",
    "\n",
    "group = \"resnet\"\n",
    "\n",
    "notes = \"Same thing as baseline but with larger model and better loss\"\n",
    "\n",
    "name = \"resnet34_lr1e-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 8,\n",
    "    \"transforms\": \"basic\",\n",
    "    \"loss\": \"dice_loss\",\n",
    "    \"lr\": 1e-5,\n",
    "    \"model\": \"resnet34\",\n",
    "    \"train_type\": \"fit\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project='HuBMAP_model_experiments', entity='stantonius', name=name, tags=tags, group=group, notes=notes, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks \n",
    "\n",
    "#os.environ['WANDB_MODE'] = 'dryrun'\n",
    "# wandb_callback = WandbCallback(log='all')\n",
    "path = Path()\n",
    "\n",
    "model_name = arrow.utcnow().format(\"DDMMMYY\") + \"_\" + name\n",
    "save_model_callback = SaveModelCallback(fname=model_name, every_epoch=True)\n",
    "save_image_path = path/\"training_image_logs\"\n",
    "\n",
    "\n",
    "\n",
    "updated_train_transforms = tio.Compose([tio.Resample((4,4,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "updated_valid_transforms = tio.Compose([tio.Resample((4,4,1)), custom_reshape, custom_normalization,])\n",
    "\n",
    "updated_transforms = ModifyTransformsCallback(updated_train_transforms, updated_valid_transforms)\n",
    "\n",
    "\n",
    "cbs=[PrinterCallback(save_image_path), ConvertY(), AddSigmoidActivation(), save_model_callback]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs) \n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        \n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "    \n",
    "dice_loss = DiceLoss()    \n",
    "    \n",
    "class DiceBCELoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs)  \n",
    "\n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        \n",
    "        # Note changed before from binary_cross_entropy to binary_cross_entropy_with_logits\n",
    "        # got an error\n",
    "        # However this step requires us to combine our sigmoid layer\n",
    "        BCE = F.binary_cross_entropy(inputs.float(), targets.float(), reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE\n",
    "    \n",
    "bce_dice_loss = DiceBCELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        loss = torch.zeros(inputs.shape[0])\n",
    "        for i, targ in enumerate(targets):\n",
    "            if 1 in targ:\n",
    "                targ = targ.view(-1)\n",
    "                inp = inputs[i].view(-1)                    \n",
    "                loss[i] = dice_loss(targ, inp)\n",
    "            else:\n",
    "                loss[i] = F.binary_cross_entropy(inputs[i].float(), targ.float(), reduction='mean')\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "adaptive_loss = AdaptiveLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.ndimage.morphology import distance_transform_edt as edt\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "\"\"\"\n",
    "Hausdorff loss implementation based on paper:\n",
    "https://arxiv.org/pdf/1904.10030.pdf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class HausdorffDTLoss(nn.Module):\n",
    "    \"\"\"Binary Hausdorff loss based on distance transform\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=2.0, **kwargs):\n",
    "        super(HausdorffDTLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def distance_field(self, img: np.ndarray) -> np.ndarray:\n",
    "        field = np.zeros_like(img)\n",
    "\n",
    "        for batch in range(len(img)):\n",
    "            fg_mask = img[batch] > 0.5\n",
    "\n",
    "            if fg_mask.any():\n",
    "                bg_mask = ~fg_mask\n",
    "\n",
    "                fg_dist = edt(fg_mask)\n",
    "                bg_dist = edt(bg_mask)\n",
    "\n",
    "                field[batch] = fg_dist + bg_dist\n",
    "\n",
    "        return field\n",
    "\n",
    "    def forward(\n",
    "        self, pred: torch.Tensor, target: torch.Tensor, debug=False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Uses one binary channel: 1 - fg, 0 - bg\n",
    "        pred: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        target: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        \"\"\"\n",
    "        assert pred.dim() == 4 or pred.dim() == 5, \"Only 2D and 3D supported\"\n",
    "        assert (\n",
    "            pred.dim() == target.dim()\n",
    "        ), \"Prediction and target need to be of same dimension\"\n",
    "\n",
    "        # pred = torch.sigmoid(pred)\n",
    "\n",
    "        pred_dt = torch.from_numpy(self.distance_field(pred.detach().cpu().numpy())).float()\n",
    "        target_dt = torch.from_numpy(self.distance_field(target.detach().cpu().numpy())).float()\n",
    "\n",
    "        pred_error = (pred - target) ** 2\n",
    "        distance = pred_dt ** self.alpha + target_dt ** self.alpha\n",
    "\n",
    "        dt_field = pred_error.cpu() * distance\n",
    "        loss = dt_field.mean()\n",
    "\n",
    "        if debug:\n",
    "            return (\n",
    "                loss.cpu().numpy(),\n",
    "                (\n",
    "                    dt_field.cpu().numpy()[0, 0],\n",
    "                    pred_error.cpu().numpy()[0, 0],\n",
    "                    distance.cpu().numpy()[0, 0],\n",
    "                    pred_dt.cpu().numpy()[0, 0],\n",
    "                    target_dt.cpu().numpy()[0, 0],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "class HausdorffERLoss(nn.Module):\n",
    "    \"\"\"Binary Hausdorff loss based on morphological erosion\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=2.0, erosions=10, **kwargs):\n",
    "        super(HausdorffERLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.erosions = erosions\n",
    "        self.prepare_kernels()\n",
    "\n",
    "    def prepare_kernels(self):\n",
    "        cross = np.array([cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))])\n",
    "        bound = np.array([[[0, 0, 0], [0, 1, 0], [0, 0, 0]]])\n",
    "\n",
    "        self.kernel2D = cross * 0.2\n",
    "        self.kernel3D = np.array([bound, cross, bound]) * (1 / 7)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perform_erosion(\n",
    "        self, pred: np.ndarray, target: np.ndarray, debug\n",
    "    ) -> np.ndarray:\n",
    "        bound = (pred - target) ** 2\n",
    "\n",
    "        if bound.ndim == 5:\n",
    "            kernel = self.kernel3D\n",
    "        elif bound.ndim == 4:\n",
    "            kernel = self.kernel2D\n",
    "        else:\n",
    "            raise ValueError(f\"Dimension {bound.ndim} is nor supported.\")\n",
    "\n",
    "        eroted = np.zeros_like(bound)\n",
    "        erosions = []\n",
    "\n",
    "        for batch in range(len(bound)):\n",
    "\n",
    "            # debug\n",
    "            erosions.append(np.copy(bound[batch][0]))\n",
    "\n",
    "            for k in range(self.erosions):\n",
    "\n",
    "                # compute convolution with kernel\n",
    "                dilation = convolve(bound[batch], kernel, mode=\"constant\", cval=0.0)\n",
    "\n",
    "                # apply soft thresholding at 0.5 and normalize\n",
    "                erosion = dilation - 0.5\n",
    "                erosion[erosion < 0] = 0\n",
    "\n",
    "                if erosion.ptp() != 0:\n",
    "                    erosion = (erosion - erosion.min()) / erosion.ptp()\n",
    "\n",
    "                # save erosion and add to loss\n",
    "                bound[batch] = erosion\n",
    "                eroted[batch] += erosion * (k + 1) ** self.alpha\n",
    "\n",
    "                if debug:\n",
    "                    erosions.append(np.copy(erosion[0]))\n",
    "\n",
    "        # image visualization in debug mode\n",
    "        if debug:\n",
    "            return eroted, erosions\n",
    "        else:\n",
    "            return eroted\n",
    "\n",
    "    def forward(\n",
    "        self, pred: torch.Tensor, target: torch.Tensor, debug=False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Uses one binary channel: 1 - fg, 0 - bg\n",
    "        pred: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        target: (b, 1, x, y, z) or (b, 1, x, y)\n",
    "        \"\"\"\n",
    "        assert pred.dim() == 4 or pred.dim() == 5, \"Only 2D and 3D supported\"\n",
    "        assert (\n",
    "            pred.dim() == target.dim()\n",
    "        ), \"Prediction and target need to be of same dimension\"\n",
    "\n",
    "        # pred = torch.sigmoid(pred)\n",
    "\n",
    "        if debug:\n",
    "            eroted, erosions = self.perform_erosion(\n",
    "                pred.cpu().numpy(), target.cpu().numpy(), debug\n",
    "            )\n",
    "            return eroted.mean(), erosions\n",
    "\n",
    "        else:\n",
    "            eroted = torch.from_numpy(\n",
    "                self.perform_erosion(pred, target, debug)\n",
    "            ).float()\n",
    "\n",
    "            loss = eroted.mean()\n",
    "\n",
    "            return loss\n",
    "        \n",
    "hh_erloss = HausdorffERLoss()\n",
    "hh_dtloss = HausdorffDTLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.autograd.function import Function\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def odd_flip(H):\n",
    "    '''\n",
    "    generate frequency map\n",
    "    when height or width of image is odd number,\n",
    "    creat a array concol [0,1,...,int(H/2)+1,int(H/2),...,0]\n",
    "    len(concol) = H\n",
    "    '''\n",
    "    m = int(H/2)\n",
    "    col = np.arange(0,m+1)\n",
    "    flipcol = col[m-1::-1]\n",
    "    concol = np.concatenate((col,flipcol),0)\n",
    "    return concol\n",
    "\n",
    "def even_flip(H):\n",
    "    '''\n",
    "    generate frequency map\n",
    "    when height or width of image is even number,\n",
    "    creat a array concol [0,1,...,int(H/2),int(H/2),...,0]\n",
    "    len(concol) = H\n",
    "    '''\n",
    "    m = int(H/2)\n",
    "    col = np.arange(0,m)\n",
    "    flipcol = col[m::-1]\n",
    "    concol = np.concatenate((col,flipcol),0)\n",
    "    return concol\n",
    "\n",
    "def dist(target):\n",
    "    '''\n",
    "    sqrt(m^2 + n^2) in eq(8)\n",
    "    '''\n",
    "\n",
    "    _,_,H,W = target.shape\n",
    "\n",
    "    if H%2 ==1:\n",
    "        concol = odd_flip(H)\n",
    "    else:\n",
    "        concol = even_flip(H)\n",
    "        \n",
    "    if W%2 == 1:\n",
    "        conrow = odd_flip(W)\n",
    "    else:\n",
    "        conrow = even_flip(W)\n",
    "        \n",
    "    m_col = concol[:,np.newaxis] \n",
    "    m_row = conrow[np.newaxis,:]\n",
    "    dist = np.sqrt(m_col*m_col + m_row*m_row) # sqrt(m^2+n^2)\n",
    "  \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        dist_ = torch.from_numpy(dist).float().cuda()\n",
    "    else:\n",
    "        dist_ = torch.from_numpy(dist).float()\n",
    "    return dist_\n",
    "\n",
    "class EnergyLoss(nn.Module):\n",
    "    def __init__(self,cuda,alpha,sigma):\n",
    "        super(EnergyLoss, self).__init__()\n",
    "        self.energylossfunc = EnergylossFunc.apply\n",
    "        self.alpha = alpha\n",
    "        self.cuda = cuda\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self,feat,label):\n",
    "        return self.energylossfunc(self.cuda,feat, label,self.alpha,self.sigma)\n",
    "    \n",
    "class EnergylossFunc(Function):\n",
    "    '''\n",
    "    target: ground truth \n",
    "    feat: Z -0.5. Zprob of your target class(here is vessel) with shape[B,H,W]. \n",
    "    Z from softmax output of unet with shape [B,C,H,W]. C: number of classes\n",
    "    alpha: default 0.35\n",
    "    sigma: default 0.25\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def forward(ctx,cuda,feat_levelset,target,alpha,sigma,Gaussian = False):\n",
    "        hardtanh = nn.Hardtanh(min_val=0, max_val=1, inplace=False)\n",
    "        target = target.float()\n",
    "        index_ = dist(target)\n",
    "        dim_ = target.shape[1]\n",
    "        target = torch.squeeze(target,1)\n",
    "        I1 = target + alpha*hardtanh(feat_levelset/sigma) # G_t + alpha*H(phi) in eq(5)\n",
    "        dmn = torch.rfft(I1,2,normalized = True, onesided = False)\n",
    "        dmn_r = dmn[:,:,:,0] # dmn's real part\n",
    "        dmn_i = dmn[:,:,:,1] # dmm's imagine part\n",
    "        dmn2 = dmn_r * dmn_r + dmn_i * dmn_i # dmn^2\n",
    "\n",
    "        ctx.save_for_backward(feat_levelset,target,dmn,index_)\n",
    "            \n",
    "        F_energy = torch.sum(index_*dmn2)/feat_levelset.shape[0]/feat_levelset.shape[1]/feat_levelset.shape[2] # eq(8)\n",
    "        \n",
    "        return F_energy\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output):\n",
    "        feature,label,dmn,index_ = ctx.saved_tensors\n",
    "        index_ = torch.unsqueeze(index_,0)\n",
    "        index_ = torch.unsqueeze(index_,3)\n",
    "        F_diff = -0.5*index_*dmn # eq(9) \n",
    "        diff = torch.irfft(F_diff,2,normalized = True, onesided = False)/feature.shape[0] # eq\n",
    "        return None,Variable(-grad_output*diff),None,None,None\n",
    "    \n",
    "    \n",
    "score1 = y_out[:,0,:,:] # prob for class target\n",
    "score2 = (score1-0.5) # for energyloss\n",
    "\n",
    "training_loss = self.loss(score2, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    def forward(self, pred, targ):\n",
    "        loss1 = hh_dtloss(pred, targ)\n",
    "        loss2 = adaptive_loss(pred, targ)\n",
    "        return loss1*0.0001 + loss2\n",
    "    \n",
    "combined_loss = CombinedLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annoyingly we cannot use mix precision fp16 with BCE loss. Otherwise we would slap .to_fp16() on the end of the learner\n",
    "\n",
    "learner = unet_learner(dls, resnet34, n_out=1, loss_func=combined_loss)\n",
    "# learner = Learner(dls, test_model, loss_func=dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(config[\"epochs\"], lr=config[\"lr\"], cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learner.model, \"./models/test_export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = unet_learner(dls, resnet34, n_out=1, loss_func=combined_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load(\"./BASELINE_0.06-0.128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learner.model, \"./to_upload/baseline_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sett ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??torch.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lookahead_qhadam(p, lr, mom, eps):\n",
    "    return Lookahead(QHAdam(p, lr, mom, eps))\n",
    "\n",
    "#optim = partial(lookahead_qhadam, lr=1e-7, mom=0.5, eps=1e-5)\n",
    "optim = partial(QHAdam, lr=2e-5, mom=0.5, eps=1e-5, sqr_mom=0.98, wd=0.0075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "\n",
    "updated_transforms = ModifyTransformsCallback(updated_train_transforms, updated_valid_transforms)\n",
    "\n",
    "updated_train_transforms = tio.Compose([tio.Resample((4,4,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "updated_valid_transforms = tio.Compose([tio.Resample((4,4,1)), custom_reshape, custom_normalization,])\n",
    "#updated_train_transforms = tio.Compose([custom_reshape, custom_normalization])\n",
    "#updated_valid_transforms = tio.Compose([custom_reshape, custom_normalization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaccardLoss(nn.Module):\n",
    "    def __init__(self, axis=1):\n",
    "        super(JaccardLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        return JaccardCoeff()(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this training, adjusting the learning rate on its own isn't seeming to work. So we need to try updating the params using a sweep for a single epoch and evaluate the results.\n",
    "\n",
    "Params to update:\n",
    "`eps`\n",
    "`wd`\n",
    "`lr`\n",
    "`mom`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WandB tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use WandB to log model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the fastai docs, models are saved in `learner.path/learner.model_dir/name.pth` so if this isn't set we need to pick a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can save only when a certain improvement happens but will do this later\n",
    "\n",
    "save_model_callback = SaveModelCallback(every_epoch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each Model Run, we will initialise `wandb` so that we can specify the name of the run (instead of having the generic autoassigned names). The following setup will be used for each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(entity=\"stantonius\", project=\"Notebooks-kidney_glomeruli\", name=\"INSERT MEANINGFUL RUN NAME HERE\")\n",
    "\n",
    "# Add the following to the list of callbacks\n",
    "#wandb_callback = WandbCallback(log='all', log_preds=True, log_model=True, log_dataset=False, dataset_name=None, valid_dl=None, n_preds=36, seed=12345, reorder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WandB Sweep Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep config for the WandB workflow\n",
    "\n",
    "# NOTE: there is a specific format these parameters must be provided (ie. you cannot pass functions here)\n",
    "# you can only provide dict and lists, which contain strings or numbers (its or floats)\n",
    "\n",
    "\"\"\"\n",
    "if you need to pass functions, you can ....\n",
    "\"\"\"\n",
    "\n",
    "################\n",
    "# TODO - include other transforms, hyperparameters (dropout, nadam), different loss, different architecture\n",
    "################\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": \"hyperparams\",\n",
    "    \"method\": \"random\",   # or \"grid\" ?\n",
    "    \"metric\": {\n",
    "        \"name\": \"valid_loss\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\n",
    "            \"distribution\": \"uniform\",    # e notation are floats\n",
    "            \"min\": 1e-9,\n",
    "            \"max\": 1e-4\n",
    "        },\n",
    "        \"mom\": {\n",
    "            \"values\": [0.99, 0.9, 0.85, 0.7, 0.5, 0.15]    # may need to change to distribution if error\n",
    "        },\n",
    "        \"sqr_mom\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.8,\n",
    "            \"max\": 0.99\n",
    "        },\n",
    "        \"eps\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-9,\n",
    "            \"max\": 1e-1\n",
    "        },\n",
    "        \"wd\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.0,\n",
    "            \"max\": 0.4\n",
    "        },\n",
    "        \"grad_accum\": {\n",
    "            \"values\": [\"grad_accum\", \"no_grad_accum\"]\n",
    "        },\n",
    "        \"optims\": {\n",
    "            \"values\": [\"adam\", \"radam\", \"qhadam\", \"larc\", \"lamb\", \"lookahead\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "#sweep_id = wandb.sweep(sweep_config, entity=\"stantonius\", project=\"Notebooks-kidney_glomeruli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.gather_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # below would be helpful if we could choose the name based on params but I dont think this will work\n",
    "    # run = wandb.init(entity=\"stantonius\", project=\"Notebooks-kidney_glomeruli\", name=\"INSERT MEANINGFUL RUN NAME HERE\")\n",
    "    # self.run = wandb.init(entity=\"stantonius\", project=\"kidneys-cv\", config=self.sweep_defaults)\n",
    "    run = wandb.init()\n",
    "    \n",
    "    # set transforms\n",
    "    updated_train_transforms = tio.Compose([tio.Resample((4,4,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization])\n",
    "    updated_valid_transforms = tio.Compose([tio.Resample((4,4,1)), custom_reshape, custom_normalization])\n",
    "    \n",
    "    # callback setup\n",
    "    updated_transforms = ModifyTransformsCallback(updated_train_transforms, updated_valid_transforms)\n",
    "    save_model_callback = SaveModelCallback(every_epoch=True)\n",
    "    save_image_path = path/\"training_image_logs\"\n",
    "    # needs to be called after wandb.init\n",
    "    wandb_callback = WandbCallback(log='all', log_preds=True, log_model=True, log_dataset=False, dataset_name=None, valid_dl=None, n_preds=36, seed=12345, reorder=True)\n",
    "    \n",
    "    if run.config.grad_accum == \"grad_accum\":\n",
    "        cbs=[PrinterCallback(save_image_path), ConvertY(), AddSigmoidActivation(), save_model_callback, wandb_callback, updated_transforms, GradientClip(), GradientAccumulation()]\n",
    "    else:\n",
    "        cbs=[PrinterCallback(save_image_path), ConvertY(), AddSigmoidActivation(), save_model_callback, wandb_callback, updated_transforms, GradientClip()]\n",
    "\n",
    "    optims = {\n",
    "        \"adam\": partial(Adam, lr=run.config.lr, mom=run.config.mom, sqr_mom=run.config.sqr_mom, eps=run.config.eps, wd=run.config.wd),\n",
    "        \"radam\": partial(RAdam, lr=run.config.lr, mom=run.config.mom, sqr_mom=run.config.sqr_mom, eps=run.config.eps, wd=run.config.wd),\n",
    "        \"qhadam\": partial(QHAdam, lr=run.config.lr, mom=run.config.mom, sqr_mom=run.config.sqr_mom, eps=run.config.eps, wd=run.config.wd),\n",
    "        \"larc\": partial(Larc, lr=run.config.lr, mom=run.config.mom, eps=run.config.eps, wd=run.config.wd),  # only one that doesnt take sqr_mom\n",
    "        \"lamb\": partial(Lamb, lr=run.config.lr, mom=run.config.mom, sqr_mom=run.config.sqr_mom, eps=run.config.eps, wd=run.config.wd),\n",
    "        \"lookahead\": Lookahead(partial(Adam, lr=run.config.lr, mom=run.config.mom, sqr_mom=run.config.sqr_mom, eps=run.config.eps, wd=run.config.wd))\n",
    "    }\n",
    "    \n",
    "    learner = unet_learner(dls, resnet18, n_out=1, loss_func=dice_loss, opt_func=optims.get(run.config.optims)).to_fp16()\n",
    "    \n",
    "    learner.fine_tune(1, cbs=cbs)\n",
    "    \n",
    "    \n",
    "#wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "Try different:\n",
    "* resnet architecture\n",
    "* other archs specific for cell identification\n",
    "* slice LR\n",
    "* not fine tune but others\n",
    "* fix lookahead\n",
    "* early stopping when loss is very low (below 0.1)\n",
    "* lrfind as callback between epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReduceLR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(entity=\"stantonius\", project=\"Notebooks-kidney_glomeruli\", name=\"FIT_xresenet_deeper_noBatchNorm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = LitUNET.load_from_checkpoint(path/\"models\"/\"21_Feb_20_8_15-epoch=4-step=8124.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_model_details = torch.load(path/\"models\"/\"21_Feb_19_16_56-epoch=8-step=14624.ckpt\")\n",
    "#print(checkpoint_model_details['hyper_parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[cut_image(item, path/\"test\", path/\"test/smaller\") for item in [img.name.split(\".\")[0] for img in (path/\"test\").glob(\"*.tiff\")]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An aside on inference/test transforms**: for some reason I don't see the ability to perform transforms on inference/test data in the TorchIO library. Therefore I am starting to question whether you are supposed to do this? Anyway, the evaluation doesn't work if you do not do this.\n",
    "\n",
    "I looked into how to create a *batch* transform but from what I read, I can iterate over each of the items in the batch very quickly because a) the patches are small, b) the transforms occur in C (therefore are already optimised) and c) GPU memory is limited - transfering data to the GPU to perform transforms that are already optimised for the CPU doesn't make much sense, and the transfer itself takes time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_images(imgs: Union[Path, list], transforms):\n",
    "    \"\"\"\n",
    "    Takes only image FILE NAMES (in Path of list form) and converts to ScalarImage\n",
    "    Then applies any transforms provided in tio.COMPOSE object\n",
    "    Returns a LIST of image tio.SUBJECTSDATASET\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, Path):\n",
    "        image_paths = [img.name for img in (imgs).rglob(\"*\") if not item.is_dir()]\n",
    "    if isinstance(imgs, list):\n",
    "        image_paths = imgs\n",
    "    \n",
    "    images =[]\n",
    "    for image_path in image_paths:\n",
    "        images.append(\n",
    "            tio.Subject(\n",
    "                img = tio.ScalarImage(image_path),\n",
    "                img_id = image_path.name.split(\".\")[0]\n",
    "            )\n",
    "        )\n",
    "    images_dataset = tio.SubjectsDataset(images, transforms)\n",
    "    return images_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_pred(subject, model, patch_size=(256,256,1)):\n",
    "    \"\"\"\n",
    "    Take a tio.SUBJECT and return a TUPLE of the image ID and its predicted output tensor\n",
    "    \"\"\"\n",
    "    grid_sampler = tio.inference.GridSampler(subject, patch_size)\n",
    "    patch_loader = torch.utils.data.DataLoader(grid_sampler, batch_size=4)\n",
    "    aggregator = tio.inference.GridAggregator(grid_sampler)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for patches_batch in patch_loader:\n",
    "            img_id = patches_batch['img_id']\n",
    "            input_tensor = patches_batch['img'][tio.DATA]\n",
    "            # Need to run the data through the non-random transforms that were applied to the training and validation data\n",
    "            #input_tensor = torch.stack([test_transforms(item) for item in input_tensor]).squeeze()\n",
    "            input_tensor.squeeze_()\n",
    "            if len(input_tensor.shape) == 3:\n",
    "                input_tensor.unsqueeze_(0)\n",
    "            locations = patches_batch[tio.LOCATION]\n",
    "            logits = model(input_tensor)\n",
    "            labels = logits\n",
    "            outputs = labels.unsqueeze(4)\n",
    "            #print(outputs.shape)\n",
    "            aggregator.add_batch(outputs, locations)\n",
    "        output_tensor = aggregator.get_output_tensor()\n",
    "        return (img_id, output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = [path/\"test/smaller\"/\"26dc41664_1.tiff\"]\n",
    "test_transforms = tio.Compose([tio.Resample((2,2,1)), custom_reshape, custom_normalization,])\n",
    "model = load_model.eval()\n",
    "\n",
    "test_preds = [subject_pred(item, model) for item in get_test_images(test_img_path, test_transforms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(test_preds[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(to_3chan(test_preds[0][1],0).squeeze().permute(1,2,0), cmap=\"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rle_encoding(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup & Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[img.unlink() for img in (path/\"training_image_logs\").glob(\"*\") if img.name != \".ipynb_checkpoints\"]\n",
    "\n",
    "############################################################ \n",
    "# BE CAREFUL WITH BELOW - MAKE SURE THE DIRECTORY IS CORRECT\n",
    "############################################################\n",
    "\n",
    "# For lightning logs\n",
    "#[shutil.rmtree(folder) for folder in (path/\"lightning_logs\").glob(\"*\")]\n",
    "\n",
    "# For lightning logs\n",
    "#[model.unlink() for model in (path/\"models\").glob(\"*\")]\n",
    "\n",
    "# For wandb\n",
    "\"\"\"\n",
    "for folder in (path/\"wandb\").glob(\"*\"):\n",
    "    try: \n",
    "        shutil.rmtree(folder)\n",
    "    except: \n",
    "        continue\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run test set through above code and include in the model\n",
    "    * Utilise the inference from TorchIO\n",
    "* Add the stitch function to take inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(affine_options[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graveyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_loader = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitUNETDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, train_loader, valid_loader):\n",
    "        super().__init__()\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.valid_loader\n",
    "    \n",
    "    def transfer_batch_to_device(self, batch, device):\n",
    "        # squeeze the depth dimension as our model doesn't account for this but TorchIO utilises this\n",
    "        x = batch['img'][tio.DATA].squeeze()\n",
    "        y = batch['mask'][tio.DATA].squeeze()\n",
    "        # need to override the labels to ensure we are only guessing presence of glomeruli\n",
    "        # remember the additional category was introduced just to improve sampling selection area\n",
    "        y = torch.where(y != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "        return x.to(device), y.to(device)\n",
    "\n",
    "class LitUNET(pl.LightningModule):\n",
    "    def __init__(self, model, loss_fxn, bs=32, learning_rate=3e-3, num_workers=0, manual_optimization=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.loss_fxn = loss_fxn\n",
    "        self.lr = learning_rate\n",
    "        self._manual_optimization = manual_optimization\n",
    "        if self._manual_optimization:\n",
    "            self.training_step = self.training_step_manual\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('valid_loss', loss, logger=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # self.hparams available because we called self.save_hyperparameters()\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossSpikeAnalyzer(Callback):\n",
    "    def __init__(self):\n",
    "        self.train_batch_losses = []\n",
    "        self.valid_batch_losses =[]\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        callback_stats = trainer.callback_metrics\n",
    "        if \"train_loss_step\" in callback_stats.keys():\n",
    "            self.train_batch_losses.append(callback_stats[\"train_loss_step\"].item())\n",
    "            #print(f\"The shape of the batch is {batch['img'][tio.DATA].shape} - the batch actually returns {batch.keys()} - and {outputs}, and the loss is {np.asarray(self.batch_loss).mean()}\")\n",
    "            #if callback_stats[\"train_loss_step\"].item() > 0.9 or batch_idx % 60 == 0:\n",
    "            if batch_idx % 600 == 0:\n",
    "                img_list = []\n",
    "                with torch.no_grad():\n",
    "                    pl_module.eval()\n",
    "                    preds = pl_module(batch['img'][tio.DATA].squeeze().cuda())\n",
    "                    pl_module.train()\n",
    "                for i, img in enumerate(batch['img'][tio.DATA].squeeze()):\n",
    "                    mask = torch.where(batch['mask'][tio.DATA].squeeze()[i,...].unsqueeze(0) != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "                    mask = to_3chan(mask, 0)\n",
    "                    pred = to_3chan(preds[i,...],0)\n",
    "                    img_list.append(img)\n",
    "                    img_list.append(mask)\n",
    "                    img_list.append(pred.cpu())\n",
    "                grid = torchvision.utils.make_grid(\n",
    "                    img_list,\n",
    "                    nrow=3,\n",
    "                )\n",
    "                self._save(grid, batch_idx, callback_stats[\"train_loss_step\"].item())\n",
    "                \n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        self.valid_batch_losses.append(outputs.cpu())\n",
    "       \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if len(self.train_batch_losses) > 10:\n",
    "            train_loss_total = np.asarray(self.train_batch_losses).mean()\n",
    "            train_loss_last50 = np.asarray(self.train_batch_losses[round(len(self.train_batch_losses)*(1-0.5)):]).mean()\n",
    "            train_loss_last15 = np.asarray(self.train_batch_losses[round(len(self.train_batch_losses)*(1-0.15)):]).mean()\n",
    "            print(f\"The validation loss at VALIDATION END is {trainer.callback_metrics['valid_loss_epoch'].item()}\")\n",
    "            wandb.log({\n",
    "                \"valid_loss_epoch\": trainer.callback_metrics[\"valid_loss_epoch\"].item(),\n",
    "                \"valid_loss\": trainer.callback_metrics[\"valid_loss\"].item(),\n",
    "                \"train_loss_total_epoch\": train_loss_total,\n",
    "                \"train_loss_last50_epoch\": train_loss_last50,\n",
    "                \"train_loss_last15_epoch\": train_loss_last15,\n",
    "            })\n",
    "\n",
    "            self.train_batch_losses = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _show(img):\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _save(img, batch_idx, loss):\n",
    "        npimg = img.numpy()\n",
    "        loss = str(round(loss,2)).replace(\".\", \"_\")\n",
    "        plt.imsave(path/\"training_image_logs\"/f\"{batch_idx}__{loss}.png\", np.transpose(npimg, (1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterTrainer:\n",
    "    def __init__(self):\n",
    "        self.sweep_defaults = {\n",
    "            \"sample_ratio\": 1,\n",
    "            \"patch_size\": \"256\",\n",
    "            \"lr\": 3e-3,\n",
    "            \"gradient_clipping\": 0.5,\n",
    "            \"epochs\": 10,\n",
    "        }\n",
    "        self.sample_ratio_options = {\n",
    "            1: {0: 8, 1: 3, 2: 2},\n",
    "            2: {0: 1, 1: 1, 2: 1},\n",
    "            3: {0: 3, 1: 3, 2: 1}\n",
    "        }\n",
    "        self.patch_size_options = {\n",
    "            \"256\": (256, 256, 1),\n",
    "            \"512\": (512, 512, 1)\n",
    "        }\n",
    "        self.run = wandb.init(entity=\"stantonius\", project=\"kidneys-cv\", config=self.sweep_defaults)\n",
    "        self.config = wandb.config\n",
    "        \n",
    "        self.sample_ratio = self.sample_ratio_options.get(self.config.sample_ratio)\n",
    "        self.patch_size = self.patch_size_options.get(self.config.patch_size)\n",
    "    \n",
    "        self.subjects_list = subject_creator(new_masks)\n",
    "        self.subjects_list_copy = self.subjects_list[:]     # needed because shuffle does in place\n",
    "        \n",
    "        random.seed(57)\n",
    "        random.shuffle(self.subjects_list_copy)\n",
    "        \n",
    "        self.train_subjects = self.subjects_list_copy[:round(len(self.subjects_list_copy)*0.8)]\n",
    "        self.valid_subjects = self.subjects_list_copy[round(len(self.subjects_list_copy)*0.8):]\n",
    "        #train_subjects = subjects_list_copy[:1]\n",
    "        #valid_subjects = subjects_list_copy[1:2]\n",
    "    \n",
    "        self.train_transforms = tio.Compose([tio.Resample((20,20,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "        self.valid_transforms = tio.Compose([tio.Resample((20,20,1)), custom_reshape, custom_normalization,])\n",
    "        #self.train_transforms = tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "        #self.valid_transforms = tio.Compose([custom_reshape, custom_normalization,])\n",
    "    \n",
    "        self.train_dataset = tio.SubjectsDataset(self.train_subjects, transform=self.train_transforms)\n",
    "        self.valid_dataset = tio.SubjectsDataset(self.valid_subjects, transform=self.valid_transforms)\n",
    "    \n",
    "        self.queue_length = 10\n",
    "        self.samples_per_volume = 10\n",
    "    \n",
    "        self.sampler = tio.data.LabelSampler(self.patch_size, label_probabilities=self.sample_ratio)\n",
    "    \n",
    "        self.train_queue = tio.Queue(\n",
    "            self.train_dataset,\n",
    "            self.queue_length,\n",
    "            self.samples_per_volume,\n",
    "            self.sampler,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        self.valid_queue = tio.Queue(\n",
    "            self.valid_dataset,\n",
    "            self.queue_length,\n",
    "            self.samples_per_volume,\n",
    "            self.sampler,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_queue, batch_size=16, pin_memory=True)\n",
    "        self.valid_loader = DataLoader(self.valid_queue, batch_size=16, pin_memory=True)\n",
    "        \n",
    "    def set_transforms(self, train_transforms, valid_transforms):\n",
    "        \"\"\"\n",
    "        Clearly this should be written so that code isnt duplicated\n",
    "        Couldnt decide if I should have a setter for every parameter\n",
    "        \"\"\"\n",
    "        self.train_transforms = train_transforms\n",
    "        self.valid_transforms = valid_transforms\n",
    "        self.train_dataset = tio.SubjectsDataset(self.train_subjects, transform=self.train_transforms)\n",
    "        self.valid_dataset = tio.SubjectsDataset(self.valid_subjects, transform=self.valid_transforms)\n",
    "    \n",
    "        self.queue_length = 10\n",
    "        self.samples_per_volume = 10\n",
    "    \n",
    "        self.sampler = tio.data.LabelSampler(self.patch_size, label_probabilities=self.sample_ratio)\n",
    "    \n",
    "        self.train_queue = tio.Queue(\n",
    "            self.train_dataset,\n",
    "            self.queue_length,\n",
    "            self.samples_per_volume,\n",
    "            self.sampler,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        self.valid_queue = tio.Queue(\n",
    "            self.valid_dataset,\n",
    "            self.queue_length,\n",
    "            self.samples_per_volume,\n",
    "            self.sampler,\n",
    "            num_workers=0,\n",
    "        )\n",
    "        \n",
    "        self.train_loader = torch.utils.data.DataLoader(self.train_queue, batch_size=16, pin_memory=True)\n",
    "        self.valid_loader = torch.utils.data.DataLoader(self.valid_queue, batch_size=16, pin_memory=True)\n",
    "        \n",
    "    def train(self, model, loss, epochs, dryrun = True):\n",
    "        data = LitUNETDataLoader(self.train_loader, self.valid_loader)\n",
    "        model = LitUNET(custom_unet, dice_loss, num_workers=16, learning_rate=self.config.lr)\n",
    "        checkpoint_callback = ModelCheckpoint(monitor='valid_loss_epoch', dirpath=path/\"models\", save_top_k=3, mode='min', save_weights_only=True, prefix=arrow.now().format(\"MMM_DD_YY_H_mm\"))\n",
    "        trainer = pl.Trainer(gpus=1, callbacks=[LossSpikeAnalyzer(), checkpoint_callback], gradient_clip_val=self.config.gradient_clipping, max_epochs=epochs)\n",
    "        trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old WandB sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually we will want to try the sweep if we can get the training fast enough to make this worthwhile. Currently the training takes too long and therefore a sweep would take days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep config for the WandB workflow\n",
    "\n",
    "################\n",
    "# TODO - include other transforms, hyperparameters (dropout, nadam), different loss, different architecture\n",
    "################\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": \"custom_unet\",\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"valid_loss_epoch\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"sample_ratio\": {\n",
    "            \"values\": [1, 2, 3]\n",
    "        },\n",
    "\n",
    "        \"patch_size\": {\n",
    "            \"values\": [\"256\", \"512\"]\n",
    "        },\n",
    "        \"lr\": {\n",
    "            \"values\": [3e-3, 3e-2, 3e-4]\n",
    "        },\n",
    "        \"gradient_clipping\": {\n",
    "            \"values\": [0.5, 1]\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "#sweep_id = wandb.sweep(sweep_config, entity=\"stantonius\", project=\"kidneys-cv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
