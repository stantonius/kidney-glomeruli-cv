{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuBMAP - Hacking the Kidney - Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Kaggle competition page](https://www.kaggle.com/c/hubmap-kidney-segmentation)\n",
    "\n",
    "Helpful Notebooks:\n",
    "* [https://www.kaggle.com/markalavin/hubmap-tile-images-w-overlap-and-build-tfrecords](https://www.kaggle.com/markalavin/hubmap-tile-images-w-overlap-and-build-tfrecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDO\n",
    "* Look at impact of different affine matrices\n",
    "* Look at impact of removing alpha channel on model size and performance\n",
    "* Add Deepmind's architecture optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Downloads for Offline use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update -n base conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! conda config --set always_yes True\n",
    "#! conda install -c fastai -c pytorch fastai\n",
    "#! conda install pytorch torchvision torchaudio fastai -c pytorch\n",
    "#! conda update pytorch torchvision torchaudio cudatoolkit -c pytorch\n",
    "#! conda install pandas\n",
    "#! conda install -c conda-forge kaggle\n",
    "#! conda install -c conda-forge tifffile\n",
    "#! conda install -c conda-forge tqdm\n",
    "# !conda install -c conda-forge matplotlib\n",
    "#! conda install -c conda-forge pytorch-lightning\n",
    "#! conda install -c conda-forge wandb\n",
    "#! conda install -c conda-forge arrow\n",
    "#!conda install -c conda-forge pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install arrow pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed if running in wsl2\n",
    "#! pip install pytorch-lightning wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have no idea why the conda-forge version doesn't work\n",
    "\n",
    "#!python -m pip install opencv-python\n",
    "\n",
    "# If you are running this notebook on a server (like Linux on WSL2) you need the headless version of opencv\n",
    "# The regular opencv requires GUI packages that serves dont have, and will raise an error\n",
    "#!python -m pip install opencv-python-headless\n",
    "\n",
    "# temporary solution to use tab complete - something wrong with jupyter jedi - need to downgrade\n",
    "#!pip install jedi==0.17.2\n",
    "\n",
    "#!pip install torchio --upgrade\n",
    "\n",
    "#!pip install pytorch-lightning-bolts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the finicky local CUDA is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, import PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Craig\\miniconda3\\envs\\kidneys-cv\\lib\\site-packages\\pl_bolts\\utils\\warnings.py:32: UserWarning: You want to use `gym` which is not installed yet, install it with `pip install gym`.\n",
      "  f' install it with `pip install {pypi_name}`.' + extra_text\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# prebuilt models\n",
    "from pl_bolts.models import UNet\n",
    "\n",
    "import tensorboard as tb\n",
    "\n",
    "# Need to put kaggle.json in /%USERS%/.kaggle folder (C:/Users/Craig/.kaggle)\n",
    "#import kaggle\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from typing import Union\n",
    "\n",
    "# Read tiff images\n",
    "#import tifffile\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torchio as tio\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import time\n",
    "import wandb\n",
    "import arrow\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Memory management tools\n",
    "import gc\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.imports import *\n",
    "from fastai.callback.wandb import *\n",
    "\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()\n",
    "#kaggle.api.competition_download_files(\"hubmap-kidney-segmentation\", path=paLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you are about to download the data in the cvorrect directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the data in the correct folder - commented out so as to not repeat the unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "#with zipfile.ZipFile(path/\"hubmap-kidney-segmentation.zip\", 'r') as zipref:\n",
    "#    zipref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path/\"train.csv\").rename(columns={\"id\": \"img_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path()\n",
    "#path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Main Functions\n",
    "################\n",
    "\n",
    "\n",
    "def rle2mask(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: encoding string value from csv\n",
    "    shape: (width,height) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    # return a list of starting pixels and a list of lengths\n",
    "    starts, lengths = [\n",
    "        np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])\n",
    "    ]\n",
    "    # subtract 1 from every starting pixel\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    # calculate a background of 0 (empty) with size defined by image\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    # replace every 0 within each range with 1\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = 1\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def mask2rle(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def get_id_by_index(index, df=train_df):\n",
    "    return df.iloc[index]['img_id']\n",
    "\n",
    "def get_single_img(id, folder=\"train\"):\n",
    "    img = tifffile.imread(path/folder/(id+\".tiff\"))\n",
    "    if len(img.shape) == 5:\n",
    "        img = img.squeeze().transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "def show_single_img(id, **kwargs):\n",
    "    return plt.imshow(get_single_img(id), **kwargs)\n",
    "\n",
    "def show_img_by_index(index, df=train_df):\n",
    "    return plt.imshow(tifffile.imread(path/\"train\"/(train_df.iloc[TEST_IMAGE_INDEX]['id']+\".tiff\")))\n",
    "\n",
    "def get_single_encs(id, df=train_df):\n",
    "    return df[df['img_id'] == id]['encoding'].array[0]\n",
    "\n",
    "def get_mask(id, df=train_df, folder=\"train\"):\n",
    "    return rle2mask(\n",
    "        get_single_encs(id, df=df),\n",
    "        get_single_img(id, folder=folder).shape[::-1][1:]\n",
    "    )\n",
    "\n",
    "def show_single_img_and_mask_by_id(id):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    mask = get_mask(id)\n",
    "    img = get_single_img(id)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image\", fontsize=18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, cmap=\"hot\", alpha=0.5)\n",
    "    plt.title(f\"Image + mask\", fontsize=18)    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, cmap=\"hot\")\n",
    "    plt.title(f\"Mask\", fontsize=18)    \n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def show_single_img_and_mask(subject: tio.data.subject.Subject, resize_to=50):\n",
    "    plt.figure(figsize=(120, 100))\n",
    "    \n",
    "    if not isinstance(subject, tio.data.subject.Subject):\n",
    "        raise TypeError(f\"The subject is required to be of type torchio.data.subject.Subject but you provided {type(subject)}\")\n",
    "    \n",
    "    img = subject[\"img\"][tio.DATA].squeeze().permute(1,2,0)\n",
    "    mask = subject[\"mask\"][tio.DATA].squeeze().unsqueeze(2)\n",
    "    \n",
    "    if resize_to:\n",
    "        img = resizer(img, scale=resize_to)\n",
    "        mask = resizer(mask, scale=resize_to)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image\", fontsize=18)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, cmap=\"hot\", alpha=0.5)\n",
    "    plt.title(f\"Image + mask\", fontsize=18)    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask, cmap=\"hot\")\n",
    "    plt.title(f\"Mask\", fontsize=18)    \n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "def to_4d(img, input_chan_first=False, output_chan_first=True):\n",
    "    if not len(img.shape)==3:\n",
    "        raise ValueError(\"Function only converts 3D arrayto 4D array\")\n",
    "    return np.expand_dims(np.transpose(img, \n",
    "                   (0,1,2) if input_chan_first else (2,0,1)), \n",
    "                   3 if output_chan_first else 0)\n",
    "\n",
    "def to_3d(img, input_chan_first=True, output_chan_first=False):\n",
    "    if not len(img.shape)==4:\n",
    "        raise ValueError(\"Function only converts 4D arrayto 3D array\")\n",
    "    return np.transpose(img.squeeze(), (0,1,2) if output_chan_first else (1,2,0))\n",
    "\n",
    "def to_3chan(x, dim=1):\n",
    "    return torch.cat((x,x,x), dim=dim)\n",
    "\n",
    "def resizer(img, scale=5, show=False):\n",
    "    \"\"\"\n",
    "    Returns an smaller array of the same dimensions, but converts to 3D to allow for resizing\n",
    "    \"\"\"\n",
    "    scale_percent = scale # percent of original size\n",
    "    im_dims = (len(img.shape) == 4)\n",
    "    if im_dims:\n",
    "        img = to_3d(img)\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    img_reshaped = cv2.resize(img.numpy(), dim)\n",
    "    if show:\n",
    "        return plt.imshow(img_reshaped)\n",
    "    if im_dims:\n",
    "        return to_4d(img_reshaped)\n",
    "    return img_reshaped\n",
    "\n",
    "def squeeze_and_reshape(img_tensor, remove_alpha=False):\n",
    "    if not isinstance(img_tensor, torch.Tensor):\n",
    "        raise TypeError(\"Image needs to be a tensor\")\n",
    "    if len(img_tensor.shape) == 5:\n",
    "        img_tensor = img_tensor.squeeze().permute(2, 1, 0)\n",
    "    img_tensor = img_tensor.unsqueeze(2).permute(3,1,0,2)\n",
    "    return img_tensor\n",
    "\n",
    "def to_pil(image):\n",
    "    # for \n",
    "    data = image.numpy().squeeze().T\n",
    "    data = data.astype(np.uint8)\n",
    "    image = Image.fromarray(data)\n",
    "    w, h = image.size\n",
    "    display(image)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remask(img, mask, tile, threshold=8, show=False):\n",
    "    \n",
    "    img_height = img.shape[1]\n",
    "    img_width = img.shape[0]\n",
    "    \n",
    "    number_of_vertical_tiles = (img_height // tile)+1\n",
    "    number_of_horizontal_tiles = (img_width // tile)+1\n",
    "    \n",
    "    #background = np.zeros((tile*number_of_horizontal_tiles, tile*number_of_vertical_tiles,3))[:img.shape[0],:img.shape[1],:img.shape[2]]\n",
    "    \n",
    "    tile_coords = []\n",
    "    for h_idx in range(number_of_horizontal_tiles):\n",
    "        for v_idx in range(number_of_vertical_tiles):\n",
    "            tile_coords.append((h_idx+1, v_idx+1)) # +1 to remove 0 indexing\n",
    "\n",
    "    cropped_images = []\n",
    "    for h,v in tile_coords:\n",
    "        cropped_images.append((h, v, img[tile*(h-1):tile*h, tile*(v-1):tile*v, :]))\n",
    "        \n",
    "    for horiz,vert,im in cropped_images:\n",
    "        if not 0 in im.shape:      #required in case tile is \n",
    "            hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
    "            h, s, v = cv2.split(hsv)\n",
    "            if s.mean() < threshold:\n",
    "                all_black = np.full((im.shape[0], im.shape[1]),2)\n",
    "                mask[tile*(horiz-1):tile*horiz,tile*(vert-1):tile*vert] = all_black\n",
    "                #im = im*0.\n",
    "            #background[tile*(horiz-1):tile*horiz,tile*(vert-1):tile*vert,:] = im\n",
    "    \n",
    "    if show:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img.astype('uint8'))\n",
    "        plt.title(f\"Image\", fontsize=18)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(img.astype('uint8'))\n",
    "        plt.imshow(mask.astype('uint8'), cmap=\"hot\", alpha=0.5)\n",
    "        plt.title(f\"Image + mask\", fontsize=18)    \n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(mask.astype('uint8'), cmap=\"hot\")\n",
    "        plt.title(f\"Mask\", fontsize=18)    \n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "#img_id = get_id_by_index(7)\n",
    "#img_id = '095bf7a1f'\n",
    "#with tifffile.TiffFile(path/\"train\"/(img_id+\".tiff\")) as tif:\n",
    "#    imgg = tif.asarray()\n",
    "#print(imgg.shape)\n",
    "#mask = get_mask(img_id)\n",
    "#new_mask = remask(to_3d(squeeze_and_reshape(imgg)), mask, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_df(df, directory):\n",
    "    mask_list = []\n",
    "    for idx,_ in tqdm(enumerate(df.iterrows()), total=len(df)):\n",
    "        img_id = get_id_by_index(idx, df=df)\n",
    "        with tifffile.TiffFile(path/directory/(img_id+\".tiff\")) as tif:\n",
    "            base_im = tif.asarray()\n",
    "            im_tensor = squeeze_and_reshape(torch.from_numpy(base_im)).numpy()\n",
    "            mask = remask(to_3d(im_tensor), get_mask(img_id), 1000)\n",
    "            mask_list.append((img_id, mask))\n",
    "    return pd.DataFrame(mask_list, columns=[\"img_id\", \"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't recreate the dataset everytime - pull from local directory if available as pickle file\n",
    "# I understand pickles aren't safe - so only do this in your local env and never open an unfamiliar pickle file\n",
    "if not (path/\"new_masks.pkl\").exists():\n",
    "    new_masks = create_mask_df(train_df, \"train\")\n",
    "    new_masks.to_pickle(path/\"new_masks.pkl\")\n",
    "if (path/\"new_masks.pkl\").exists():\n",
    "    with open(path/\"new_masks.pkl\", \"rb\") as fh:\n",
    "        new_masks = pickle.load(fh)\n",
    "    #new_masks = pd.read_pickle(\"new_masks.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_image(img_id, source_path:Path, destination_path: Path, mask_df=None):\n",
    "    \"\"\"\n",
    "    Cut image (and corresponding mask - in Dataframe - if supplied) into QUARTERS and save them to a directory called smaller\n",
    "    \"\"\"\n",
    "    \n",
    "    img = tio.Image(source_path/f\"{img_id}.tiff\").data\n",
    "    if len(img.shape) != 4:\n",
    "        raise ValueError(\"Tensor shape needs to have 4 dimensions\")\n",
    "    if img.shape[0] != 4:\n",
    "        raise ValueError(\"First dimension must have 4 channels\")\n",
    "    vertical_tiles = img.shape[2] // 2\n",
    "    horizontal_tiles = img.shape[1] // 2\n",
    "    \n",
    "    if mask_df:\n",
    "        mask = torch.from_numpy(mask_df[mask_df[\"img_id\"]==img_id][\"mask\"].values[0]).unsqueeze(0).unsqueeze(3)\n",
    "        # I have managed to flip the axes somewhere and am too lazy or stubborn to fix the root issue. So need to permute axes\n",
    "        mask = mask.permute(0,2,1,3)\n",
    "    \n",
    "    img1 = img[:,:horizontal_tiles,:vertical_tiles,:]\n",
    "    img2 = img[:,horizontal_tiles:,:vertical_tiles,:]\n",
    "    img3 = img[:,:horizontal_tiles,vertical_tiles:,:]\n",
    "    img4 = img[:,horizontal_tiles:,vertical_tiles:,:]\n",
    "    \n",
    "    if mask_df:\n",
    "        mask1 = mask[:,:horizontal_tiles,:vertical_tiles,:]\n",
    "        mask2 = mask[:,horizontal_tiles:,:vertical_tiles,:]\n",
    "        mask3 = mask[:,:horizontal_tiles,vertical_tiles:,:]\n",
    "        mask4 = mask[:,horizontal_tiles:,vertical_tiles:,:]\n",
    "    \n",
    "    tio.Image(tensor=img1).save(destination_path/f\"{img_id}_1.tiff\")\n",
    "    tio.Image(tensor=img2).save(destination_path/f\"{img_id}_2.tiff\")\n",
    "    tio.Image(tensor=img3).save(destination_path/f\"{img_id}_3.tiff\")\n",
    "    tio.Image(tensor=img4).save(destination_path/f\"{img_id}_4.tiff\")\n",
    "    \n",
    "    if mask_df:\n",
    "        tio.Image(tensor=mask1).save(destination_path/f\"{img_id}_1_mask.tiff\")\n",
    "        tio.Image(tensor=mask2).save(destination_path/f\"{img_id}_2_mask.tiff\")\n",
    "        tio.Image(tensor=mask3).save(destination_path/f\"{img_id}_3_mask.tiff\")\n",
    "        tio.Image(tensor=mask4).save(destination_path/f\"{img_id}_4_mask.tiff\")\n",
    "\n",
    "\n",
    "#[cut_image(item, new_masks) for item in new_masks.img_id.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restitch_image(img_id, pred_mask=None):\n",
    "    for name in (path/\"smaller/imgs\").glob(f\"{img_id}_?.tiff\"):\n",
    "        img_quarter = name.name.split(\"_\")[1].split(\".\")[0]\n",
    "        if img_quarter == \"1\":\n",
    "            img1 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "        if img_quarter == \"2\":\n",
    "            img2 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "        if img_quarter == \"3\":\n",
    "            img3 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "        if img_quarter == \"4\":\n",
    "            img4 = tio.Image(path/\"smaller/imgs\"/f\"{name.name}\").data\n",
    "    \n",
    "    # make a 4D tensor with 4 channels and 1 depth channel\n",
    "    whole_image = torch.zeros(\n",
    "        img1.shape[0],\n",
    "        img1.shape[1] + img3.shape[1],\n",
    "        img1.shape[2] + img2.shape[2],\n",
    "    ).unsqueeze(3)\n",
    "    \n",
    "    whole_image[:,:whole_image.shape[1]//2, :whole_image.shape[2]//2, :] = img1\n",
    "    whole_image[:,whole_image.shape[1]//2-1:, :whole_image.shape[2]//2, :] = img2\n",
    "    whole_image[:,:whole_image.shape[1]//2,  whole_image.shape[2]//2:, :] = img3\n",
    "    whole_image[:,whole_image.shape[1]//2-1:,  whole_image.shape[2]//2:, :] = img4\n",
    "    \n",
    "    to_pil(whole_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restitch_image(get_id_by_index(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    The user specifies what the first input channels size will be and the ultimate output size will be\n",
    "    The downblock and upblock functions also take input and out values - but these are PER CONVOLUTION\n",
    "    They do not necessarily inherit the values specified by the user\n",
    "    \n",
    "    The architecture is 3 down blocks, followed by 3 up blocks\n",
    "    Output is squeezed if the channel_out is 1 - masks are single channels so this matches the dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channel_in, channel_out, stride=1, ks=3):\n",
    "        super(CustomUnet, self).__init__()\n",
    "        self.down_conv1 = self._downblock(channel_in, 16, stride=stride, ks=ks)\n",
    "        self.down_conv2 = self._downblock(16, 32, stride=stride, ks=ks)\n",
    "        self.down_conv3 = self._downblock(32, 64, stride=stride, ks=ks)\n",
    "        self.up_conv3 = self._upblock(64, 32, stride=stride, ks=ks)\n",
    "        self.up_conv2 = self._upblock(32*2, 16, stride=stride, ks=ks) # key to notice the doubling of input size\n",
    "        self.up_conv1 = self._upblock(16*2, channel_out, stride=stride, ks=ks)\n",
    "    \n",
    "    # downward (contracting) block\n",
    "    def _downblock(self, n_in, n_out, stride, ks):\n",
    "        down_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_in, n_out, stride=stride, kernel_size=ks, padding=ks//2), \n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_out, n_out, stride=stride, kernel_size=ks, padding=ks//2), \n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=ks, stride=2, padding=ks//2) # 256/2 = 128\n",
    "        )\n",
    "        return down_conv\n",
    "    \n",
    "    def _upblock(self, n_in, n_out, stride, ks):\n",
    "        up_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_in, n_out, stride=stride, kernel_size=ks, padding=ks//2),\n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_out, n_out, stride=stride, kernel_size=ks, padding=ks//2), \n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(n_out, n_out, stride=2, kernel_size=ks, padding=ks//2, output_padding=ks//2),\n",
    "        )\n",
    "        return up_conv\n",
    "    \n",
    "    def forward(self, x):\n",
    "        down_conv1 = self.down_conv1(x)\n",
    "        down_conv2 = self.down_conv2(down_conv1)\n",
    "        down_conv3 = self.down_conv3(down_conv2)\n",
    "        \n",
    "        up_conv3 = self.up_conv3(down_conv3)\n",
    "        \n",
    "        up_conv2 = self.up_conv2(torch.cat([up_conv3, down_conv2], 1))\n",
    "        up_conv1 = self.up_conv1(torch.cat([up_conv2, down_conv1], 1))\n",
    "        \n",
    "        return nn.Sigmoid()(up_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "    \n",
    "dice_loss = DiceLoss()    \n",
    "    \n",
    "class DiceBCELoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return 1 - Dice_BCE\n",
    "    \n",
    "bce_dice_loss = DiceBCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_metric(predb, yb):\n",
    "    return (torch.round(predb) == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_loader = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitUNETDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, train_loader, valid_loader):\n",
    "        super().__init__()\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.valid_loader\n",
    "    \n",
    "    def transfer_batch_to_device(self, batch, device):\n",
    "        # squeeze the depth dimension as our model doesn't account for this but TorchIO utilises this\n",
    "        x = batch['img'][tio.DATA].squeeze()\n",
    "        y = batch['mask'][tio.DATA].squeeze()\n",
    "        # need to override the labels to ensure we are only guessing presence of glomeruli\n",
    "        # remember the additional category was introduced just to improve sampling selection area\n",
    "        y = torch.where(y != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "        return x.to(device), y.to(device)\n",
    "\n",
    "class LitUNET(pl.LightningModule):\n",
    "    def __init__(self, model, loss_fxn, bs=32, learning_rate=3e-3, num_workers=0, manual_optimization=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.loss_fxn = loss_fxn\n",
    "        self.lr = learning_rate\n",
    "        self._manual_optimization = manual_optimization\n",
    "        if self._manual_optimization:\n",
    "            self.training_step = self.training_step_manual\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('valid_loss', loss, logger=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fxn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # self.hparams available because we called self.save_hyperparameters()\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossSpikeAnalyzer(Callback):\n",
    "    def __init__(self):\n",
    "        self.train_batch_losses = []\n",
    "        self.valid_batch_losses =[]\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        callback_stats = trainer.callback_metrics\n",
    "        if \"train_loss_step\" in callback_stats.keys():\n",
    "            self.train_batch_losses.append(callback_stats[\"train_loss_step\"].item())\n",
    "            #print(f\"The shape of the batch is {batch['img'][tio.DATA].shape} - the batch actually returns {batch.keys()} - and {outputs}, and the loss is {np.asarray(self.batch_loss).mean()}\")\n",
    "            #if callback_stats[\"train_loss_step\"].item() > 0.9 or batch_idx % 60 == 0:\n",
    "            if batch_idx % 600 == 0:\n",
    "                img_list = []\n",
    "                with torch.no_grad():\n",
    "                    pl_module.eval()\n",
    "                    preds = pl_module(batch['img'][tio.DATA].squeeze().cuda())\n",
    "                    pl_module.train()\n",
    "                for i, img in enumerate(batch['img'][tio.DATA].squeeze()):\n",
    "                    mask = torch.where(batch['mask'][tio.DATA].squeeze()[i,...].unsqueeze(0) != torch.tensor(1), torch.tensor(0), torch.tensor(1))\n",
    "                    mask = to_3chan(mask, 0)\n",
    "                    pred = to_3chan(preds[i,...],0)\n",
    "                    img_list.append(img)\n",
    "                    img_list.append(mask)\n",
    "                    img_list.append(pred.cpu())\n",
    "                grid = torchvision.utils.make_grid(\n",
    "                    img_list,\n",
    "                    nrow=3,\n",
    "                )\n",
    "                self._save(grid, batch_idx, callback_stats[\"train_loss_step\"].item())\n",
    "                \n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        self.valid_batch_losses.append(outputs.cpu())\n",
    "       \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if len(self.train_batch_losses) > 10:\n",
    "            train_loss_total = np.asarray(self.train_batch_losses).mean()\n",
    "            train_loss_last50 = np.asarray(self.train_batch_losses[round(len(self.train_batch_losses)*(1-0.5)):]).mean()\n",
    "            train_loss_last15 = np.asarray(self.train_batch_losses[round(len(self.train_batch_losses)*(1-0.15)):]).mean()\n",
    "            print(f\"The validation loss at VALIDATION END is {trainer.callback_metrics['valid_loss_epoch'].item()}\")\n",
    "            wandb.log({\n",
    "                \"valid_loss_epoch\": trainer.callback_metrics[\"valid_loss_epoch\"].item(),\n",
    "                \"valid_loss\": trainer.callback_metrics[\"valid_loss\"].item(),\n",
    "                \"train_loss_total_epoch\": train_loss_total,\n",
    "                \"train_loss_last50_epoch\": train_loss_last50,\n",
    "                \"train_loss_last15_epoch\": train_loss_last15,\n",
    "            })\n",
    "\n",
    "            self.train_batch_losses = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _show(img):\n",
    "        npimg = img.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "        \n",
    "    @staticmethod\n",
    "    def _save(img, batch_idx, loss):\n",
    "        npimg = img.numpy()\n",
    "        loss = str(round(loss,2)).replace(\".\", \"_\")\n",
    "        plt.imsave(path/\"training_image_logs\"/f\"{batch_idx}__{loss}.png\", np.transpose(npimg, (1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_creator(df, affine = torch.tensor([[-1.,  0.,  0.,  0.], [ 0., -1.,  0.,  0.], [ 0.,  0.,  1.,  0.], [ 0.,  0.,  0.,  1.]])):\n",
    "    subjects_list = []\n",
    "    for idx,_ in tqdm(enumerate(df.iterrows()), total=len(df)):\n",
    "        \n",
    "        img_id = get_id_by_index(idx, df=df)\n",
    "        \n",
    "        pic_list = [item for item in (path/\"smaller/imgs\").rglob(\"*\") if not item.is_dir() and img_id in item.name]\n",
    "        \n",
    "        for pic in pic_list:\n",
    "            pic_name = pic.name.split(\".\")[0]\n",
    "            im = tio.ScalarImage(path/\"smaller/imgs\"/(pic_name+\".tiff\"))\n",
    "            mask = tio.LabelMap(path/\"smaller/masks\"/(pic_name+\"_mask.tiff\"), affine=affine)\n",
    "\n",
    "            subjects_list.append(tio.Subject(\n",
    "                img = im,\n",
    "                mask = mask,\n",
    "                img_id = pic_name\n",
    "            ))\n",
    "    return subjects_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_items = subject_creator(new_masks)\n",
    "    transforms = tio.Compose([custom_reshape, custom_normalization])\n",
    "    test_dataset = tio.SubjectsDataset(test_items, transform=transforms)\n",
    "    \n",
    "    test_img = test_dataset[0]\n",
    "    \n",
    "    downsized_img = tio.Resample((4,4,1))(test_img[\"img\"][tio.DATA])\n",
    "    \n",
    "    downsized_img.shape\n",
    "    \n",
    "    plt.imshow(downsized_img.squeeze().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_single_img_and_mask(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_normalization = tio.Lambda(lambda x: (x/255).float(), types_to_apply=[tio.INTENSITY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_reshape = tio.Lambda(lambda x: x[:3,...], types_to_apply=[tio.INTENSITY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_to3d = tio.Lambda(lambda x: to_3chan(x, 0), types_to_apply=[tio.LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnecessary as I should find out why there are different shapes but I want to get to model building\n",
    "def shuffle_axes(img_tensor):\n",
    "    return img_tensor.permute(0,2,1,3)\n",
    "reshuffle = tio.Lambda(shuffle_axes, types_to_apply=[tio.LABEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_shrink = tio.Lambda(lambda x: torch.tensor(resizer(x, 15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WandB Sweep Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "\n",
    "pl_unet = UNet(1)\n",
    "custom_unet = CustomUnet(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_unet = CustomUnet(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep config for the WandB workflow\n",
    "\n",
    "################\n",
    "# TODO - include other transforms, hyperparameters (dropout, nadam), different loss, different architecture\n",
    "################\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": \"custom_unet\",\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"valid_loss_epoch\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"sample_ratio\": {\n",
    "            \"values\": [1, 2, 3]\n",
    "        },\n",
    "\n",
    "        \"patch_size\": {\n",
    "            \"values\": [\"256\", \"512\"]\n",
    "        },\n",
    "        \"lr\": {\n",
    "            \"values\": [3e-3, 3e-2, 3e-4]\n",
    "        },\n",
    "        \"gradient_clipping\": {\n",
    "            \"values\": [0.5, 1]\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "#sweep_id = wandb.sweep(sweep_config, entity=\"stantonius\", project=\"kidneys-cv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterTrainer:\n",
    "    def __init__(self):\n",
    "        self.sweep_defaults = {\n",
    "            \"sample_ratio\": 1,\n",
    "            \"patch_size\": \"256\",\n",
    "            \"lr\": 3e-3,\n",
    "            \"gradient_clipping\": 0.5,\n",
    "            \"epochs\": 10,\n",
    "        }\n",
    "        self.sample_ratio_options = {\n",
    "            1: {0: 8, 1: 3, 2: 2},\n",
    "            2: {0: 1, 1: 1, 2: 1},\n",
    "            3: {0: 3, 1: 3, 2: 1}\n",
    "        }\n",
    "        self.patch_size_options = {\n",
    "            \"256\": (256, 256, 1),\n",
    "            \"512\": (512, 512, 1)\n",
    "        }\n",
    "        self.run = wandb.init(entity=\"stantonius\", project=\"kidneys-cv\", config=self.sweep_defaults)\n",
    "        self.config = wandb.config\n",
    "        \n",
    "        self.sample_ratio = self.sample_ratio_options.get(self.config.sample_ratio)\n",
    "        self.patch_size = self.patch_size_options.get(self.config.patch_size)\n",
    "    \n",
    "        self.subjects_list = subject_creator(new_masks)\n",
    "        self.subjects_list_copy = self.subjects_list[:]     # needed because shuffle does in place\n",
    "        \n",
    "        random.seed(57)\n",
    "        random.shuffle(self.subjects_list_copy)\n",
    "        \n",
    "        self.train_subjects = self.subjects_list_copy[:round(len(self.subjects_list_copy)*0.8)]\n",
    "        self.valid_subjects = self.subjects_list_copy[round(len(self.subjects_list_copy)*0.8):]\n",
    "        #train_subjects = subjects_list_copy[:1]\n",
    "        #valid_subjects = subjects_list_copy[1:2]\n",
    "    \n",
    "        self.train_transforms = tio.Compose([tio.Resample((20,20,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "        self.valid_transforms = tio.Compose([tio.Resample((20,20,1)), custom_reshape, custom_normalization,])\n",
    "        #self.train_transforms = tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "        #self.valid_transforms = tio.Compose([custom_reshape, custom_normalization,])\n",
    "    \n",
    "        self.train_dataset = tio.SubjectsDataset(self.train_subjects, transform=self.train_transforms)\n",
    "        self.valid_dataset = tio.SubjectsDataset(self.valid_subjects, transform=self.valid_transforms)\n",
    "    \n",
    "        self.queue_length = 10\n",
    "        self.samples_per_volume = 10\n",
    "    \n",
    "        self.sampler = tio.data.LabelSampler(self.patch_size, label_probabilities=self.sample_ratio)\n",
    "    \n",
    "        self.train_queue = tio.Queue(\n",
    "            self.train_dataset,\n",
    "            self.queue_length,\n",
    "            self.samples_per_volume,\n",
    "            self.sampler,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        self.valid_queue = tio.Queue(\n",
    "            self.valid_dataset,\n",
    "            self.queue_length,\n",
    "            self.samples_per_volume,\n",
    "            self.sampler,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_queue, batch_size=16, pin_memory=True)\n",
    "        self.valid_loader = DataLoader(self.valid_queue, batch_size=16, pin_memory=True)\n",
    "        \n",
    "    def set_transforms(self, train_transforms, valid_transforms):\n",
    "        \"\"\"\n",
    "        Clearly this should be written so that code isnt duplicated\n",
    "        Couldnt decide if I should have a setter for every parameter\n",
    "        \"\"\"\n",
    "        self.train_transforms = train_transforms\n",
    "        self.valid_transforms = valid_transforms\n",
    "        self.train_dataset = tio.SubjectsDataset(self.train_subjects, transform=self.train_transforms)\n",
    "        self.valid_dataset = tio.SubjectsDataset(self.valid_subjects, transform=self.valid_transforms)\n",
    "    \n",
    "        self.queue_length = 10\n",
    "        self.samples_per_volume = 10\n",
    "    \n",
    "        self.sampler = tio.data.LabelSampler(self.patch_size, label_probabilities=self.sample_ratio)\n",
    "    \n",
    "        self.train_queue = tio.Queue(\n",
    "            self.train_dataset,\n",
    "            self.queue_length,\n",
    "            self.samples_per_volume,\n",
    "            self.sampler,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        self.valid_queue = tio.Queue(\n",
    "            self.valid_dataset,\n",
    "            self.queue_length,\n",
    "            self.samples_per_volume,\n",
    "            self.sampler,\n",
    "            num_workers=0,\n",
    "        )\n",
    "        \n",
    "        self.train_loader = torch.utils.data.DataLoader(self.train_queue, batch_size=16, pin_memory=True)\n",
    "        self.valid_loader = torch.utils.data.DataLoader(self.valid_queue, batch_size=16, pin_memory=True)\n",
    "        \n",
    "    def train(self, model, loss, epochs, dryrun = True):\n",
    "        data = LitUNETDataLoader(self.train_loader, self.valid_loader)\n",
    "        model = LitUNET(custom_unet, dice_loss, num_workers=16, learning_rate=self.config.lr)\n",
    "        checkpoint_callback = ModelCheckpoint(monitor='valid_loss_epoch', dirpath=path/\"models\", save_top_k=3, mode='min', save_weights_only=True, prefix=arrow.now().format(\"MMM_DD_YY_H_mm\"))\n",
    "        trainer = pl.Trainer(gpus=1, callbacks=[LossSpikeAnalyzer(), checkpoint_callback], gradient_clip_val=self.config.gradient_clipping, max_epochs=epochs)\n",
    "        trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 275.85it/s]\n"
     ]
    }
   ],
   "source": [
    "patch_size = (256, 256, 1)\n",
    "sample_ratio = {0: 4, 1: 4, 2: 1}\n",
    "\n",
    "subjects_list = subject_creator(new_masks)\n",
    "subjects_list_copy = subjects_list[:]     # needed because shuffle does in place\n",
    "\n",
    "random.seed(57)\n",
    "random.shuffle(subjects_list_copy)\n",
    "\n",
    "train_subjects = subjects_list_copy[:round(len(subjects_list_copy)*0.8)]\n",
    "valid_subjects = subjects_list_copy[round(len(subjects_list_copy)*0.8):]\n",
    "#train_subjects = subjects_list_copy[:1]\n",
    "#valid_subjects = subjects_list_copy[1:2]\n",
    "\n",
    "#train_transforms = tio.Compose([tio.Resample((20,20,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "#valid_transforms = tio.Compose([tio.Resample((20,20,1)), custom_reshape, custom_normalization,])\n",
    "train_transforms = tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "valid_transforms = tio.Compose([custom_reshape, custom_normalization,])\n",
    "\n",
    "train_dataset = tio.SubjectsDataset(train_subjects, transform=train_transforms)\n",
    "valid_dataset = tio.SubjectsDataset(valid_subjects, transform=valid_transforms)\n",
    "\n",
    "queue_length = 100\n",
    "samples_per_volume = 100\n",
    "\n",
    "sampler = tio.data.LabelSampler(patch_size, label_probabilities=sample_ratio)\n",
    "\n",
    "train_queue = tio.Queue(\n",
    "    train_dataset,\n",
    "    queue_length,\n",
    "    samples_per_volume,\n",
    "    sampler,\n",
    "    num_workers=0,\n",
    "    shuffle_subjects=True,\n",
    "    shuffle_patches=True,\n",
    ")\n",
    "\n",
    "valid_queue = tio.Queue(\n",
    "    valid_dataset,\n",
    "    queue_length,\n",
    "    samples_per_volume,\n",
    "    sampler,\n",
    "    num_workers=0,\n",
    "    shuffle_subjects=False,\n",
    "    shuffle_patches=False,\n",
    ")\n",
    "\n",
    "def printer(x):\n",
    "    print(x)\n",
    "\n",
    "train_loader = DataLoader(train_queue, batch_size=16)\n",
    "valid_loader = DataLoader(valid_queue, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_creator(subjects_list):\n",
    "    \"\"\"\n",
    "    Takes a list of objects and returns a tuple of same length\n",
    "    First value in tuple is a list of the x-values, second is a list of y-values\n",
    "    \"\"\"\n",
    "    x = torch.stack([img[\"img\"][tio.DATA] for img in subjects_list], 0).squeeze()\n",
    "    y = torch.stack([mask[\"mask\"][tio.DATA] for mask in subjects_list], 0).squeeze().unsqueeze(1)\n",
    "    return (x, y)\n",
    "\n",
    "dls = DataLoaders(\n",
    "    TfmdDL(\n",
    "        train_queue, \n",
    "        batch_size=16, \n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        #chunkify=lambda x: print(str(x)),\n",
    "        # returns generator of indices (provided by sample attribute), length is provided by queue sample length\n",
    "        #create_batches=lambda x: print(x),\n",
    "        # passed a list of length batchsize and collates into a batch\n",
    "        #create_batch=lambda x: print(x[1][\"img\"][tio.DATA].shape),\n",
    "        create_batch=batch_creator,\n",
    "    ),\n",
    "    TfmdDL(\n",
    "        valid_queue, \n",
    "        batch_size=16, \n",
    "        pin_memory=True, \n",
    "        num_workers=0, \n",
    "        create_batch=batch_creator\n",
    "    ),\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prebatched=False\n",
    "\n",
    "#def create_batch(b): return (fa_collate,fa_convert)[prebatched](b)\n",
    "#create_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai callbacks\n",
    "\n",
    "class PrinterCallback(Callback):\n",
    "    \"\"\"\n",
    "    Snaps image of x, y, and preds every specified number of batches\n",
    "    Saves images to path specified\n",
    "    \"\"\"\n",
    "    def __init__(self, path, img_freq=105):\n",
    "        self.img_freq = img_freq\n",
    "        self.path = path\n",
    "    def after_batch(self):\n",
    "        if self.iter % self.img_freq == 0:\n",
    "            img_list = []\n",
    "            with torch.no_grad():\n",
    "                for i in range(self.pred.shape[0]):\n",
    "                    x = self.x[i,...]\n",
    "                    y = to_3chan(self.y[i,...], 0)\n",
    "                    pred = to_3chan(self.pred[i,...], 0)\n",
    "                    img_list.append(x)\n",
    "                    img_list.append(y)\n",
    "                    img_list.append(pred)\n",
    "                grid = torchvision.utils.make_grid(\n",
    "                    img_list,\n",
    "                    nrow=3,\n",
    "                )\n",
    "                self._save(self.path, grid, self.epoch, self.iter, round(self.loss.item(), 3))\n",
    "                \n",
    "        #print(f\"The learning rate is {self.opt.hypers[0]['lr']}\")\n",
    "        #print({self.dls.valid.subjects_dataset._transform})\n",
    "        \n",
    "    @staticmethod\n",
    "    def _save(img_path, img, epoch, batch, loss):\n",
    "        npimg = img.cpu().detach().float().numpy()\n",
    "        plt.imsave(img_path/f\"epoch{epoch}batch{batch}__{loss}.png\", np.transpose(npimg, (1,2,0)))\n",
    "        \n",
    "class ConvertY(Callback):\n",
    "    \"\"\"\n",
    "    Since we used TorchIO to sample the data, we first need to convert the y back to its normal values\n",
    "    \"\"\"\n",
    "    def before_batch(self):\n",
    "        \"\"\"\n",
    "        NOTE: as per the docs, you can only assign to `yb`, not `y`\n",
    "        `yb` is a tuple (which is immutable) therefore you must override the `self.learn.yb` - note we are assigning to to `learn.yb`\n",
    "        \"\"\"\n",
    "        #self.yb = tuple([torch.where(self.y != torch.tensor(1).cuda(), torch.tensor(0).cuda(), torch.tensor(1).cuda())])\n",
    "        self.learn.yb = tuple([torch.where(self.y != torch.tensor(1).cuda(), torch.tensor(0).cuda(), torch.tensor(1).cuda())])\n",
    "        #print(self.yb[0].shape)\n",
    "        #print(len(self.yb))\n",
    "        \n",
    "    #def after_pred(self):\n",
    "        # To check to see that the overwritten values of y did change\n",
    "        #print(self.y)\n",
    "        #print(dir(self))\n",
    "        \n",
    "class AddSigmoidActivation(Callback):\n",
    "    \"\"\"\n",
    "    Change the output to add a Sigmoid function \n",
    "    Needed since:\n",
    "        a) Using a pretrained Resnet model that doesn't support adding a final activation layer\n",
    "        b) unlike `cnn_learner`, a `unet_learner` doesn't have the `custom_head` parameter (which the forums suggest is an option to effectively add a layer to a pretarined model)\n",
    "    Note: need to check if `learner.model[-1].add_module` would work if you subclassed `nn.Module` and created a `forward()` method that added this activation?\n",
    "    \"\"\"\n",
    "    def after_pred(self):\n",
    "        \"\"\"\n",
    "        As per the documentation, this callback hook is specifically designed for modifying the outputs BEFORE theyre sent to the loss function\n",
    "        Thus it is a perfect place to add our sigmoid function to the outputs\n",
    "        \"\"\"\n",
    "        self.learn.pred = nn.Sigmoid()(self.pred)\n",
    "        \n",
    "class ProgressiveTransformsUpdateCallback(Callback):\n",
    "    def before_epoch(self):\n",
    "        if self.epoch < 4:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((4,4,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((4,4,1)), custom_reshape, custom_normalization,])\n",
    "            )\n",
    "            #for h in self.opt.hypers:\n",
    "            #    h[\"lr\"] = 0.00001\n",
    "        if 3 < self.epoch < 8:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((2,2,1)), custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([tio.Resample((2,2,1)), custom_reshape, custom_normalization,])\n",
    "            )\n",
    "            for h in self.opt.hypers:\n",
    "                h[\"lr\"] = 0.00001\n",
    "        if self.epoch > 7:\n",
    "            self.dls.train.subjects_dataset.set_transform(\n",
    "                tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "            )\n",
    "            self.dls.valid.subjects_dataset.set_transform(\n",
    "                tio.Compose([custom_reshape, custom_normalization,])\n",
    "            )\n",
    "        #print(self.data.dataset.subjects_dataset.dry_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = unet_learner(dls, resnet34, n_out=1, loss_func=dice_loss, lr=0.00015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fit\n",
      "   - before_fit     : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "  Start Epoch Loop\n",
      "     - before_epoch   : [Recorder, ProgressCallback]\n",
      "    Start Train\n",
      "       - before_train   : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      Start Batch Loop\n",
      "         - before_batch   : []\n",
      "         - after_pred     : []\n",
      "         - after_loss     : []\n",
      "         - before_backward: []\n",
      "         - before_step    : []\n",
      "         - after_step     : []\n",
      "         - after_cancel_batch: []\n",
      "         - after_batch    : [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      End Batch Loop\n",
      "    End Train\n",
      "     - after_cancel_train: [Recorder]\n",
      "     - after_train    : [Recorder, ProgressCallback]\n",
      "    Start Valid\n",
      "       - before_validate: [TrainEvalCallback, Recorder, ProgressCallback]\n",
      "      Start Batch Loop\n",
      "         - **CBs same as train batch**: []\n",
      "      End Batch Loop\n",
      "    End Valid\n",
      "     - after_cancel_validate: [Recorder]\n",
      "     - after_validate : [Recorder, ProgressCallback]\n",
      "  End Epoch Loop\n",
      "   - after_cancel_epoch: []\n",
      "   - after_epoch    : [Recorder]\n",
      "End Fit\n",
      " - after_cancel_fit: []\n",
      " - after_fit      : [ProgressCallback]\n"
     ]
    }
   ],
   "source": [
    "learner.show_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.dls.dataset.subjects_dataset.set_transform\n",
    "new_train_transforms = tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,])\n",
    "new_valid_transforms = tio.Compose([custom_reshape, custom_normalization,])\n",
    "#learner.dls.train.subjects_dataset.set_transform(new_train_transforms)\n",
    "#learner.dls.valid.subjects_dataset.set_transform(new_valid_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the fastai docs, models are saved in `learner.path/learner.model_dir/name.pth` so if this isn't set we need to pick a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can save only when a certain improvement happens but will do this later\n",
    "\n",
    "save_model_callback = SaveModelCallback(every_epoch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilise WandB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3a920z7j) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11064<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\Craig\\Documents\\Notebooks\\kidney_glomeruli\\wandb\\run-20210228_184411-3a920z7j\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\Craig\\Documents\\Notebooks\\kidney_glomeruli\\wandb\\run-20210228_184411-3a920z7j\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>3.78528</td></tr><tr><td>train_loss</td><td>0.56998</td></tr><tr><td>raw_loss</td><td>0.63942</td></tr><tr><td>wd_0</td><td>0.01</td></tr><tr><td>sqr_mom_0</td><td>0.99</td></tr><tr><td>lr_0</td><td>1e-05</td></tr><tr><td>mom_0</td><td>0.86282</td></tr><tr><td>eps_0</td><td>1e-05</td></tr><tr><td>wd_1</td><td>0.01</td></tr><tr><td>sqr_mom_1</td><td>0.99</td></tr><tr><td>lr_1</td><td>9e-05</td></tr><tr><td>mom_1</td><td>0.86282</td></tr><tr><td>eps_1</td><td>1e-05</td></tr><tr><td>wd_2</td><td>0.01</td></tr><tr><td>sqr_mom_2</td><td>0.99</td></tr><tr><td>lr_2</td><td>0.00087</td></tr><tr><td>mom_2</td><td>0.86282</td></tr><tr><td>eps_2</td><td>1e-05</td></tr><tr><td>_runtime</td><td>3331</td></tr><tr><td>_timestamp</td><td>1614559182</td></tr><tr><td>_step</td><td>617</td></tr><tr><td>valid_loss</td><td>0.64471</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▃▂▂▁▁▁▁▁▁▁▂▁▁▁▂▂▂▂▂▂▁▁▂▂▂▂▁▁▁▁▂▂▂▂▂▁▁▁</td></tr><tr><td>raw_loss</td><td>█▃▄▃▁▄▆▄▄▆▄▇▃▃▅▃▃▆▃▆▃▃▄▅▄▅▇▃▄▄▃█▄▆▇▃▃▄▄▄</td></tr><tr><td>wd_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_0</td><td>▁▁▂▃▃▄▆▆▇██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mom_0</td><td>██▇▇▆▅▄▃▂▁▁███▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂</td></tr><tr><td>eps_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wd_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_1</td><td>▁▁▂▂▃▄▅▆▇██▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>mom_1</td><td>██▇▇▆▅▄▃▂▁▁███▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂</td></tr><tr><td>eps_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wd_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_2</td><td>▁▁▂▂▃▄▅▆▇██▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>mom_2</td><td>██▇▇▆▅▄▃▂▁▁███▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂</td></tr><tr><td>eps_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid_loss</td><td>▂█▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">divine-donkey-38</strong>: <a href=\"https://wandb.ai/stantonius/Notebooks-kidney_glomeruli/runs/3a920z7j\" target=\"_blank\">https://wandb.ai/stantonius/Notebooks-kidney_glomeruli/runs/3a920z7j</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3a920z7j). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">restful-mountain-39</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/stantonius/Notebooks-kidney_glomeruli\" target=\"_blank\">https://wandb.ai/stantonius/Notebooks-kidney_glomeruli</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/stantonius/Notebooks-kidney_glomeruli/runs/c5ohydzl\" target=\"_blank\">https://wandb.ai/stantonius/Notebooks-kidney_glomeruli/runs/c5ohydzl</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\Craig\\Documents\\Notebooks\\kidney_glomeruli\\wandb\\run-20210228_193942-c5ohydzl</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init()\n",
    "wandb_callback = WandbCallback(log='all', log_preds=True, log_model=True, log_dataset=False, dataset_name=None, valid_dl=None, n_preds=36, seed=12345, reorder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a model has already been created and you want to continue training \n",
    "# note you should create the original learner first, and then run the code below\n",
    "#learner.load(\"could_do_beter_0.25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.594227</td>\n",
       "      <td>0.676311</td>\n",
       "      <td>16:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/6 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='16' class='' max='163' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      9.82% [16/163 01:31<14:04 0.5589]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train with standard resolution\n",
    "\n",
    "save_image_path = path/\"training_image_logs\"\n",
    "cbs=[PrinterCallback(save_image_path), ConvertY(), AddSigmoidActivation(), save_model_callback, wandb_callback, GradientAccumulation()]\n",
    "learner.fine_tune(6, cbs=cbs, base_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Progressive transformation\n",
    "\n",
    "save_image_path = path/\"training_image_logs\"\n",
    "cbs=[PrinterCallback(save_image_path), ConvertY(), AddSigmoidActivation(), save_model_callback, wandb_callback, ProgressiveTransformsUpdateCallback(), GradientAccumulation()]\n",
    "learner.freeze_to(-1)\n",
    "learner.fit_flat_cos(12, lr=1e-5, pct_start=0.4, cbs=cbs)\n",
    "#learner.fit_flat_cos(5, lr=1e-5, cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function fastai.optimizer.Adam(params, lr, mom=0.9, sqr_mom=0.99, eps=1e-05, wd=0.01, decouple_wd=True)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.opt_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with lower resolution and then upsampling at the very end\n",
    "\n",
    "save_image_path = path/\"training_image_logs\"\n",
    "cbs=[PrinterCallback(save_image_path), ConvertY(), AddSigmoidActivation(), save_model_callback, wandb_callback]\n",
    "learner.fine_tune(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with standard resolution\n",
    "\n",
    "save_image_path = path/\"training_image_logs\"\n",
    "cbs=[PrinterCallback(save_image_path), ConvertY(), AddSigmoidActivation(), save_model_callback, wandb_callback]\n",
    "learner.freeze_to(-1)\n",
    "learner.fit_one_cycle(12, lr_max=0.00001, div=100, cbs=cbs)\n",
    "#learner.fit_flat_cos(5, lr=1e-5, cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.save(\"can_still_do_better_0.115\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on training:\n",
    "\n",
    "* Initial LR was 0.001 - I think this was too high because we quickly (after 300 batches) got to a low loss, then the **loss explosion** happened (quickly improved loss, then loss deteriorates very fast and never comes back down - plateaus in the opposite direction)\n",
    "    * NOTE: Resampling doesn't incur this loss explosion - loss always trends down and then plateaus at the min\n",
    "* Running the `learner.lr_find()` suggests my LR was 10x too high\n",
    "    * However running `learner.fine_tune()` with this LR did not massively improve (loss around 0.79)\n",
    "        * One theory could be that the `lr_find()` only looks at the LR when we expect loss to improve in the first few batches. So this is the optimal LR in the early stages of training but it may get \"stale\" later in training (which the `lr_find()` has no insight on\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Fine TUNE [DONE]\n",
    "- Shorten the size of each epoch by reducing `samples_per_volume`\n",
    "- fp16\n",
    "- Train with resize and then scale up progressively by adjusting transforms through callbacks. Or just train and then fine tune (inclu unfeeze a few layers) with the larger data\n",
    "- TTA\n",
    "- Train with Resize(2) then upscale using a callback - try to round the edges where the hand drawn lines are sharp and innacurate\n",
    "- Try with bigger patch size\n",
    "- Try with different sample ratio\n",
    "- Plateau callback to adjust LR \n",
    "- Use an open-source histology pretrained model and fine tune with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['WANDB_MODE'] = 'dryrun'\n",
    "#master_trainer = MasterTrainer()\n",
    "#master_trainer.train(custom_unet, dice_loss, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(iter(master_trainer.train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.agent(wandb.sweep(sweep_config, entity=\"stantonius\", project=\"kidneys-cv\"), master_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb login --relogin\n",
    "#master_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_log_path = (path/\"lightning_logs/version_126/checkpoints/\")\n",
    "#training_log_path.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir lightning_logs/version_3/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update model without resized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = LitUNET.load_from_checkpoint(path/\"models\"/\"21_Feb_18_16_34-epoch=9-step=16249.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_trainer.set_transforms(\n",
    "    tio.Compose([custom_reshape, tio.RandomFlip(), tio.RandomAffine(), custom_normalization,]),\n",
    "    tio.Compose([custom_reshape, custom_normalization,])\n",
    ")\n",
    "master_trainer.train(load_model, dice_loss, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = LitUNET.load_from_checkpoint(path/\"models\"/\"21_Feb_20_8_15-epoch=4-step=8124.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_model_details = torch.load(path/\"models\"/\"21_Feb_19_16_56-epoch=8-step=14624.ckpt\")\n",
    "#print(checkpoint_model_details['hyper_parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[cut_image(item, path/\"test\", path/\"test/smaller\") for item in [img.name.split(\".\")[0] for img in (path/\"test\").glob(\"*.tiff\")]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An aside on inference/test transforms**: for some reason I don't see the ability to perform transforms on inference/test data in the TorchIO library. Therefore I am starting to question whether you are supposed to do this? Anyway, the evaluation doesn't work if you do not do this.\n",
    "\n",
    "I looked into how to create a *batch* transform but from what I read, I can iterate over each of the items in the batch very quickly because a) the patches are small, b) the transforms occur in C (therefore are already optimised) and c) GPU memory is limited - transfering data to the GPU to perform transforms that are already optimised for the CPU doesn't make much sense, and the transfer itself takes time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_images(imgs: Union[Path, list], transforms):\n",
    "    \"\"\"\n",
    "    Takes only image FILE NAMES (in Path of list form) and converts to ScalarImage\n",
    "    Then applies any transforms provided in tio.COMPOSE object\n",
    "    Returns a LIST of image tio.SUBJECTSDATASET\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, Path):\n",
    "        image_paths = [img.name for img in (imgs).rglob(\"*\") if not item.is_dir()]\n",
    "    if isinstance(imgs, list):\n",
    "        image_paths = imgs\n",
    "    \n",
    "    images =[]\n",
    "    for image_path in image_paths:\n",
    "        images.append(\n",
    "            tio.Subject(\n",
    "                img = tio.ScalarImage(image_path),\n",
    "                img_id = image_path.name.split(\".\")[0]\n",
    "            )\n",
    "        )\n",
    "    images_dataset = tio.SubjectsDataset(images, transforms)\n",
    "    return images_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_pred(subject, model, patch_size=(256,256,1)):\n",
    "    \"\"\"\n",
    "    Take a tio.SUBJECT and return a TUPLE of the image ID and its predicted output tensor\n",
    "    \"\"\"\n",
    "    grid_sampler = tio.inference.GridSampler(subject, patch_size)\n",
    "    patch_loader = torch.utils.data.DataLoader(grid_sampler, batch_size=4)\n",
    "    aggregator = tio.inference.GridAggregator(grid_sampler)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for patches_batch in patch_loader:\n",
    "            img_id = patches_batch['img_id']\n",
    "            input_tensor = patches_batch['img'][tio.DATA]\n",
    "            # Need to run the data through the non-random transforms that were applied to the training and validation data\n",
    "            #input_tensor = torch.stack([test_transforms(item) for item in input_tensor]).squeeze()\n",
    "            input_tensor.squeeze_()\n",
    "            if len(input_tensor.shape) == 3:\n",
    "                input_tensor.unsqueeze_(0)\n",
    "            locations = patches_batch[tio.LOCATION]\n",
    "            logits = model(input_tensor)\n",
    "            labels = logits\n",
    "            outputs = labels.unsqueeze(4)\n",
    "            #print(outputs.shape)\n",
    "            aggregator.add_batch(outputs, locations)\n",
    "        output_tensor = aggregator.get_output_tensor()\n",
    "        return (img_id, output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = [path/\"test/smaller\"/\"26dc41664_1.tiff\"]\n",
    "test_transforms = tio.Compose([tio.Resample((2,2,1)), custom_reshape, custom_normalization,])\n",
    "model = load_model.eval()\n",
    "\n",
    "test_preds = [subject_pred(item, model) for item in get_test_images(test_img_path, test_transforms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(test_preds[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(to_3chan(test_preds[0][1],0).squeeze().permute(1,2,0), cmap=\"hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rle_encoding(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup & Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[img.unlink() for img in (path/\"training_image_logs\").glob(\"*\") if img.name != \".ipynb_checkpoints\"]\n",
    "\n",
    "############################################################ \n",
    "# BE CAREFUL WITH BELOW - MAKE SURE THE DIRECTORY IS CORRECT\n",
    "############################################################\n",
    "\n",
    "# For lightning logs\n",
    "#[shutil.rmtree(folder) for folder in (path/\"lightning_logs\").glob(\"*\")]\n",
    "\n",
    "# For lightning logs\n",
    "#[model.unlink() for model in (path/\"models\").glob(\"*\")]\n",
    "\n",
    "# For wandb\n",
    "\"\"\"\n",
    "for folder in (path/\"wandb\").glob(\"*\"):\n",
    "    try: \n",
    "        shutil.rmtree(folder)\n",
    "    except: \n",
    "        continue\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run test set through above code and include in the model\n",
    "    * Utilise the inference from TorchIO\n",
    "* Add the stitch function to take inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(affine_options[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kidneys-cv] *",
   "language": "python",
   "name": "conda-env-kidneys-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
